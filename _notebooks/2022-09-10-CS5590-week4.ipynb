{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines CS5590 week 4\n",
    "> Foundations of Machine Learning CS5590  week 4\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Foundations of Machine Learning ]\n",
    "- author: Abhishek Kumar Dubey\n",
    "- image: images/CS5590_images/Acrobat_vct8kMqVEu.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview and History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is a discriminative classifier.\n",
    "- Inspired by Statistical Learning.\n",
    "- Developed in 1992 by Vapnik, Guyon, Boser \n",
    "- Was one of the go-to methods in ML since mid 1990s (only recently displaced by deep learning.)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Margin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Formulation <br><br>\n",
    "    - $f(\\mathbf{X},\\mathbf{W},b)=\\mathrm{sign} (\\mathbf{W} \\cdot \\mathbf{X}+b)$<br><br> \n",
    "![](../images/CS5590_images/Acrobat_vct8kMqVEu.png)\n",
    "    - Basic formulation of SVM can only handle two classes. \n",
    "    - There are improvised method to handle more than tow class.\n",
    "    - The __Maximum margin classifier__ is the linear classifier with the maximum margin. This is the simplest kind of SVM ( called an LSVM).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate the Margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The points those lies on the two margin lines are called support vector.\n",
    "- The model is immune to removal of any non-support-vector data points.\n",
    "- The equation of the line  is given by $\\mathbf{W}^T \\cdot \\mathbf{X}+b=0$<br><br>\n",
    "![](../images/CS5590_images/Acrobat_nYNJPaG71I.png)<br>\n",
    "- $\\mathbf{W}$ is always normal to the line $\\mathbf{W}^T \\cdot \\mathbf{X}+b=0$<br> \n",
    "  This can be proved by taking two vector $\\mathbf{X_1}$ and  $\\mathbf{X_2}$ on line  $\\mathbf{W}^T \\cdot \\mathbf{X}+b=0$,<br> \n",
    "  Now if we subtract the two vector we get $\\mathbf{W}^T(\\mathbf{X_1}-\\mathbf{X_2})=\\mathbf{0} \\Leftrightarrow  (\\mathbf{X_1}-\\mathbf{X_2}) \\perp \\mathbf{W}$. The same is explained on [stack overflow](https://datascience.stackexchange.com/a/49295).\n",
    "- Dotted line $\\mathbf{X'-X}$ is perpendicular to decision boundary so parallel to $\\mathbf{W} $ let it's length (magnitude) be $r$\n",
    "- The Unit vector along Dotted line $\\mathbf{X'-X}$ is given by $\\frac{\\mathit{\\mathbf{W}}}{\\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert }$\n",
    "- The equation of the dotted line $\\mathbf{X'-X}$ can be also given by magnitude multiplied by unit vector : <br>\n",
    "  $\\displaystyle r\\cdot \\frac{ \\mathit{\\mathbf{W}}}{\\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert }$\n",
    "- But as the dotted line can be on any side of the main line so we need to multiply with $y$, as $y$ takes value of $1$ or $-1$ depending on the side: <br>\n",
    "  $\\displaystyle \\mathbf{X'-X} = yr\\cdot \\frac{ \\mathit{\\mathbf{W}}}{\\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert }$<br>\n",
    "  $\\displaystyle \\mathbf{X'} = \\mathbf{X} - yr\\cdot \\frac{ \\mathit{\\mathbf{W}}}{\\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert }$\n",
    "- Now since $\\mathbf{X'}$ lies on the line so we can write  $\\mathbf{W}^T \\cdot \\mathbf{X'}+b=0$\n",
    "- Substituting value of $\\mathbf{X'}$ in $\\mathbf{W}^T \\cdot \\mathbf{X'}+b=0$ we get: <br>\n",
    "  $\\displaystyle \\mathbf{W}^T \\cdot \\left( \\mathbf{X} - yr\\cdot \\frac{ \\mathit{\\mathbf{W}}}{\\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert } \\right)+b=0$<br><br>\n",
    "- substituting $\\displaystyle \\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert =\\sqrt{\\mathit{\\mathbf{W}}^T \\mathit{\\mathbf{W}}}$ in above equation we get: <br>\n",
    "  $\\displaystyle \\mathbf{W}^T \\cdot \\left( \\mathbf{X} - yr\\cdot \\frac{ \\mathit{\\mathbf{W}}}{\\sqrt{\\mathit{\\mathbf{W}}^T \\mathit{\\mathbf{W}}}} \\right)+b=0$<br><br>\n",
    "  $\\displaystyle  \\left(  \\mathbf{W}^T \\mathbf{X} -   yr\\cdot \\frac{ \\mathit{\\mathbf{W}}^T\\mathit{\\mathbf{W}}}{\\sqrt{\\mathit{\\mathbf{W}}^T \\mathit{\\mathbf{W}}}} \\right)+b=0$<br><br>\n",
    "  $\\displaystyle  \\left(  \\mathbf{W}^T \\mathbf{X} -   yr\\cdot \\sqrt{\\mathit{\\mathbf{W}}^T \\mathit{\\mathbf{W}}} \\right)+b=0$<br><br>\n",
    "  $\\displaystyle    \\mathbf{W}^T \\mathbf{X} -   yr\\cdot \\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert  +b=0$<br><br>\n",
    "  $\\displaystyle    \\mathbf{W}^T \\mathbf{X}  +b =  yr\\cdot \\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert$<br><br>\n",
    "  $\\displaystyle    r = \\frac{\\mathbf{W}^T \\mathbf{X}  +b}{y\\cdot \\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert}$<br><br>\n",
    "- Since $y$ takes value of only $1$ or $-1$, hence we can bring $y$ to numerator. <br>\n",
    "\n",
    "  $\\displaystyle    r = y \\frac{\\mathbf{W}^T \\mathbf{X}  +b}{\\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since $\\mathbf{W}^T \\cdot \\mathbf{X}+b=0$ and $c\\left(\\mathbf{W}^T \\cdot \\mathbf{X}\\right)+b=0$ define the same plane, we have the freedom to choose the normalization of $\\mathbf{W}$ \n",
    "- Let us choose normalization such that $\\mathbf{W}^T \\cdot \\mathbf{X}_+ + b = +1$  and $\\mathbf{W}^T \\cdot \\mathbf{X}_- +b = -1$ for the positive and negative support vectors respectively.<br><br>\n",
    "![](../images/CS5590_images/Acrobat_AwymHLT9aY.png)<br><br>\n",
    "Hence, Margin now is: <br><br>\n",
    "$\\displaystyle  \\left( +1 \\right) \\frac{\\mathbf{W}^T \\mathbf{X_+} +b}{ \\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert} +  \\left( -1 \\right) \\frac{\\mathbf{W}^T \\mathbf{X_-} +b}{ \\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert}$<br><br>\n",
    "Since $\\mathbf{W}^T \\mathbf{X_+} +b=+1$ and $\\mathbf{W}^T \\mathbf{X_-} +b=-1$, substituting these  in above equation we get: <br><br>\n",
    "$\\displaystyle  \\left( +1 \\right) \\frac{\\mathbf{W}^T \\mathbf{X_+} +b}{ \\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert} +  \\left( -1 \\right) \\frac{\\mathbf{W}^T \\mathbf{X_-} +b}{ \\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert}=\\displaystyle  \\left( +1 \\right) \\frac{+1}{ \\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert} +  \\left( -1 \\right) \\frac{-1}{ \\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert}=\\frac{2}{\\left\\lVert \\mathbf{W}\\right\\rVert}$<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tip: <br>Margin between the two support vector is   given  by: $$\\displaystyle    \\frac{2}{\\left\\lVert \\mathbf{W}\\right\\rVert}$$\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximize the Margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we know the margin between the two support vector.\n",
    "- We need to maximize the margin in such a way that $+1$ class points lies on one side of the margin and $-1$ class points lies on the other side of the margin.\n",
    "- We can formulate this as the _quadratic optimization problem_:<br>\n",
    "  Find $\\mathbf{W}$ such that <br> <br>\n",
    "  $ \\displaystyle \\rho = \\frac{2}{\\left\\lVert \\mathbf{W}\\right\\rVert } $ is maximized; and for all $\\left\\{ \\left( \\mathbf{X}_i,\\mathbf{y}_i \\right) \\right\\}$<br> <br>\n",
    "  and $\\mathbf{W}^T \\cdot \\mathbf{X}_i+b \\ge 1$ if $y_i=+1$<br> <br>\n",
    "  and $\\mathbf{W}^T \\cdot \\mathbf{X}_i+b \\le -1$ if $y_i=-1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better formulation is to minimize inverse of $\\rho$ instead of maximizing it.\n",
    "- We know that <br>\n",
    "  $ \\displaystyle  \\max \\frac{2}{\\left\\lVert \\mathbf{W}\\right\\rVert } =\\min  \\frac{\\left\\lVert \\mathbf{W}\\right\\rVert}{2} =\\min\\frac{\\sqrt{ \\mathbf{W}^T\\mathbf{W}}}{2} $<br>\n",
    "-  Instead of minimizing  $ \\displaystyle  \\frac {\\left\\lVert \\mathbf{W}\\right\\rVert}{2}$ we minimize $ \\displaystyle  \\frac {\\left\\lVert \\mathbf{W}\\right\\rVert^2}{2} = \\displaystyle  \\frac{ \\mathbf{W}^T\\mathbf{W}}{2}  $   as both (with or without square)   are equivalent. we select square one  as math (derivative) becomes easy.<br>\n",
    "\n",
    ">Tip:  Maximization problem can be written in terms of minimization as follows:<br><br>Find $\\mathbf{W}$ and $b$ such that<br><br>$\\displaystyle \\frac{\\mathbf{W}^T\\mathbf{W}}{2}$ is minimized.<br><br>and for all  $\\left\\{ \\left( \\mathbf{X}_i,\\mathbf{y}_i \\right) \\right\\} : \\displaystyle y_i\\left( \\mathbf{W}^T\\mathbf{X}_i +b\\right) \\ge 1$<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Lagrange Multipliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of Lagrange Multipliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Optimization problem:<br>\n",
    "  __Minimize__ : $\\displaystyle f\\left( \\overrightarrow{x}  \\right)$ <br>\n",
    "  __Such that__ for all $i,$ $\\displaystyle g_i\\left( \\overrightarrow{x}  \\right)\\le 0$ <br>\n",
    "- To solve the above problem we create augmented Lagrange function:<br><br>\n",
    "  $\\displaystyle L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda}   \\right):=f\\left( \\overrightarrow{x}  \\right)+\\sum_{i=1}^{n}\\lambda_ig_i\\left( \\overrightarrow{x}  \\right)$ <br><br>\n",
    "  $\\displaystyle \\underbrace{L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda}   \\right)}_{\\text{lagrange function}} :=f\\left( \\overrightarrow{x}  \\right)+\\sum_{i=1}^{n}\\underbrace{\\lambda_i}_{\\text{lagrange variable  or dual varialbe }}g_i \\left( \\overrightarrow{x}  \\right)$ <br><br>\n",
    "- Observation:<br>\n",
    "  For any feasible $x$ and all $\\lambda_i \\ge 0$,  <br><br>\n",
    "  $\\displaystyle L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda}   \\right):=f\\left( \\overrightarrow{x}  \\right)+\\overbrace{\\sum_{i=1}^{n}\\overbrace{\\lambda_i}^{\\text{this is positve}} \\underbrace{g_i\\left( \\overrightarrow{x}  \\right)}_{\\text{this is negative}}}^{\\text{This is negative}}  $ <br><br>\n",
    "  Hence , $\\displaystyle L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda}   \\right) \\le f\\left( \\overrightarrow{x}  \\right)$ <br>\n",
    "  $\\displaystyle \\Longrightarrow \\max_{\\lambda_i \\ge 0} L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda}   \\right) \\le f\\left( \\overrightarrow{x}  \\right)$ <br><br>\n",
    "- So, the optimal value to the constrained optimization:<br><br>\n",
    "  $\\displaystyle p^*:=\\min_{\\overrightarrow{x} } \\max_{\\lambda_i \\ge 0} L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda}   \\right) $ <br><br>\n",
    "  We can see that now problem becomes unconstrained in $x$ <br>\n",
    "  Also $p^*$ is called __The primal__ problem<br><br>\n",
    "- Observation:<br>\n",
    "  consider a function: $\\displaystyle \\min_{\\overrightarrow{x} }  L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda}   \\right) $<br>\n",
    "  Since   $p^*$ is solution for maximum possible $\\lambda$ so for any feasible $x$ and all $\\lambda_i \\ge 0$<br><br>\n",
    "  $\\displaystyle p^* \\ge \\min_{\\overrightarrow{x} }  L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda}   \\right) $ <br>\n",
    "  Thus:<br><br>\n",
    "  $\\displaystyle d^*:= \\max_{\\lambda_i \\ge 0} \\min_{\\overrightarrow{x} } L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda}   \\right) \\le p^*$ <br><br>\n",
    "  Also $d^*$ is called __The dual__ problem<br><br>\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note:<br><br>__Optimization problem__:<br>_Minimize_ : $\\displaystyle f\\left( \\overrightarrow{x}  \\right)$ <br>_Such that_ for all $i,$ $\\displaystyle g_i\\left( \\overrightarrow{x}  \\right)\\le 0$ <br>_Lagrange Function_ : $\\displaystyle L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda}   \\right):=f\\left( \\overrightarrow{x}  \\right)+\\sum_{i=1}^{n}\\lambda_ig_i\\left( \\overrightarrow{x}  \\right)$ <br><br> __Primal__: $\\displaystyle p^*:=\\min_{\\overrightarrow{x} } \\max_{\\lambda_i \\ge 0} L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda}   \\right) $ <br><br>__Dual__: $\\displaystyle d^*:= \\max_{\\lambda_i \\ge 0} \\min_{\\overrightarrow{x} } L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda}   \\right) $ <br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Theorem (weak Lagrangian duality):<br><br>\n",
    "  $d^* \\le p^*$<br><br>\n",
    "  This is also called as minimax inequality<br><br>\n",
    "  $p^*-d^*$ is called duality gap<br><br>\n",
    "\n",
    "- There are certain condition when duality gap becomes zero, for that we need to understand convexity\n",
    "  - A function $f:\\mathbb{R}^d \\rightarrow \\mathbb{R} $ is called convex iff for any two point $x$ and $x'$ and $\\beta \\in \\left[ 0,1 \\right]$<br><br>\n",
    "    $f\\left( \\beta\\overrightarrow{x}+\\left( 1-\\beta \\right)\\overrightarrow{x}   \\right) \\le \\beta f\\left( \\overrightarrow{x}  \\right)+\\left( 1-\\beta \\right)f\\left( \\overrightarrow{x}  \\right)$<br><br>\n",
    "![](../images/CS5590_images/Acrobat_9xAcM49wbq.png)<br><br>\n",
    "  - A set $S \\subset\\mathbb{R}^d $ is called conved iff for any tow points  $x, x' \\in S$ and any  $\\beta \\in \\left[ 0,1 \\right]$<br><br>\n",
    "    $\\beta \\overrightarrow{x}+\\left( 1-\\beta \\right)\\overrightarrow{x} \\in S $<br><br>\n",
    "![](../images/CS5590_images/Acrobat_4WENH5ng1n.png)<br><br>\n",
    "  - Convex Optimization problem <br><br>\n",
    "    $\\displaystyle \\min_{\\overrightarrow{x} \\in \\mathbb{R}^d } f\\left( \\overrightarrow{x}  \\right)$ <br><br>\n",
    "    subject to: $\\displaystyle g_i\\left( \\overrightarrow{x}  \\right)\\le 0$ for $1 \\le i \\le n$<br><br>\n",
    "    is called convex optimization problem if:<br>\n",
    "    - The objective function $f\\left( \\overrightarrow{x}  \\right)$ is convex function, and\n",
    "    - the feasible set induced by the constraints $g_i$ is a convex set.<br><br>\n",
    "- Theorem (strong Lagrangian duality):<br><br>\n",
    "  if $f$ is convex and for a feasible point $x^*$<br><br>\n",
    "  $g_i\\left( \\overrightarrow{x^*}  \\right)<0$, or <br><br>\n",
    "  $g_i\\left( \\overrightarrow{x^*}  \\right) \\le 0$ when $g$ is affine<br><br>\n",
    "  Then $d^*=p^*$<br><br>\n",
    "  This is called Slater's condition. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM standard (primal) form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle \\min_{w,b}\\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2 $ <br><br>\n",
    "such that: $\\forall$ $i,$ $y_i\\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i}   +b \\right) \\ge 1$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observations :\n",
    "    - Objective function is _convex_\n",
    "    - the constraints are _affine_, inducing a polytope constraint set.\n",
    "- So SVM is a convex optimization problem (in fact a quadratic program)\n",
    "- Moreover, __strong duality holds__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lagrangian for SVM\n",
    "    - For Lagrangian the constraint should always  written as less than $0$ format: <br><br>\n",
    "      $y_i\\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i}   +b \\right) -1 \\ge 0$ <br><br>\n",
    "      $- y_i\\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i}   +b \\right) +1 \\le 0$ <br><br>\n",
    "      $ 1 - y_i\\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i}   +b \\right) \\le 0$ <br><br>\n",
    "    - Now the Lagrangian for SVM can be written as:<br><br>\n",
    "      $\\displaystyle L\\left( \\overrightarrow{w},b,\\overrightarrow{\\alpha}   \\right)=\\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2 + \\underbrace{\\sum_{i = 1}^{n}\\alpha_i\\left( 1-y_i\\left( \\overrightarrow{w}\\cdot\\overrightarrow{x_i}+b   \\right) \\right)}_{\\text{appears like a hinge loss}}   $<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Dual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Primal<br><br>\n",
    "  $\\displaystyle \\min_{\\overrightarrow{w},b }\\max_{\\overrightarrow{\\alpha } \\ge 0 }\\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2 + \\sum_{i = 1}^{n}\\alpha_i\\left( 1-y_i\\left( \\overrightarrow{w}\\cdot\\overrightarrow{x_i}+b   \\right) \\right)  $<br><br>\n",
    "- Dual<br><br>\n",
    "  $\\displaystyle\\max_{\\overrightarrow{\\alpha } \\ge 0 } \\min_{\\overrightarrow{w},b }\\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2 + \\sum_{i = 1}^{n}\\alpha_i\\left( 1-y_i\\left( \\overrightarrow{w}\\cdot\\overrightarrow{x_i}+b   \\right) \\right)   $<br><br>\n",
    "- Slater's condition from convex optimization guarantees that these two optimization problems are equivalent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving using KKT condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KKT stands for Karush-Kuhn-Tucker Condition\n",
    "- We solve Dual problem:<br><br>\n",
    "  $\\displaystyle\\max_{\\overrightarrow{\\alpha } \\ge 0 } \\min_{\\overrightarrow{w},b }\\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2 + \\sum_{i = 1}^{n}\\alpha_i\\left( 1-y_i\\left( \\overrightarrow{w}\\cdot\\overrightarrow{x_i}+b   \\right) \\right)   $<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can solve for optimal $w$, $b$ as function of $\\alpha$<br><br>\n",
    "  $\\displaystyle \\frac{\\partial L }{\\partial \\overrightarrow{w}}=  w - \\sum_{i}\\alpha_iy_i\\overrightarrow{x}_i=0 \\Rightarrow w  =  \\sum_{i}\\alpha_iy_i\\overrightarrow{x}_i $<br><br>\n",
    "  $\\displaystyle \\frac{\\partial L }{\\partial b}=  \\sum_{i}\\alpha_iy_i=0 \\Rightarrow  \\sum_{i}\\alpha_iy_i =0$<br><br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- substituting these values back in Dual we get: <br><br>\n",
    "  $\\displaystyle \\max_{\\overrightarrow{\\alpha } \\ge 0 }  \\frac{1}{2} \\left( \\sum_{i}\\alpha_iy_i\\overrightarrow{x}_i  \\right) \\cdot \\left( \\sum_{j}\\alpha_jy_j\\overrightarrow{x}_j \\right) + \\sum_{i = 1}^{n}\\alpha_i\\left( 1-y_i\\left( \\sum_{j}\\alpha_jy_j\\overrightarrow{x}_j \\cdot\\overrightarrow{x_i}+b   \\right) \\right)   $<br><br>\n",
    "  $\\displaystyle \\max_{\\overrightarrow{\\alpha } \\ge 0 } \\frac{1}{2} \\sum_{i,j}\\alpha_i\\alpha_jy_iy_j\\overrightarrow{x}_i\\cdot\\overrightarrow{x} _j  + \\sum_{i = 1}^{n}\\alpha_i-\\sum_{i = 1}^{n}  \\left(     \\alpha_iy_i\\left( \\sum_{j}\\alpha_jy_j\\overrightarrow{x} _j\\cdot\\overrightarrow{x_i}+b   \\right) \\right)   $<br><br>\n",
    "  $\\displaystyle \\max_{\\overrightarrow{\\alpha } \\ge 0 } \\frac{1}{2} \\sum_{i,j}\\alpha_i\\alpha_jy_iy_j\\overrightarrow{x}_i\\cdot\\overrightarrow{x} _j  + \\sum_{i = 1}^{n}\\alpha_i- \\sum_{i = 1}^{n}\\alpha_iy_i\\sum_{j}\\alpha_jy_j\\overrightarrow{x} _j\\cdot\\overrightarrow{x_i}+  \\sum_{i = 1}^{n}\\alpha_iy_ib      $<br><br>\n",
    "  $\\displaystyle \\max_{\\overrightarrow{\\alpha } \\ge 0 } -\\frac{1}{2} \\sum_{i,j}\\alpha_i\\alpha_jy_iy_j\\overrightarrow{x}_i\\cdot\\overrightarrow{x} _j  + \\sum_{i = 1}^{n}\\alpha_i +  \\sum_{i = 1}^{n}\\alpha_iy_ib      $<br><br>\n",
    "  $\\displaystyle \\max_{\\overrightarrow{\\alpha } \\ge 0 } \\sum_{i = 1}^{n}\\alpha_i  -\\frac{1}{2} \\sum_{i,j}\\alpha_i\\alpha_jy_iy_j\\overrightarrow{x}_i\\cdot\\overrightarrow{x} _j  $<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The above equation can also be written as: <br><br>\n",
    "  $\\displaystyle \\max \\sum_{k = 1}^{R}\\alpha_k  - \\frac{1}{2}\\sum_{k=1}^{R}\\sum_{l = 1}^{R}\\alpha_k \\alpha_l Q_{kl}, $ where $\\displaystyle Q_{kl} =y_ky_l\\left( \\mathbf{X_k}\\cdot \\mathbf{X_l} \\right)$<br><br>\n",
    "  subject to constrains:<br><br>$\\alpha_k \\ge 0,$ and $\\forall k , $ $\\displaystyle \\sum_{k=1}^{R}\\alpha_ky_k=0$<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Above problem can be solved using SMO (Sequential minimal optimization) or any other quadratic programming or gradient descent. \n",
    "- Once we solve we get optimum $\\alpha^*$\n",
    "- Using  $\\alpha^*$   we can get $w^*$ as below:<br><br>\n",
    " $\\displaystyle  w^*  =  \\sum_{i}\\alpha_i^*y_i\\overrightarrow{x}_i $<br><br>\n",
    "- $b^*$ can be calculated as follows:<br><br>\n",
    "  $y_i\\left( \\overrightarrow{w}^* \\cdot \\overrightarrow{x_i}   +b \\right) = 1$ <br><br>\n",
    "  $y_i\\left( \\overrightarrow{w}^* \\cdot \\overrightarrow{x_i} \\right) +  y_ib = 1$ <br><br>\n",
    "  Multiplying $y_i$ both the sides: <br><br>\n",
    "  $y_i y_i\\left( \\overrightarrow{w}^* \\cdot \\overrightarrow{x_i} \\right) +  y_i y_ib = y_i$ <br><br>\n",
    "  $y_i y_ib = y_i -y_i y_i\\left( \\overrightarrow{w}^* \\cdot \\overrightarrow{x_i} \\right) $ <br><br>\n",
    "  $y_i y_i b$ can be written as $b$ because $y_i$ can be only $\\pm 1$ so in either case $y_i y_i =1$<br><br>\n",
    "  $b = y_i \\left( 1-  y_i\\left( \\overrightarrow{w}^* \\cdot \\overrightarrow{x_i} \\right)  \\right)$ <br><br>\n",
    "  $b^* = - y_i \\left( y_i\\left( \\overrightarrow{w}^* \\cdot \\overrightarrow{x_i} \\right)- 1  \\right)$ <br><br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we can classify with:<br><br>\n",
    "$f(\\mathbf{X},\\mathbf{W}^*,b^*)=\\mathrm{sign} (\\mathbf{W}^* \\cdot \\mathbf{X}+b^*)$<br><br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Margin SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Till now what we discussed is called Hard margin SVM.\n",
    "- In practice data is not always separable\n",
    "- We allow misclassification of the data point in soft margin SVM.\n",
    "- Soft margin SVM is also called as C-SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now our Lagrangian is formulated as below:<br><br>\n",
    "$\\displaystyle \\min_{w,b,\\varepsilon}\\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2 + c \\sum_{j=1}^{N}\\varepsilon_j  $  <br><br>\n",
    "such that: $\\forall$ $i,$ $y_i\\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i}   +b \\right) \\ge 1-\\varepsilon _i$, and $\\varepsilon _i\\ge0$ <br><br>\n",
    "\n",
    "- Significance of $\\varepsilon$<br><br>\n",
    "    - $\\displaystyle \\min_{w,b,\\varepsilon}\\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2 + \\overbrace{c}^{\\text{Controls amount of misclassification}}  \\underbrace{\\sum_{j=1}^{N}\\varepsilon_j }_{\\text{Minimize }\\varepsilon \\text{ also  } }  $<br><br> \n",
    "    such that: $\\forall$ $i,$ $y_i\\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i}   +b \\right) \\ge 1-\\underbrace{\\varepsilon _i}_{\\text{to allow misclassification}} ,$ and $\\overbrace{\\varepsilon _i\\ge0}^{\\text{keep } \\varepsilon \\text{ positive} } $ <br><br> \n",
    "    - $\\varepsilon_i \\ge 1 \\Longleftrightarrow y_i \\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i}   +b \\right) < 0,$ i.e., misclassification.\n",
    "    - $0<\\varepsilon_i < 1 \\Longleftrightarrow  x_i$ is correctly classified, but lies inside the margin\n",
    "    - $\\varepsilon_i =0 \\Longleftrightarrow  x_i$ is correctly classified, and lies outside of margin\n",
    "    - $\\sum_{j=1}^{N}\\varepsilon_j$ is an upper bound on training errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lagrangian for Soft Margin SVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We need to Solve:<br><br>\n",
    "$\\displaystyle \\min_{w,b,\\varepsilon}\\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2 + c \\sum_{i=1}\\varepsilon_i  $  <br><br>\n",
    "such that: $\\forall$ $i,$ $y_i\\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i}   +b \\right) \\ge 1-\\varepsilon _i$, and $\\varepsilon _i\\ge0$ <br><br>\n",
    "- For Lagrangian the constraint should always  written as less than $0$ condition: <br><br>\n",
    "  $y_i\\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i}   +b \\right) \\ge 1-\\varepsilon _i$, and $\\varepsilon _i\\ge0$ <br><br>\n",
    "  $y_i\\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i}   +b \\right) - 1 + \\varepsilon _i \\ge 0$, and $-\\varepsilon _i \\le 0$ <br><br>\n",
    "  $1 - \\varepsilon _i  -y_i\\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i}   +b \\right) \\le 0$, and $-\\varepsilon _i \\le 0$ <br><br>\n",
    "- Lagrangian formulation of above problem:<br><br>\n",
    "  $\\displaystyle L\\left( \\overrightarrow{w},b,\\overrightarrow{\\alpha},\\overrightarrow{\\beta},\\overrightarrow{\\varepsilon }     \\right)= \\\\ \\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2 +c\\sum_{i}\\varepsilon_i + \\sum_{i = 1}^{n}\\alpha_i\\left( 1- \\varepsilon _i - y_i\\left( \\overrightarrow{w}\\cdot\\overrightarrow{x_i}+b   \\right) \\right) -\\sum_{i}\\beta_i \\varepsilon_i  $<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Primal<br><br>\n",
    "  $\\displaystyle  \\min_{ \\left(  \\overrightarrow{w},b,\\overrightarrow{\\varepsilon } \\right)}\\max_{\\left(  \\overrightarrow{\\alpha } \\ge 0 ,\\overrightarrow{\\beta} \\ge 0  \\right)}\\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2 +c\\sum_{i}\\varepsilon_i + \\sum_{i = 1}^{n}\\alpha_i\\left( 1- \\varepsilon _i - y_i\\left( \\overrightarrow{w}\\cdot\\overrightarrow{x_i}+b   \\right) \\right) -\\sum_{i}\\beta_i \\varepsilon_i  $<br><br>\n",
    "- Dual<br><br>\n",
    "  $\\displaystyle  \\max_{\\left( \\overrightarrow{\\alpha } \\ge 0 , \\overrightarrow{\\beta} \\ge 0  \\right)} \\min_{\\left( \\overrightarrow{w},b,\\overrightarrow{\\varepsilon }  \\right)}\\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2 +c\\sum_{i}\\varepsilon_i + \\sum_{i = 1}^{n}\\alpha_i\\left( 1- \\varepsilon _i - y_i\\left( \\overrightarrow{w}\\cdot\\overrightarrow{x_i}+b   \\right) \\right) -\\sum_{i}\\beta_i \\varepsilon_i  $<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Same as Hard margin SVM we use KKT condition and solve minimization of dual:<br><br>\n",
    "   $\\displaystyle \\frac{\\partial L }{\\partial \\overrightarrow{w}}=  w - \\sum_{i}\\alpha_iy_i\\overrightarrow{x}_i=0 \\Rightarrow w  =  \\sum_{i}\\alpha_iy_i\\overrightarrow{x}_i $<br><br>\n",
    "  $\\displaystyle \\frac{\\partial L }{\\partial b}=  \\sum_{i}\\alpha_iy_i=0 \\Rightarrow  \\sum_{i}\\alpha_iy_i =0$<br><br>\n",
    "  $\\displaystyle \\frac{\\partial L }{\\partial \\varepsilon _i}= c -\\alpha_i  - \\beta_i = 0\\Rightarrow  c=\\beta_i+\\alpha_i$<br><br>\n",
    "  - Observation:<br><br>\n",
    "   $\\overbrace{c}^{\\text{Upper bound of }\\alpha \\text{ and }\\beta} =\\underbrace{\\beta_i}_{\\text{always +ve}} +\\underbrace{\\alpha_i}_{\\text{always +ve}}$<br><br>\n",
    "   so we can say: $0 \\le \\alpha_i \\le c$ $ \\forall i$<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- substituting these values back in Dual we get: <br><br>\n",
    "$\\displaystyle \\Rightarrow   \\max_{\\overrightarrow{\\alpha } \\ge 0 , \\overrightarrow{\\beta} \\ge 0} \\frac{1}{2}\\left( \\sum_{i}\\alpha_iy_i\\overrightarrow{x}_i  \\right) \\cdot \\left( \\sum_{j}\\alpha_jy_j\\overrightarrow{x}_j \\right) +\\sum_{i}\\varepsilon_i\\left( \\beta_i+\\alpha_i \\right) + \\sum_{i = 1}^{n}\\alpha_i\\left( 1- \\varepsilon _i - y_i\\left( \\sum_{j}\\alpha_jy_j\\overrightarrow{x}_j \\cdot\\overrightarrow{x_i}+b   \\right) \\right) -\\sum_{i}\\beta_i \\varepsilon_i  $<br><br>\n",
    "$\\displaystyle \\Rightarrow \\max_{\\overrightarrow{\\alpha } \\ge 0 , \\overrightarrow{\\beta} \\ge 0} \\frac{1}{2}\\left( \\sum_{i}\\alpha_iy_i\\overrightarrow{x}_i  \\right) \\cdot \\left( \\sum_{j}\\alpha_jy_j\\overrightarrow{x}_j \\right) +\\sum_{i}\\varepsilon_i \\beta_i+\\sum_{i}\\varepsilon_i  \\alpha_i  +  \\sum_{i = 1}^{n}\\alpha_i - \\sum_{i = 1}^{n}\\alpha_i \\varepsilon _i - \\sum_{i = 1}^{n}\\alpha_i y_i\\left( \\sum_{j}\\alpha_jy_j\\overrightarrow{x}_j \\cdot\\overrightarrow{x_i}+b   \\right)  -\\sum_{i}\\beta_i \\varepsilon_i  $<br><br>\n",
    "$\\displaystyle \\Rightarrow \\max_{\\overrightarrow{\\alpha } \\ge 0 , \\overrightarrow{\\beta} \\ge 0} \\frac{1}{2} \\sum_{i}\\alpha_i \\alpha_j y_i y_j \\overrightarrow{x}_i \\overrightarrow{x}_j   +\\sum_{i}\\varepsilon_i \\beta_i+\\sum_{i}\\varepsilon_i  \\alpha_i  +  \\sum_{i = 1}^{n}\\alpha_i - \\sum_{i = 1}^{n}\\alpha_i \\varepsilon _i - \\sum_{i = 1}^{n}\\alpha_i y_i \\sum_{j}\\alpha_jy_j\\overrightarrow{x}_j \\cdot\\overrightarrow{x_i}+  \\sum_{i = 1}^{n}\\alpha_i y_ib     -\\sum_{i}\\beta_i \\varepsilon_i  $<br><br>\n",
    "$\\displaystyle \\Rightarrow \\max_{\\overrightarrow{\\alpha } \\ge 0 } \\frac{1}{2} \\sum_{i}\\alpha_i \\alpha_j y_i y_j \\overrightarrow{x}_i \\overrightarrow{x}_j     +  \\sum_{i = 1}^{n}\\alpha_i  - \\sum_{i = 1}^{n}\\alpha_i y_i \\sum_{j}\\alpha_jy_j\\overrightarrow{x}_j \\cdot\\overrightarrow{x_i}+  \\sum_{i = 1}^{n}\\alpha_i y_ib    $<br><br>\n",
    "$\\displaystyle \\Rightarrow \\max_{\\overrightarrow{\\alpha } \\ge 0 } \\frac{1}{2} \\sum_{i}\\alpha_i \\alpha_j y_i y_j \\overrightarrow{x}_i \\overrightarrow{x}_j     +  \\sum_{i = 1}^{n}\\alpha_i  - \\sum_{i=1,j=1}^{n}\\alpha_i \\alpha_j y_i  y_j \\overrightarrow{x_i} \\cdot \\overrightarrow{x_j}+  \\sum_{i = 1}^{n}\\alpha_i y_ib    $<br><br>\n",
    "$\\displaystyle \\Rightarrow \\max_{\\overrightarrow{\\alpha } \\ge 0 } \\sum_{i = 1}^{n}\\alpha_i  -\\frac{1}{2} \\sum_{i}\\alpha_i \\alpha_j y_i y_j \\overrightarrow{x}_i \\overrightarrow{x}_j     $<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Notice neither $\\overrightarrow{\\beta}$ nor $\\overrightarrow{\\varepsilon }$ appears in the above equation__.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The above equation  can also be written as: <br><br>\n",
    "  $\\displaystyle \\max \\sum_{k = 1}^{R}\\alpha_k  - \\frac{1}{2}\\sum_{k=1}^{R}\\sum_{l = 1}^{R}\\alpha_k \\alpha_l Q_{kl}, $ where $\\displaystyle Q_{kl} =y_ky_l\\left( \\mathbf{X_k}\\cdot \\mathbf{X_l} \\right)$<br><br>\n",
    "  subject to constrains:<br><br>$0 \\le \\alpha_k \\le c,$ and $\\forall k , $ $\\displaystyle \\sum_{k=1}^{R}\\alpha_ky_k=0$<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note:  One of the constraint of soft margin SVM is $0 \\le \\alpha_k \\le c$ which is different for hard margin SVM constraint $\\alpha_k \\ge 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class Classification with SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SVM can handle only tow-class outputs.\n",
    "- what to do for multi-class case:\n",
    "    - one vs all SVM\n",
    "        - Learn N SVMs\n",
    "        - SVM 1 learns class $1$ vs not class $1$\n",
    "        - SVM 2 learns class $2$ vs not class $2$ and so on.\n",
    "        - Then to predict the output for a new point, just predict with each SVM and fond out which one puts the prediction the furthest into the positive region.\n",
    "    - Other approaches:\n",
    "        - pair-wise SVM\n",
    "        - Tree-structured SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Trick  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we require the Kernel Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We found that after solving minimization problem of dual of SVM we get following: <br>\n",
    "  $\\displaystyle \\max \\sum_{k = 1}^{R}\\alpha_k  - \\frac{1}{2}\\sum_{k=1}^{R}\\sum_{l = 1}^{R}\\alpha_k \\alpha_l Q_{kl}, $ where $\\displaystyle Q_{kl} =y_ky_l\\left( \\mathbf{X_k}\\cdot \\mathbf{X_l} \\right)$<br><br>\n",
    "  subject to constrains:<br><br>$0 \\le \\alpha_k \\le c,$ and $\\forall k , $ $\\displaystyle \\sum_{k=1}^{R}\\alpha_ky_k=0$<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- But if the data can't be separated linearly we transform the data to higher dimension space using the transformation $\\phi$. So that the data can be separated using a hyper-plane in higher dimension space. In that case the above equation changes as below : <br>\n",
    "$\\displaystyle \\max \\sum_{k = 1}^{R}\\alpha_k  - \\frac{1}{2}\\sum_{k=1}^{R}\\sum_{l = 1}^{R}\\alpha_k \\alpha_l Q_{kl}, $ where \n",
    "$\\displaystyle Q_{kl} =y_ky_l \\underbrace{\\left(\\mathbf{\\Phi} \\left(  \\mathbf{X}_k \\right)\\cdot \\mathbf{\\Phi}  \\left( \\mathbf{X}_l \\right)\\right)}_{\\text{Notice the term } \\Phi } $<br><br>\n",
    "  subject to constrains:<br><br>$0 \\le \\alpha_k \\le c,$ and $\\forall k , $ $\\displaystyle \\sum_{k=1}^{R}\\alpha_ky_k=0$<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then compute : <br>\n",
    "$\\displaystyle  \\mathbf{W}  =  \\sum_{\\text{k s.t } \\alpha_k >0 }\\alpha_k^*y_k \\mathbf{\\Phi} \\left(  \\mathbf{X}_k  \\right)$\n",
    "- Then classify with <br>\n",
    "$\\displaystyle f(\\mathbf{X},w,b)=\\mathrm{sign}\\left( \\mathbf{W}  \\cdot \\mathbf{\\Phi}(\\mathbf{X})+b \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most important change : <br>\n",
    "$\\mathbf{X} \\rightarrow \\mathbf{\\Phi}(\\mathbf{X})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notice that in the term $\\displaystyle Q_{kl} =y_ky_l \\left(\\mathbf{\\Phi} \\left(  \\mathbf{X}_k \\right)\\cdot \\mathbf{\\Phi}  \\left( \\mathbf{X}_l \\right)\\right) $ we must do $\\frac{R^2}{2}$ dot products to get this matrix ready.<br><br>\n",
    "Assuming a quadratic polynomial kernel, each dot product requires $\\frac{m^2}{2}$ addition and multiplication ( where $m$ is the dimension of $X$) <br><br>\n",
    "The whole thing costs $\\frac{R^2m^2}{4}$<br><br>\n",
    "This is the reason we require a trick so that we need not do this large computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How  do we do the kernel Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b92a85dd111e7f234cbdfd9e3dc75ce12717f6161daa18a82d26f0e8e840c205"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
