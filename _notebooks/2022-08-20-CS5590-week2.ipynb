{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of Machine Learning (Evaluation Measures) \n",
    "> Foundations of Machine Learning  week 2\n",
    "\n",
    "- toc: true \n",
    "- badges: false\n",
    "- comments: true\n",
    "- categories: [Foundations of Machine Learning ]\n",
    "- author: Abhishek Kumar Dubey\n",
    "- image: images/CS6660_images/2022-08-13-CS6660-week2-preview.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Measures \n",
    "- Classification\n",
    "    - It is a measure of being  right/wrong,0-1, eg:  hinge loss, cross entropy loss \n",
    "- Regression loss\n",
    "    - It is a measure if how close we are to target, eg:  MEA, MES\n",
    "- Ranking/search\n",
    "    - It is a measure of top K search\n",
    "- Clustering \n",
    "    - How well we have described the data ( not straight forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is accuracy adequate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy may not not be useful in cases where:\n",
    "- There is a large class skew.\n",
    "- There are differential misclassification cost, say getting a positive wrong costs more than getting a negative wrong.\n",
    "- we are most interested in a subset of high confidence predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](CS5590_images/chrome_CNeo323E9V.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tips: <br><br> True  positive rate also called sensitivity, recall, hit rate <br> Specificity = 1 - False Alarm<br> sensitivity = Probability of positive test given a patient has a disease.<br> Specificity = Probability of a negative test given a patient is well.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility and cost\n",
    "- Detection Cost:\n",
    "    - cost = $C_{\\mathrm{FP}} \\times \\mathrm{FP}+C_{\\mathrm{FN}} \\times \\mathrm{FN}$\n",
    "- Fmeasure\n",
    "    - $F1=\\frac{2\\times \\left(\\mathrm{Recall}\\times \\mathrm{Precision}\\right)}{\\mathrm{Recall}+\\mathrm{Precision}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curve\n",
    "- Receiver Operative Curve\n",
    "- Plot between True positive rate on y axis and False positive rate on x axis \n",
    "- AUC : Area under the curve, higher the area, better the performance\n",
    "\n",
    "![](CS5590_images/MATLAB_eYMZjl3LEJ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision Recall Curve\n",
    "- Plot between Precision on y axis and recall (TPR) on x axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other performance measures\n",
    "- Kullback-Leibler Diverfence : $D_{\\mathrm{KL}} \\left(P\\|Q\\right)=\\sum_i P\\left(i\\right)\\log \\frac{P\\left(i\\right)}{Q\\left(i\\right)}$\n",
    "- Gini Statistic : $2\\times \\mathrm{AUC}-1$\n",
    "- F1 score: $\\frac{2\\times \\left(\\mathrm{Recall}\\times \\mathrm{Precision}\\right)}{\\mathrm{Recall}+\\mathrm{Precision}}$\n",
    "- Akaike Information Criterion (AIC): $2k-2\\ln \\left(L\\right)$, here  $k$ is number of model parameters, L is max value of the Likelihood  function for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important points\n",
    "- Randomization of data is essential so that held-aside test data can be really representative of new data. \n",
    "- Test set should never be used in any way for normalization, hyper parameter tuning etc.\n",
    "- Any preprocessing done over entire data set ( feature selection, parameter tuning, threshold selection ) must not use labels from test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic idea\n",
    "- If it walks like a duck, quacks like a duck , then it's probably a duck.\n",
    "- If data points are represented well then KNN works well.\n",
    "- choosing K is important, if K is too small then it becomes sensitive to noise point. If k is too large, neighborhood may incudes points from other class.\n",
    "- Euclidean distance between two instance $d\\left(X_i ,X_j \\right)=\\sqrt{\\sum_{r=1}^n {\\left(a_r \\left(X_i \\right)-a_{r\\left(X_j \\right)} \\right)}^2 }$ here $a_i \\left(X\\right)\\;$ denotes features.\n",
    "- In case of continuous valued target function, Mean value of K nearest training examples is taken\n",
    "\n",
    "### How to determinke K\n",
    "- experiment with different value of K starting form 1 on test set to validate the error, in case of binary classification use odd number for k to avoid ties.<br>\n",
    "> KNN is a transductive method, there is no training involved , it is refereed as Lazy learning, Learning is just storing all the training instances <br> \n",
    "> Similar Keywords: KNN, Memory Based Reasoning, Example Based Reasoning, Instance Based Learning, Case Based Reasoning, Lazy Learning<br>\n",
    "> Voronoi Diagram: Decision surface formed by the training Examples for 1 nearest neighbors classifier<br> \n",
    "### Improvements\n",
    "- Distance weighted Nearest Neighbors\n",
    "- Scaling (normalization) attributes for fair computation fo distances \n",
    "- Measure \"closeness\" differently\n",
    "- Finding \"close\" example in large training set quickly , eg Efficient memory indexing using kd-tree\n",
    "### Pros\n",
    "- Highly effective transductive inference method for noisy training data and complex target functions.\n",
    "- Target function for a whole space may be described as a combinations of less complex local  approximations \n",
    "- Trains very fast (Lazy Learner)\n",
    "### Cons\n",
    "- Curse of dimensionality \n",
    "- Storage: all training example are saved in memory \n",
    "- slow at query time, can be overcome by  pre sorting and indexing training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence of 1-NN\n",
    "$P\\left(\\mathrm{KNNError}\\right)=2\\left(\\mathrm{Bayes}\\;\\mathrm{Optimal}\\;\\mathrm{Error}\\;\\mathrm{Rate}\\right)$ , Probability of K NN error is at most twice the bayes optimal error, bayes optimal error is best(least) error we can get using machine learning <br>\n",
    "It is Possible to show that: as the size of training data set approaches infinity, the one nearest neighbor classifier guarantees an error rate of no worse than twice the bayes error rate ( the minimum achievable error rate given the distribution of the data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density Estimation using KNN\n",
    "Non parametric Density Estimation using KNN. <br>\n",
    "nstead of fixing bin width h and counting the mumber of instances, fix the instances(neighbors) k and check bin width $\\hat{p} \\left(X\\right)=\\frac{k}{2{\\mathrm{Nd}}_k \\left(X\\right)}$ here $d_k \\left(X\\right)$ is the $k_{\\mathrm{th}}$ closest distance to to $X$ , This is also known as __Parzen density estimation__."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a54084e6b208ee8d1ce3989ffc20924477a5f55f5a43e22e699a6741623861e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
