{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks  CS5590 week 5\n",
    "> Foundations of Machine Learning CS5590  week 5\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Foundations of Machine Learning ]\n",
    "- author: Abhishek Kumar Dubey\n",
    "- image: images/CS5590_images/Acrobat_EZY9SgFMeU.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deep learning : A sub area of machine learning, that is today understood as representation learning.\n",
    "- Inspired by the human brain.\n",
    "- How do Neural Networks learn:\n",
    "    - We initialize the weights with random value.\n",
    "    - Then present a trining patter to the network.\n",
    "    - Feed it through tho get output. (feed forward)\n",
    "    - compare with target output.\n",
    "    - Adjust weights based on the error.\n",
    "    - And so on ...\n",
    "- Deep learning models can learn complex decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptrons  (Linear Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Consider the below pic:<br><br>\n",
    "![](../images/CS5590_images/Acrobat_EZY9SgFMeU.png)<br><br>\n",
    "Mathematical formulation is given as below:<br>\n",
    "$$\\displaystyle  z = \\left\\lbrace \n",
    " \\begin{array}{ccc}\n",
    "    1 & \\text{if} & \\displaystyle  \\sum_{i=1}^n x_iw_i \\ge \\theta \\\\\n",
    "    0 & \\text{if} & \\displaystyle  \\sum_{i=1}^n x_iw_i < \\theta\n",
    "\\end{array} \\right.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Learn weight such that the objective function is maximized.\n",
    "- Loss calculation : <br>\n",
    "  $\\Delta W_i = c(t-z)X_i$ <br>\n",
    "  where $W_i$ is the weight from input $i$ to perceptron node, $c$ is the learning rate, $t_j$ is the target for the current instance, $z$ is the current output, and $X_i$ is $i^{th}$ input\n",
    "- Least perturbation principle \n",
    "  - only change weights if there is an error\n",
    "  - small $c$ sufficient to make current pattern corret\n",
    "  - scale by $X_i$\n",
    "- create a perceptron node with $n$ inputs.\n",
    "- Iteratively apply a pattern from the training set and apply the perceptron rule\n",
    "- Each iteration through the training set is an epoch \n",
    "- continue training until total training set error ceases to improve \n",
    "- Peceptron Convergence Theorem : Guaranteed to find a solution in finite time if a solution exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptrons  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP From PRML book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Network with inputs, one hidden unit and outputs:<br><br>\n",
    "![](../images/CS5590_images/Acrobat_aQFGImBqkl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The output of the above network can be given as follows: <br> <br>\n",
    "$\\displaystyle y_{k}({\\bf x},{\\bf w})=\\sigma\\left(\\sum_{j=1}^{M}w_{k j}^{(2)}h\\left(\\sum_{i=1}^{D}w_{j i}^{(1)}x_{i}+w_{j0}^{(1)}\\right)+w_{k0}^{(2)}\\right)$<br> <br>\n",
    "This equation is also interpreted as _forward propagation_ fo information through the newwork.<br>\n",
    "It should be emphasized that these diagrams do not represent probabilistic graphical models, because the internal nodes represent deterministic variables rather than stochastic ones.<br><br>\n",
    "- The above equation can be written as below if bias is absorbed into the set of weight by defining additional input variable $x_0$ whose value is clamped at $x_0=1$ <br><br>\n",
    "$\\displaystyle  y_{k}({\\bf x},{\\bf w})=\\sigma\\left(\\sum_{j=0}^{M}w_{k j}^{(2)}h\\left(\\sum_{i=0}^{D}w_{j i}^{(1)}x_{i}\\right)\\right)$<br><br>\n",
    "- A key difference among neural network and  perceptron,  is that the neural network uses continuous sigmoidal non-linearities in the hidden units, whereas the perceptron uses step-function non-linearities.\n",
    "- If the activation functions of all the hidden units in a network are taken to be linear, then for any such network we can always find an equivalent network without hidden units.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In principle, a network with sigmoidal hidden units can always mimic skip layer connections by using a sufficiently small first-layer weight that, over its operating range.\n",
    "- In practice, however, it may be advantageous to include skip-layer connections explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Form lecture PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extension of perceptrons to multiple layers\n",
    "    1. Initialize network with random weights \n",
    "    2. For all training cases ( called examples):\n",
    "        - present training inputs to network and calculate output\n",
    "        - for all layers (starting with output layer, back to input layer):\n",
    "            - compare network output with correct putput\n",
    "            - Adapt weight in current layer\n",
    "- Method for _Learning Weights_ in feed forward nets\n",
    "    - Can't use Perceptron Rule\n",
    "        - No teacher values (loss) are possible for hidden units.\n",
    "    - Use Gradient decent to minimize the error\n",
    "        - Propagate the deltas to adjust for errors\n",
    "        - Backward from outputs to hidden layers to inputs \n",
    "        - The algorithm can be summarized as follows:\n",
    "            - Computes the error term for the output units using the observed error.\n",
    "            - From output layer , repeat\n",
    "                - Propagating the error term back to the previous layer and updating the weights between the two layers until the earliest layer is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Algorithm in detail:\n",
    "    - Initialize weights (typically random)\n",
    "    - Keep doing epoch\n",
    "        - For each example $e$ in the training set do\n",
    "            - __Forward Pass__ to compute\n",
    "                - $y = $ neural new output (network , $e$) \n",
    "                - miss = $(T-y)$ at each output unit\n",
    "            - __backward pass__ to calculate deltas to weights \n",
    "            - update all weights\n",
    "        - end\n",
    "    - until tuning set error stops improving \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation From PRML book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Think of the N weights as a point in an N-dimensional space<br><br>\n",
    "![](../images/CS5590_images/Acrobat_o3ufCNSWL5.png)<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Many error functions fo practical interest, comprise a sum of terms, onw for each data point in training set, so that <br>\n",
    "$\\displaystyle E(\\mathbf{w})=\\sum_{n=1}^{N}E_{n}(\\mathbf{w})$\n",
    "- Error function for one particular input patter $n$ takes the form <br>\n",
    "$\\displaystyle E_{n}={\\frac{1}{2}}\\sum_{k}(y_{n k}-t_{n k})^{2}$ where $y_{n k}=y_{k}(\\mathbf{x}_{n},\\mathbf{w})$\n",
    "- The gradient of this error with respect to a weight $w_{ji}$ is given by <br>\n",
    "$\\displaystyle {\\frac{\\partial E_{n}}{\\partial w_{j i}}}=(y_{n j}-t_{n j})x_{n i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In a general feed-forward network, each unit computes a weighted sum of its inputs of the form: <br>\n",
    "$\\displaystyle a_{j}=\\sum_{i}w_{j i}z_{i}$<br>\n",
    "where $z_i$ is the activation of a unit, or input, that sends a connection to unit $j$, and $w_{ji}$ is the weight associated with that connection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A non-linear activation function $h(.)$ transforms $a_j$ to produce $z_j$ of unit $j$ in the form <br>\n",
    "$z_{j}=h(a_{j})$<br>\n",
    "Note $z_i$ in equation  $a_{j}=\\sum_{i}w_{j i}z_{i}$ could be an input, and the unit $j$ in  equation $z_{j}=h(a_{j})$ could be an output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we consider to evaluate derivative of $E_n$ with respect to $w_{ji}$, $E_n$ depends on the weight $w_{ji}$ only via the summed input $a_j$ to unit $j$. Applying chain rule for partial derivatives we get <br>\n",
    "$\\displaystyle \\frac{\\partial E_{n}}{\\partial w_{j i}}=\\frac{\\partial E_{n}}{\\partial a_{j}}\\frac{\\partial a_{j}}{\\partial w_{j i}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Consider a useful notation <br>\n",
    "$\\displaystyle \\delta_{j}\\equiv\\frac{\\partial{ E}_{n}}{\\partial a_{j}}$\n",
    "- we can find derivative of $a_j$ with respect to $w_{ji}$ using $ a_{j}=\\sum_{i}w_{j i}z_{i}$, we get <br>\n",
    "$\\displaystyle {\\frac{\\partial a_{j}}{\\partial w_{j i}}}=z_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using above three bullet points we get <br>\n",
    "$\\displaystyle \\frac{\\partial E_{n}}{\\partial w_{j i}}=\\delta_{j}z_{i}$<br>\n",
    "__This tells us required derivative is obtained simply by multiplying the value of $\\delta$ for the unit at the output end of the weight by the value of $z$ for the unit at the input end of the weight.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For output unit we have <br>\n",
    "$\\delta_k = y_k - t_k$ <br>\n",
    "- For hidden units, we again make use of chain rule for partial derivatives<br><br>\n",
    "![](../images/CS5590_images/Acrobat_HejF7tOCqF.png)<br>\n",
    "$\\displaystyle \\delta_{j}\\equiv\\frac{\\partial E_{n}}{\\partial a_{j}}=\\sum_{k}\\frac{\\partial E_{n}}{\\partial a_{k}}\\frac{\\partial a_{k}}{\\partial a_{j}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we know that <br><br>\n",
    "$\\displaystyle \\frac{\\partial E_{n}}{\\partial a_{k}} = \\delta _k$<br><br>\n",
    "also, $ a_{k}=\\sum_{j}w_{k j}z_{j}$ and $z_{j}=h(a_{j})$ <br><br>\n",
    "so $\\displaystyle \\frac{\\partial a_{k}}{\\partial a_{j}} =  \\frac{\\partial \\sum_{j}w_{k j}h(a_{j})}{\\partial a_{j}} = w_{k j} h^\\prime(a_{j})  $ <br><br>\n",
    "putting value of $\\frac{\\partial E_{n}}{\\partial a_{k}}$ and $ \\frac{\\partial a_{k}}{\\partial a_{j}} $ in the equation of above bullet point we get <br><br>\n",
    " $\\displaystyle \\delta_{j}=h^{\\prime}(a_{j})\\sum_{k}w_{k j}\\delta_{k}$<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In short what we discussed till now <br><br>\n",
    "$\\displaystyle  \\frac{\\partial E_{n}}{\\partial w_{j i}}=\\frac{\\partial E_{n}}{\\partial a_{j}}\\frac{\\partial a_{j}}{\\partial w_{j i}} $<br><br>\n",
    "$\\displaystyle  \\frac{\\partial E_{n}}{\\partial w_{j i}}= \\left(  \\sum_{k}{\\frac{\\partial E_{n}}{\\partial a_{k}}}{\\frac{\\partial a_{k}}{\\partial a_{j}}}  \\right)   \\frac{\\partial a_{j}}{\\partial w_{j i}} $<br><br>\n",
    "$\\displaystyle  \\frac{\\partial E_{n}}{\\partial w_{j i}}= \\left(  \\sum_{k}{\\frac{\\partial E_{n}}{\\partial a_{k}}}\\left( w_{k j}h^{\\prime}(a_{j}) \\right)  \\right)   \\frac{\\partial a_{j}}{\\partial w_{j i}} $<br><br>\n",
    "$\\displaystyle  \\frac{\\partial E_{n}}{\\partial w_{j i}}= h^{\\prime}(a_{j})\\left(  \\sum_{k}{\\frac{\\partial E_{n}}{\\partial a_{k}}}w_{k j}    \\right)   \\frac{\\partial a_{j}}{\\partial w_{j i}} $<br><br>\n",
    "$\\displaystyle  \\frac{\\partial E_{n}}{\\partial w_{j i}}= h^{\\prime}(a_{j})\\left(  \\sum_{k}{\\frac{\\partial E_{n}}{\\partial a_{k}}}w_{k j}    \\right)  z_i $<br><br>\n",
    "$\\displaystyle  \\frac{\\partial E_{n}}{\\partial w_{j i}}= h^{\\prime}(a_{j}) z_i \\left(  \\sum_{k}{\\frac{\\partial E_{n}}{\\partial a_{k}}}w_{k j}    \\right)   $<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In sort what we discussed till now with little more elaboration <br><br>\n",
    "$\\displaystyle  y_{k}({\\bf x},{\\bf w})= \\overbrace{\\sigma \\left(  \\underbrace{ \\sum_{j=0}^{M}w_{k j}^{(2)} \\times  \\overbrace{h\\left(\\underbrace{\\sum_{i=0}^{D}w_{j i}^{(1)} \\times z_{i}}_{a_j} \\right)}^{z_j}}_{a_k} \\right)}^{z_k} $<br><br>\n",
    "$\\displaystyle  y_{k}({\\bf x},{\\bf w})= \\overbrace{ \\underbrace{\\sigma}_{\\text{activation fucntion at output }}  \\left(  \\underbrace{ \\sum_{j=0}^{M}w_{k j}^{(2)} \\times  \\overbrace{ \\underbrace{h}_{\\text{activation fucntion at hidden}}  \\left(\\underbrace{\\sum_{i=0}^{D}w_{j i}^{(1)} \\times \\overbrace{z_{i}}^{\\text{from last layer or input }x_i} }_{a_j} \\right)}^{z_j}}_{a_k} \\right)}^{z_k} $<br><br>\n",
    "$\\displaystyle E_{n}={\\frac{1}{2}}\\sum_{k}(y_{k}-t_{k})^{2}$<br><br>\n",
    "Till now we had below expression : <br><br>\n",
    "$\\displaystyle  \\frac{\\partial E_{n}}{\\partial w_{j i}}= \\left(  \\sum_{k} \\frac{\\partial E_{n}}{\\partial a_{k}} {\\frac{\\partial a_{k}}{\\partial a_{j}}}  \\right)   \\frac{\\partial a_{j}}{\\partial w_{j i}} $<br><br>\n",
    "But if we consider activation function $\\sigma $ at out layer  and $y_k = z_k = \\sigma (a_k)$ we get  : <br><br>\n",
    "$\\displaystyle  \\frac{\\partial E_{n}}{\\partial w_{j i}}= \\left(  \\sum_{k} \\frac{\\partial E_{n}}{\\partial z_{k}}  {\\frac{\\partial z_{k}}{\\partial a_{k}}} {\\frac{\\partial a_{k}}{\\partial a_{j}}}  \\right)   \\frac{\\partial a_{j}}{\\partial w_{j i}} $<br><br>\n",
    "$\\displaystyle  \\frac{\\partial E_{n}}{\\partial w_{j i}}= \\left(  \\sum_{k} \\frac{\\partial E_{n}}{\\partial z_{k}}  {\\frac{\\partial z_{k}}{\\partial a_{k}}} {\\frac{\\partial \\sum_{j}w_{k j}z_{j}}{\\partial a_{j}}}  \\right)   \\frac{\\partial a_{j}}{\\partial w_{j i}} $<br><br>\n",
    "$\\displaystyle  \\frac{\\partial E_{n}}{\\partial w_{j i}}= \\left(  \\sum_{k} \\frac{\\partial E_{n}}{\\partial z_{k}}  {\\frac{\\partial z_{k}}{\\partial a_{k}}} {\\frac{\\partial \\sum_{j}w_{k j}z_{j}}{\\partial a_{j}}}  \\right)   \\frac{\\partial a_{j}}{\\partial w_{j i}} $<br><br>\n",
    "$\\displaystyle  \\frac{\\partial E_{n}}{\\partial w_{j i}}= \\left(  \\sum_{k} \\frac{\\partial E_{n}}{\\partial z_{k}}  {\\frac{\\partial z_{k}}{\\partial a_{k}}} {\\frac{\\partial \\sum_{j}w_{k j}h(a_{j})}{\\partial a_{j}}}  \\right)   \\frac{\\partial a_{j}}{\\partial w_{j i}} $<br><br>\n",
    "$\\displaystyle  \\frac{\\partial E_{n}}{\\partial w_{j i}}= \\left(  \\sum_{k} \\frac{\\partial E_{n}}{\\partial z_{k}}  {\\frac{\\partial z_{k}}{\\partial a_{k}}} w_{kj}h^\\prime(a_j) \\right)   \\frac{\\partial a_{j}}{\\partial w_{j i}} $<br><br>\n",
    "$\\displaystyle  \\frac{\\partial E_{n}}{\\partial w_{j i}}= \\left(  \\sum_{k} \\frac{\\partial E_{n}}{\\partial z_{k}}  {\\frac{\\partial z_{k}}{\\partial a_{k}}} w_{kj} \\right)  h^\\prime(a_j)  \\frac{\\partial a_{j}}{\\partial w_{j i}} $<br><br>\n",
    "$\\displaystyle  \\frac{\\partial E_{n}}{\\partial w_{j i}}= \\left(  \\sum_{k} \\frac{\\partial E_{n}}{\\partial z_{k}}  {\\frac{\\partial z_{k}}{\\partial a_{k}}} w_{kj} \\right)  h^\\prime(a_j)  \\frac{\\partial \\sum_{i}w_{j i}z_{i}}{\\partial w_{j i}} $<br><br>\n",
    "$\\displaystyle  \\frac{\\partial E_{n}}{\\partial w_{j i}}= \\left(  \\sum_{k} \\frac{\\partial E_{n}}{\\partial z_{k}}  {\\frac{\\partial z_{k}}{\\partial a_{k}}} w_{kj} \\right)  h^\\prime(a_j)  z_i $<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since $z_k$ is same as $y_k$, we can replace $z_k$ with $y_k$ in above equation  <br> <br>\n",
    "$\\displaystyle  \\frac{\\partial E_{n}}{\\partial w_{j i}}= \\left(  \\sum_{k} {\\frac{\\partial E_{n}}{\\partial y_{k}}}  {\\frac{\\partial y_{k}}{\\partial a_{k}}} w_{kj} \\right)  h^\\prime(a_j)  z_i $<br><br>\n",
    "we know $y_k=z_k=\\sigma (a_k)$ so we get <br><br>\n",
    "$\\displaystyle  \\frac{\\partial E_{n}}{\\partial w_{j i}}= \\left(  \\sum_{k} {\\frac{\\partial E_{n}}{\\partial y_{k}}}  {\\frac{\\partial \\sigma (a_k)}{\\partial a_{k}}} w_{kj} \\right)  h^\\prime(a_j)  z_i $<br><br>\n",
    "$\\displaystyle  \\frac{\\partial E_{n}}{\\partial w_{j i}}= \\left(  \\sum_{k} {\\frac{\\partial E_{n}}{\\partial y_{k}}}  {\\frac{\\partial \\sigma (a_k)}{\\partial a_{k}}} w_{kj} \\right)  h^\\prime(a_j)  z_i $<br><br>\n",
    "$\\displaystyle  \\frac{\\partial E_{n}}{\\partial w_{j i}}= \\underbrace{\\left(  \\sum_{k} \\underbrace{\\frac{\\partial E_{n}}{\\partial y_{k}}}_{(y_k-t_k)} \\times  \\overbrace{ \\frac{\\partial \\sigma (a_k)}{\\partial a_{k}}}^{z_k(1-z_k) \\text{ or }y_k(1-y_k)} \\times  w_{kj} \\right)}_{\\text{miss}}   \\underbrace{h^\\prime(a_j) }_{ z_j(1-z_j)}  z_i $<br><br>\n",
    "$\\displaystyle  \\frac{\\partial E_{n}}{\\partial w_{j i}}=  \\left(  \\sum_{k}  (y_k-t_k) y_k (1-y_k)w_{kj} \\right) z_j(1-z_j)z_i $ <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: <br><br> $\\displaystyle  y_{k}({\\bf x},{\\bf w})= \\overbrace{ \\underbrace{\\sigma}_{\\text{activation fucntion at output }}  \\left(  \\underbrace{ \\sum_{j=0}^{M}w_{k j}^{(2)} \\times  \\overbrace{ \\underbrace{h}_{\\text{activation fucntion at hidden}}  \\left(\\underbrace{\\sum_{i=0}^{D}w_{j i}^{(1)} \\times \\overbrace{z_{i}}^{\\text{from last layer or input }x_i} }_{a_j} \\right)}^{z_j}}_{a_k} \\right)}^{z_k} $<br><br>$\\displaystyle  \\frac{\\partial E_{n}}{\\partial w_{j i}}=  \\left(  \\sum_{k}  (y_k-t_k) y_k (1-y_k)w_{kj} \\right) z_j(1-z_j)z_i $ \n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Error Backpropagation summery :\n",
    "    1. Apply an input vector $x_n$ to the network and forward propagate through the network using below two equations  to find the activations of all the hidden and output units. <br>\n",
    "    $\\displaystyle a_{j}=\\sum_{i}w_{j i}z_{i}$ <br>\n",
    "    $z_j = h(a_j)$\n",
    "    2. Evaluate the $\\delta _k $ for all the output units using  $\\delta_K = y_k - t_k$\n",
    "    3. Backpropagete all the $\\delta$ using below equation to find $\\delta _j$ for each hidden unit in the network <br>\n",
    "    $\\displaystyle \\delta_{j}=h^{\\prime}(a_{j})\\sum_{k}w_{k j}\\delta_{k}$<br>\n",
    "    4. Use below equation to evaluate the required derivatives<br>\n",
    "    $\\displaystyle \\frac{\\partial E_{n}}{\\partial w_{j i}}=\\delta_{j}z_{i}$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For batch methods, the derivative of the total error $E$ can then be obtained by repeating the above steps for each pattern in the training set and then summing over all patterns:<br>\n",
    "$\\displaystyle \\frac{\\partial E}{\\partial w_{j i}}=\\sum_{n}\\frac{\\partial E_{n}}{\\partial w_{j i}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Form lecture PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also has same points as per PRML but from different angle. But __DIFFERENT TERMINOLOGY IS USED HERE SO BE CAREFUL__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>   \n",
    "$$  \n",
    "\\mathbf{\\text{Alert !!!}}\n",
    "$$\n",
    "\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __SOME NOTATION IS WRONG IN THIS SECTION. DO NOT READ THIS SECTION, ALL CONCEPTS ARE ALREADY EXPLAINED__ <br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Terminology <br>\n",
    "$g$ is activation function <br>\n",
    "$y=g(z)$ <br>\n",
    "$E = (t_i-y_i)^2$ <br>\n",
    "$z_i = a_j\\times w_{ij}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Add a dimension for the observed error\n",
    "- Try to minimize your position on the \"error surface\"\n",
    "- Compute : <br>\n",
    "  $\\text{Grad}_E = \\left[ \\frac{dE}{dW_1},\\frac{dE}{dW_2},\\dots , \\frac{dE}{dW_n}  \\right]$\n",
    "- Change $i_{th}$ weight by <br>\n",
    "  $\\Delta W_i = -\\alpha \\frac{dE}{dW_i}$\n",
    "- We also use activation function at the end of every node.\n",
    "- consider $g(z)=y$ where $g$ is sigmoid activation function.\n",
    "- $g'(z)=g(z)\\times (1-g(z))=y(1-y)$\n",
    "- Activation function must be continuous, differential, non-decreasing, and easy to compute.\n",
    "- We want activation function to be non-decreasing because, so that it should not increase value in some reason and decrease it in some other reason. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Updating Hidden-to-Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Updating Hidden-to-Output<br><br>\n",
    "  $\\frac{\\partial E}{\\partial W_ij}= \\frac{\\partial E}{\\partial y} \\times \\frac{\\partial y}{\\partial z} \\times \\frac{\\partial z}{\\partial W_ij}$<br><br>\n",
    "  $\\Delta W_{ij} = \\alpha \\times \\underbrace{(t_i -y_i)}_{\\frac{\\partial E}{\\partial y} }  \\times \\underbrace{g'(z_i)}_{\\frac{\\partial y}{\\partial z} }  \\times \\underbrace{a_j}_{\\frac{\\partial z}{\\partial W_ij}} $ <br><br>\n",
    "  $\\Delta W_{ij} = \\overbrace{\\alpha}^{\\text{learning rate}}  \\times \\underbrace{(\\overbrace{t_i}^{\\text{Teacher supplied}}  -y_i)}_{\\text{miss}}  \\times \\overbrace{g'(z_i)}^{\\text{derivatve of acitvation function}} \\times \\underbrace{a_j}_{\\text{previous layer output}} $<br><br>\n",
    "  $\\Delta W_{ij} = \\alpha \\times (t_i -y_i) \\times y_i \\times (1-y_i)\\times a_j$ <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating interior weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Updating interior weights<br>\n",
    "  Layer $k$ units provide values to all layers $k+1$ units.<br>\n",
    "  \"miss\" is sum of the misses from all units on $k+1$<br><br>\n",
    "  $\\displaystyle \\text{miss}_j = \\sum \\left[ a_j(1-a_j)(t_i-a_j)w_{ji} \\right]$<br><br>\n",
    "  $\\displaystyle  \\frac{\\partial E}{\\partial W_kj}= \\left( \\sum  \\frac{\\partial E}{\\partial y_i} \\times \\frac{\\partial y_i}{\\partial z_i} \\times \\frac{\\partial z_i}{\\partial a_j}  \\right) \\times \\frac{\\partial a_j}{\\partial l_j} \\times \\frac{\\partial l_j}{\\partial W_ij}$<br><br>\n",
    "  $\\displaystyle  \\frac{\\partial E}{\\partial W_kj}= \\left( \\sum  \\underbrace{\\frac{\\partial E}{\\partial y_i}}_{t_i-y_i}  \\times \\overbrace{\\frac{\\partial y_i}{\\partial z_i}}^{y_i(1-y_i)}  \\times \\underbrace{\\frac{\\partial z_i}{\\partial a_j}}_{w_ji}   \\right) \\times \\underbrace{\\frac{\\partial a_j}{\\partial l_j}}_{a_j(1-a_j)}  \\times \\overbrace{\\frac{\\partial l_j}{\\partial W_ij}}^{l_k} $<br><br>\n",
    "  $\\displaystyle  \\frac{\\partial E}{\\partial W_kj} = \\left(  \\sum y_i  \\times (1-y_i)  \\times (t_i-y_i) \\times  w_{ji}\\right) \\times  l_k  \\times a_j  \\times (1-a_j) $<br><br>\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b92a85dd111e7f234cbdfd9e3dc75ce12717f6161daa18a82d26f0e8e840c205"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
