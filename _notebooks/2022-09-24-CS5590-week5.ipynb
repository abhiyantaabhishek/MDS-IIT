{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks  CS5590 week 5\n",
    "> Foundations of Machine Learning CS5590  week 5\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Foundations of Machine Learning ]\n",
    "- author: Abhishek Kumar Dubey\n",
    "- image: images/CS5590_images/Acrobat_EZY9SgFMeU.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deep learning : A sub area of machine learning, that is today understood as representation learning.\n",
    "- Inspired by the human brain.\n",
    "- How do Neural Networks learn:\n",
    "    - We initialize the weights with random value.\n",
    "    - Then present a trining patter to the network.\n",
    "    - Feed it through tho get output. (feed forward)\n",
    "    - compare with target output.\n",
    "    - Adjust weights based on the error.\n",
    "    - And so on ...\n",
    "- Deep learning models can learn complex decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptrons  (Linear Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Consider the below pic:<br><br>\n",
    "![](../images/CS5590_images/Acrobat_EZY9SgFMeU.png)<br><br>\n",
    "Mathematical formulation is given as below:<br>\n",
    "$$\\displaystyle  z = \\left\\lbrace \n",
    " \\begin{array}{ccc}\n",
    "    1 & \\text{if} & \\displaystyle  \\sum_{i=1}^n x_iw_i \\ge \\theta \\\\\n",
    "    0 & \\text{if} & \\displaystyle  \\sum_{i=1}^n x_iw_i < \\theta\n",
    "\\end{array} \\right.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Learn weight such that the objective function is maximized.\n",
    "- Loss calculation : <br>\n",
    "  $\\Delta W_i = c(t-z)X_i$ <br>\n",
    "  where $W_i$ is the weight from input $i$ to perceptron node, $c$ is the learning rate, $t_j$ is the target for the current instance, $z$ is the current output, and $X_i$ is $i^{th}$ input\n",
    "- Least perturbation principle \n",
    "  - only change weights if there is an error\n",
    "  - small $c$ sufficient to make current pattern corret\n",
    "  - scale by $X_i$\n",
    "- create a perceptron node with $n$ inputs.\n",
    "- Iteratively apply a pattern from the training set and apply the perceptron rule\n",
    "- Each iteration through the training set is an epoch \n",
    "- continue training until total training set error ceases to improve \n",
    "- Peceptron Convergence Theorem : Guaranteed to find a solution in finite time if a solution exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptrons  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extension of perceptrons to multiple layers\n",
    "    1. Initialize network with random weights \n",
    "    2. For all training cases ( called examples):\n",
    "        - present training inputs to network and calculate output\n",
    "        - for all layers (starting with output layer, back to input layer):\n",
    "            - compare network output with correct putput\n",
    "            - Adapt weight in current layer\n",
    "- Method for _Learning Weights_ in feed forward nets\n",
    "    - Can't use Perceptron Rule\n",
    "        - No teacher values (loss) are possible for hidden units.\n",
    "    - Use Gradient decent to minimize the error\n",
    "        - Propagate the deltas to adjust for errors\n",
    "        - Backward from outputs to hidden layers to inputs \n",
    "        - The algorithm can be summarized as follows:\n",
    "            - Computes the error term for the output units using the observed error.\n",
    "            - From output layer , repeat\n",
    "                - Propagating the error term back to the previous layer and updating the weights between the two layers until the earliest layer is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Algorithm in detail:\n",
    "    - Initialize weights (typically random)\n",
    "    - Keep doing epoch\n",
    "        - For each example $e$ in the training set do\n",
    "            - __Forward Pass__ to compute\n",
    "                - $y = $ neural new output (network , $e$) \n",
    "                - miss = $(T-y)$ at each output unit\n",
    "            - __backward pass__ to calculate deltas to weights \n",
    "            - update all weights\n",
    "        - end\n",
    "    - until tuning set error stops improving \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Think of the N weights as a point in an N-dimensional space<br><br>\n",
    "  ![](../images/CS5590_images/Acrobat_o3ufCNSWL5.png)<br><br>\n",
    "- Add a dimension for the observed error\n",
    "- Try to minimize your position on the \"error surface\"\n",
    "- Compute : <br>\n",
    "  $\\text{Grad}_E = \\left[ \\frac{dE}{dW_1},\\frac{dE}{dW_2},\\dots , \\frac{dE}{dW_n}  \\right]$\n",
    "- Change $i_{th}$ weight by <br>\n",
    "  $\\Delta W_i = -\\alpha \\frac{dE}{dW_i}$\n",
    "- Activation function must be continuous, differential, non-decreasing, and easy to compute.\n",
    "- Updating Hidden-to-Output<br><br>\n",
    "  $\\Delta W_i = \\alpha \\times (t_i -y_i) \\times g'(z_i)\\times a_j$<br><br>\n",
    "  $\\Delta W_i = \\overbrace{\\alpha}^{\\text{learning rate}}  \\times \\underbrace{(\\overbrace{t_i}^{\\text{Teacher supplied}}  -y_i)}_{\\text{miss}}  \\times \\overbrace{g'(z_i)}^{\\text{derivatve of acitvation function}} \\times a_j$<br><br>\n",
    "  we know that for sigmoid , $g'(x)=g(x)\\times (1-g(x))$<br><br>\n",
    "  so, $\\Delta W_i = \\alpha \\times (t_i -y_i) \\times y_i \\times (1-y_i)\\times a_j$ <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b92a85dd111e7f234cbdfd9e3dc75ce12717f6161daa18a82d26f0e8e840c205"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
