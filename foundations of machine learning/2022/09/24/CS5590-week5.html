<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Neural Networks CS5590 week 5 | HOME</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Neural Networks CS5590 week 5" />
<meta name="author" content="Abhishek Kumar Dubey" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Foundations of Machine Learning CS5590 week 5" />
<meta property="og:description" content="Foundations of Machine Learning CS5590 week 5" />
<link rel="canonical" href="https://abhiyantaabhishek.github.io/MDS-IIT/foundations%20of%20machine%20learning/2022/09/24/CS5590-week5.html" />
<meta property="og:url" content="https://abhiyantaabhishek.github.io/MDS-IIT/foundations%20of%20machine%20learning/2022/09/24/CS5590-week5.html" />
<meta property="og:site_name" content="HOME" />
<meta property="og:image" content="https://abhiyantaabhishek.github.io/MDS-IIT/images/CS5590_images/Acrobat_EZY9SgFMeU.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-09-24T00:00:00-05:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://abhiyantaabhishek.github.io/MDS-IIT/images/CS5590_images/Acrobat_EZY9SgFMeU.png" />
<meta property="twitter:title" content="Neural Networks CS5590 week 5" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Abhishek Kumar Dubey"},"dateModified":"2022-09-24T00:00:00-05:00","datePublished":"2022-09-24T00:00:00-05:00","description":"Foundations of Machine Learning CS5590 week 5","headline":"Neural Networks CS5590 week 5","image":"https://abhiyantaabhishek.github.io/MDS-IIT/images/CS5590_images/Acrobat_EZY9SgFMeU.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://abhiyantaabhishek.github.io/MDS-IIT/foundations%20of%20machine%20learning/2022/09/24/CS5590-week5.html"},"url":"https://abhiyantaabhishek.github.io/MDS-IIT/foundations%20of%20machine%20learning/2022/09/24/CS5590-week5.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/MDS-IIT/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://abhiyantaabhishek.github.io/MDS-IIT/feed.xml" title="HOME" /><link rel="shortcut icon" type="image/x-icon" href="/MDS-IIT/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/MDS-IIT/">HOME</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/MDS-IIT/abhishek_resume/">About Me</a><a class="page-link" href="/MDS-IIT/search/">Search</a><a class="page-link" href="/MDS-IIT/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Neural Networks  CS5590 week 5</h1><p class="page-description">Foundations of Machine Learning CS5590  week 5</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-09-24T00:00:00-05:00" itemprop="datePublished">
        Sep 24, 2022
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Abhishek Kumar Dubey</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      11 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/MDS-IIT/categories/#Foundations of Machine Learning">Foundations of Machine Learning</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/abhiyantaabhishek/MDS-IIT/tree/master/_notebooks/2022-09-24-CS5590-week5.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/MDS-IIT/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/abhiyantaabhishek/MDS-IIT/master?filepath=_notebooks%2F2022-09-24-CS5590-week5.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/MDS-IIT/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/abhiyantaabhishek/MDS-IIT/blob/master/_notebooks/2022-09-24-CS5590-week5.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/MDS-IIT/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
          <div class="px-2">
  <a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Fabhiyantaabhishek%2FMDS-IIT%2Fblob%2Fmaster%2F_notebooks%2F2022-09-24-CS5590-week5.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/MDS-IIT/assets/badges/deepnote.svg" alt="Launch in Deepnote"/>
  </a>
</div>

        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#Deep-Learning">Deep Learning </a></li>
<li class="toc-entry toc-h1"><a href="#Perceptrons--(Linear-Models)">Perceptrons  (Linear Models) </a></li>
<li class="toc-entry toc-h1"><a href="#Multi-Layer-Perceptrons">Multi Layer Perceptrons </a>
<ul>
<li class="toc-entry toc-h2"><a href="#MLP-From-PRML-book">MLP From PRML book </a></li>
<li class="toc-entry toc-h2"><a href="#MLP-Form-lecture-PDF">MLP Form lecture PDF </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#Error-Backpropagation">Error Backpropagation </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Backpropagation-From-PRML-book">Backpropagation From PRML book </a></li>
<li class="toc-entry toc-h2"><a href="#Backpropagation-Form-lecture-PDF">Backpropagation Form lecture PDF </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Updating-Hidden-to-Output">Updating Hidden-to-Output </a></li>
<li class="toc-entry toc-h3"><a href="#Updating-interior-weights">Updating interior weights </a></li>
</ul>
</li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-09-24-CS5590-week5.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Deep-Learning">
<a class="anchor" href="#Deep-Learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Deep Learning<a class="anchor-link" href="#Deep-Learning"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Deep learning : A sub area of machine learning, that is today understood as representation learning.</li>
<li>Inspired by the human brain.</li>
<li>How do Neural Networks learn:<ul>
<li>We initialize the weights with random value.</li>
<li>Then present a trining patter to the network.</li>
<li>Feed it through tho get output. (feed forward)</li>
<li>compare with target output.</li>
<li>Adjust weights based on the error.</li>
<li>And so on ...</li>
</ul>
</li>
<li>Deep learning models can learn complex decision boundaries.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Perceptrons--(Linear-Models)">
<a class="anchor" href="#Perceptrons--(Linear-Models)" aria-hidden="true"><span class="octicon octicon-link"></span></a>Perceptrons  (Linear Models)<a class="anchor-link" href="#Perceptrons--(Linear-Models)"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Consider the below pic:<br><br>
<img src="/MDS-IIT/images/copied_from_nb/../images/CS5590_images/Acrobat_EZY9SgFMeU.png" alt=""><br><br>
Mathematical formulation is given as below:<br>
$$\displaystyle  z = \left\lbrace 
\begin{array}{ccc}
  1 &amp; \text{if} &amp; \displaystyle  \sum_{i=1}^n x_iw_i \ge \theta \\
  0 &amp; \text{if} &amp; \displaystyle  \sum_{i=1}^n x_iw_i &lt; \theta
\end{array} \right.$$</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Learn weight such that the objective function is maximized.</li>
<li>Loss calculation : <br>
$\Delta W_i = c(t-z)X_i$ <br>
where $W_i$ is the weight from input $i$ to perceptron node, $c$ is the learning rate, $t_j$ is the target for the current instance, $z$ is the current output, and $X_i$ is $i^{th}$ input</li>
<li>Least perturbation principle <ul>
<li>only change weights if there is an error</li>
<li>small $c$ sufficient to make current pattern corret</li>
<li>scale by $X_i$</li>
</ul>
</li>
<li>create a perceptron node with $n$ inputs.</li>
<li>Iteratively apply a pattern from the training set and apply the perceptron rule</li>
<li>Each iteration through the training set is an epoch </li>
<li>continue training until total training set error ceases to improve </li>
<li>Peceptron Convergence Theorem : Guaranteed to find a solution in finite time if a solution exists</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Multi-Layer-Perceptrons">
<a class="anchor" href="#Multi-Layer-Perceptrons" aria-hidden="true"><span class="octicon octicon-link"></span></a>Multi Layer Perceptrons<a class="anchor-link" href="#Multi-Layer-Perceptrons"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="MLP-From-PRML-book">
<a class="anchor" href="#MLP-From-PRML-book" aria-hidden="true"><span class="octicon octicon-link"></span></a>MLP From PRML book<a class="anchor-link" href="#MLP-From-PRML-book"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Network with inputs, one hidden unit and outputs:<br><br>
<img src="/MDS-IIT/images/copied_from_nb/../images/CS5590_images/Acrobat_aQFGImBqkl.png" alt="">
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>The output of the above network can be given as follows: <br> <br>
$\displaystyle y_{k}({\bf x},{\bf w})=\sigma\left(\sum_{j=1}^{M}w_{k j}^{(2)}h\left(\sum_{i=1}^{D}w_{j i}^{(1)}x_{i}+w_{j0}^{(1)}\right)+w_{k0}^{(2)}\right)$<br> <br>
This equation is also interpreted as <em>forward propagation</em> fo information through the newwork.<br>
It should be emphasized that these diagrams do not represent probabilistic graphical models, because the internal nodes represent deterministic variables rather than stochastic ones.<br><br>
</li>
<li>The above equation can be written as below if bias is absorbed into the set of weight by defining additional input variable $x_0$ whose value is clamped at $x_0=1$ <br><br>
$\displaystyle  y_{k}({\bf x},{\bf w})=\sigma\left(\sum_{j=0}^{M}w_{k j}^{(2)}h\left(\sum_{i=0}^{D}w_{j i}^{(1)}x_{i}\right)\right)$<br><br>
</li>
<li>A key difference among neural network and  perceptron,  is that the neural network uses continuous sigmoidal non-linearities in the hidden units, whereas the perceptron uses step-function non-linearities.</li>
<li>If the activation functions of all the hidden units in a network are taken to be linear, then for any such network we can always find an equivalent network without hidden units.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>In principle, a network with sigmoidal hidden units can always mimic skip layer connections by using a sufficiently small first-layer weight that, over its operating range.</li>
<li>In practice, however, it may be advantageous to include skip-layer connections explicitly.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="MLP-Form-lecture-PDF">
<a class="anchor" href="#MLP-Form-lecture-PDF" aria-hidden="true"><span class="octicon octicon-link"></span></a>MLP Form lecture PDF<a class="anchor-link" href="#MLP-Form-lecture-PDF"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Extension of perceptrons to multiple layers<ol>
<li>Initialize network with random weights </li>
<li>For all training cases ( called examples):<ul>
<li>present training inputs to network and calculate output</li>
<li>for all layers (starting with output layer, back to input layer):<ul>
<li>compare network output with correct putput</li>
<li>Adapt weight in current layer</li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
<li>Method for <em>Learning Weights</em> in feed forward nets<ul>
<li>Can't use Perceptron Rule<ul>
<li>No teacher values (loss) are possible for hidden units.</li>
</ul>
</li>
<li>Use Gradient decent to minimize the error<ul>
<li>Propagate the deltas to adjust for errors</li>
<li>Backward from outputs to hidden layers to inputs </li>
<li>The algorithm can be summarized as follows:<ul>
<li>Computes the error term for the output units using the observed error.</li>
<li>From output layer , repeat<ul>
<li>Propagating the error term back to the previous layer and updating the weights between the two layers until the earliest layer is reached.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Algorithm in detail:<ul>
<li>Initialize weights (typically random)</li>
<li>Keep doing epoch<ul>
<li>For each example $e$ in the training set do<ul>
<li>
<strong>Forward Pass</strong> to compute<ul>
<li>$y = $ neural new output (network , $e$) </li>
<li>miss = $(T-y)$ at each output unit</li>
</ul>
</li>
<li>
<strong>backward pass</strong> to calculate deltas to weights </li>
<li>update all weights</li>
</ul>
</li>
<li>end</li>
</ul>
</li>
<li>until tuning set error stops improving </li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Error-Backpropagation">
<a class="anchor" href="#Error-Backpropagation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Error Backpropagation<a class="anchor-link" href="#Error-Backpropagation"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Backpropagation-From-PRML-book">
<a class="anchor" href="#Backpropagation-From-PRML-book" aria-hidden="true"><span class="octicon octicon-link"></span></a>Backpropagation From PRML book<a class="anchor-link" href="#Backpropagation-From-PRML-book"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Think of the N weights as a point in an N-dimensional space<br><br>
<img src="/MDS-IIT/images/copied_from_nb/../images/CS5590_images/Acrobat_o3ufCNSWL5.png" alt=""><br><br>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Many error functions fo practical interest, comprise a sum of terms, onw for each data point in training set, so that <br>
$\displaystyle E(\mathbf{w})=\sum_{n=1}^{N}E_{n}(\mathbf{w})$</li>
<li>Error function for one particular input patter $n$ takes the form <br>
$\displaystyle E_{n}={\frac{1}{2}}\sum_{k}(y_{n k}-t_{n k})^{2}$ where $y_{n k}=y_{k}(\mathbf{x}_{n},\mathbf{w})$</li>
<li>The gradient of this error with respect to a weight $w_{ji}$ is given by <br>
$\displaystyle {\frac{\partial E_{n}}{\partial w_{j i}}}=(y_{n j}-t_{n j})x_{n i}$</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>In a general feed-forward network, each unit computes a weighted sum of its inputs of the form: <br>
$\displaystyle a_{j}=\sum_{i}w_{j i}z_{i}$<br>
where $z_i$ is the activation of a unit, or input, that sends a connection to unit $j$, and $w_{ji}$ is the weight associated with that connection</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>A non-linear activation function $h(.)$ transforms $a_j$ to produce $z_j$ of unit $j$ in the form <br>
$z_{j}=h(a_{j})$<br>
Note $z_i$ in equation  $a_{j}=\sum_{i}w_{j i}z_{i}$ could be an input, and the unit $j$ in  equation $z_{j}=h(a_{j})$ could be an output </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Now we consider to evaluate derivative of $E_n$ with respect to $w_{ji}$, $E_n$ depends on the weight $w_{ji}$ only via the summed input $a_j$ to unit $j$. Applying chain rule for partial derivatives we get <br>
$\displaystyle \frac{\partial E_{n}}{\partial w_{j i}}=\frac{\partial E_{n}}{\partial a_{j}}\frac{\partial a_{j}}{\partial w_{j i}}$</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Consider a useful notation <br>
$\displaystyle \delta_{j}\equiv\frac{\partial{ E}_{n}}{\partial a_{j}}$</li>
<li>we can find derivative of $a_j$ with respect to $w_{ji}$ using $ a_{j}=\sum_{i}w_{j i}z_{i}$, we get <br>
$\displaystyle {\frac{\partial a_{j}}{\partial w_{j i}}}=z_{i}$</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Using above three bullet points we get <br>
$\displaystyle \frac{\partial E_{n}}{\partial w_{j i}}=\delta_{j}z_{i}$<br>
<strong>This tells us required derivative is obtained simply by multiplying the value of $\delta$ for the unit at the output end of the weight by the value of $z$ for the unit at the input end of the weight.</strong>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>For output unit we have <br>
$\delta_k = y_k - t_k$ <br>
</li>
<li>For hidden units, we again make use of chain rule for partial derivatives<br><br>
<img src="/MDS-IIT/images/copied_from_nb/../images/CS5590_images/Acrobat_HejF7tOCqF.png" alt=""><br>
$\displaystyle \delta_{j}\equiv\frac{\partial E_{n}}{\partial a_{j}}=\sum_{k}\frac{\partial E_{n}}{\partial a_{k}}\frac{\partial a_{k}}{\partial a_{j}}$</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Now we know that <br><br>
$\displaystyle \frac{\partial E_{n}}{\partial a_{k}} = \delta _k$<br><br>
also, $ a_{k}=\sum_{j}w_{k j}z_{j}$ and $z_{j}=h(a_{j})$ <br><br>
so $\displaystyle \frac{\partial a_{k}}{\partial a_{j}} =  \frac{\partial \sum_{j}w_{k j}h(a_{j})}{\partial a_{j}} = w_{k j} h^\prime(a_{j})  $ <br><br>
putting value of $\frac{\partial E_{n}}{\partial a_{k}}$ and $ \frac{\partial a_{k}}{\partial a_{j}} $ in the equation of above bullet point we get <br><br>
$\displaystyle \delta_{j}=h^{\prime}(a_{j})\sum_{k}w_{k j}\delta_{k}$<br><br>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>In short what we discussed till now <br><br>
$\displaystyle  \frac{\partial E_{n}}{\partial w_{j i}}=\frac{\partial E_{n}}{\partial a_{j}}\frac{\partial a_{j}}{\partial w_{j i}} $<br><br>
$\displaystyle  \frac{\partial E_{n}}{\partial w_{j i}}= \left(  \sum_{k}{\frac{\partial E_{n}}{\partial a_{k}}}{\frac{\partial a_{k}}{\partial a_{j}}}  \right)   \frac{\partial a_{j}}{\partial w_{j i}} $<br><br>
$\displaystyle  \frac{\partial E_{n}}{\partial w_{j i}}= \left(  \sum_{k}{\frac{\partial E_{n}}{\partial a_{k}}}\left( w_{k j}h^{\prime}(a_{j}) \right)  \right)   \frac{\partial a_{j}}{\partial w_{j i}} $<br><br>
$\displaystyle  \frac{\partial E_{n}}{\partial w_{j i}}= h^{\prime}(a_{j})\left(  \sum_{k}{\frac{\partial E_{n}}{\partial a_{k}}}w_{k j}    \right)   \frac{\partial a_{j}}{\partial w_{j i}} $<br><br>
$\displaystyle  \frac{\partial E_{n}}{\partial w_{j i}}= h^{\prime}(a_{j})\left(  \sum_{k}{\frac{\partial E_{n}}{\partial a_{k}}}w_{k j}    \right)  z_i $<br><br>
$\displaystyle  \frac{\partial E_{n}}{\partial w_{j i}}= h^{\prime}(a_{j}) z_i \left(  \sum_{k}{\frac{\partial E_{n}}{\partial a_{k}}}w_{k j}    \right)   $<br><br>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>In sort what we discussed till now with little more elaboration <br><br>
$\displaystyle  y_{k}({\bf x},{\bf w})= \overbrace{\sigma \left(  \underbrace{ \sum_{j=0}^{M}w_{k j}^{(2)} \times  \overbrace{h\left(\underbrace{\sum_{i=0}^{D}w_{j i}^{(1)} \times z_{i}}_{a_j} \right)}^{z_j}}_{a_k} \right)}^{z_k} $<br><br>
$\displaystyle  y_{k}({\bf x},{\bf w})= \overbrace{ \underbrace{\sigma}_{\text{activation fucntion at output }}  \left(  \underbrace{ \sum_{j=0}^{M}w_{k j}^{(2)} \times  \overbrace{ \underbrace{h}_{\text{activation fucntion at hidden}}  \left(\underbrace{\sum_{i=0}^{D}w_{j i}^{(1)} \times \overbrace{z_{i}}^{\text{from last layer or input }x_i} }_{a_j} \right)}^{z_j}}_{a_k} \right)}^{z_k} $<br><br>
$\displaystyle E_{n}={\frac{1}{2}}\sum_{k}(y_{k}-t_{k})^{2}$<br><br>
Till now we had below expression : <br><br>
$\displaystyle  \frac{\partial E_{n}}{\partial w_{j i}}= \left(  \sum_{k} \frac{\partial E_{n}}{\partial a_{k}} {\frac{\partial a_{k}}{\partial a_{j}}}  \right)   \frac{\partial a_{j}}{\partial w_{j i}} $<br><br>
But if we consider activation function $\sigma $ at out layer  and $y_k = z_k = \sigma (a_k)$ we get  : <br><br>
$\displaystyle  \frac{\partial E_{n}}{\partial w_{j i}}= \left(  \sum_{k} \frac{\partial E_{n}}{\partial z_{k}}  {\frac{\partial z_{k}}{\partial a_{k}}} {\frac{\partial a_{k}}{\partial a_{j}}}  \right)   \frac{\partial a_{j}}{\partial w_{j i}} $<br><br>
$\displaystyle  \frac{\partial E_{n}}{\partial w_{j i}}= \left(  \sum_{k} \frac{\partial E_{n}}{\partial z_{k}}  {\frac{\partial z_{k}}{\partial a_{k}}} {\frac{\partial \sum_{j}w_{k j}z_{j}}{\partial a_{j}}}  \right)   \frac{\partial a_{j}}{\partial w_{j i}} $<br><br>
$\displaystyle  \frac{\partial E_{n}}{\partial w_{j i}}= \left(  \sum_{k} \frac{\partial E_{n}}{\partial z_{k}}  {\frac{\partial z_{k}}{\partial a_{k}}} {\frac{\partial \sum_{j}w_{k j}z_{j}}{\partial a_{j}}}  \right)   \frac{\partial a_{j}}{\partial w_{j i}} $<br><br>
$\displaystyle  \frac{\partial E_{n}}{\partial w_{j i}}= \left(  \sum_{k} \frac{\partial E_{n}}{\partial z_{k}}  {\frac{\partial z_{k}}{\partial a_{k}}} {\frac{\partial \sum_{j}w_{k j}h(a_{j})}{\partial a_{j}}}  \right)   \frac{\partial a_{j}}{\partial w_{j i}} $<br><br>
$\displaystyle  \frac{\partial E_{n}}{\partial w_{j i}}= \left(  \sum_{k} \frac{\partial E_{n}}{\partial z_{k}}  {\frac{\partial z_{k}}{\partial a_{k}}} w_{kj}h^\prime(a_j) \right)   \frac{\partial a_{j}}{\partial w_{j i}} $<br><br>
$\displaystyle  \frac{\partial E_{n}}{\partial w_{j i}}= \left(  \sum_{k} \frac{\partial E_{n}}{\partial z_{k}}  {\frac{\partial z_{k}}{\partial a_{k}}} w_{kj} \right)  h^\prime(a_j)  \frac{\partial a_{j}}{\partial w_{j i}} $<br><br>
$\displaystyle  \frac{\partial E_{n}}{\partial w_{j i}}= \left(  \sum_{k} \frac{\partial E_{n}}{\partial z_{k}}  {\frac{\partial z_{k}}{\partial a_{k}}} w_{kj} \right)  h^\prime(a_j)  \frac{\partial \sum_{i}w_{j i}z_{i}}{\partial w_{j i}} $<br><br>
$\displaystyle  \frac{\partial E_{n}}{\partial w_{j i}}= \left(  \sum_{k} \frac{\partial E_{n}}{\partial z_{k}}  {\frac{\partial z_{k}}{\partial a_{k}}} w_{kj} \right)  h^\prime(a_j)  z_i $<br><br>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Since $z_k$ is same as $y_k$, we can replace $z_k$ with $y_k$ in above equation  <br> <br>
$\displaystyle  \frac{\partial E_{n}}{\partial w_{j i}}= \left(  \sum_{k} {\frac{\partial E_{n}}{\partial y_{k}}}  {\frac{\partial y_{k}}{\partial a_{k}}} w_{kj} \right)  h^\prime(a_j)  z_i $<br><br>
we know $y_k=z_k=\sigma (a_k)$ so we get <br><br>
$\displaystyle  \frac{\partial E_{n}}{\partial w_{j i}}= \left(  \sum_{k} {\frac{\partial E_{n}}{\partial y_{k}}}  {\frac{\partial \sigma (a_k)}{\partial a_{k}}} w_{kj} \right)  h^\prime(a_j)  z_i $<br><br>
$\displaystyle  \frac{\partial E_{n}}{\partial w_{j i}}= \left(  \sum_{k} {\frac{\partial E_{n}}{\partial y_{k}}}  {\frac{\partial \sigma (a_k)}{\partial a_{k}}} w_{kj} \right)  h^\prime(a_j)  z_i $<br><br>
$\displaystyle  \frac{\partial E_{n}}{\partial w_{j i}}= \underbrace{\left(  \sum_{k} \underbrace{\frac{\partial E_{n}}{\partial y_{k}}}_{(y_k-t_k)} \times  \overbrace{ \frac{\partial \sigma (a_k)}{\partial a_{k}}}^{z_k(1-z_k) \text{ or }y_k(1-y_k)} \times  w_{kj} \right)}_{\text{miss}}   \underbrace{h^\prime(a_j) }_{ z_j(1-z_j)}  z_i $<br><br>
$\displaystyle  \frac{\partial E_{n}}{\partial w_{j i}}=  \left(  \sum_{k}  (y_k-t_k) y_k (1-y_k)w_{kj} \right) z_j(1-z_j)z_i $ <br><br>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p></p>
<div class="flash">
    <svg class="octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong><br><br> $\displaystyle  y_{k}({\bf x},{\bf w})= \overbrace{ \underbrace{\sigma}_{\text{activation fucntion at output }}  \left(  \underbrace{ \sum_{j=0}^{M}w_{k j}^{(2)} \times  \overbrace{ \underbrace{h}_{\text{activation fucntion at hidden}}  \left(\underbrace{\sum_{i=0}^{D}w_{j i}^{(1)} \times \overbrace{z_{i}}^{\text{from last layer or input }x_i} }_{a_j} \right)}^{z_j}}_{a_k} \right)}^{z_k} $<br><br>$\displaystyle  \frac{\partial E_{n}}{\partial w_{j i}}=  \left(  \sum_{k}  (y_k-t_k) y_k (1-y_k)w_{kj} \right) z_j(1-z_j)z_i $ 
</div>
<br><br>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Error Backpropagation summery :<ol>
<li>Apply an input vector $x_n$ to the network and forward propagate through the network using below two equations  to find the activations of all the hidden and output units. <br>
$\displaystyle a_{j}=\sum_{i}w_{j i}z_{i}$ <br>
$z_j = h(a_j)$</li>
<li>Evaluate the $\delta _k $ for all the output units using  $\delta_K = y_k - t_k$</li>
<li>Backpropagete all the $\delta$ using below equation to find $\delta _j$ for each hidden unit in the network <br>
$\displaystyle \delta_{j}=h^{\prime}(a_{j})\sum_{k}w_{k j}\delta_{k}$<br>
</li>
<li>Use below equation to evaluate the required derivatives<br>
$\displaystyle \frac{\partial E_{n}}{\partial w_{j i}}=\delta_{j}z_{i}$<br>
</li>
</ol>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>For batch methods, the derivative of the total error $E$ can then be obtained by repeating the above steps for each pattern in the training set and then summing over all patterns:<br>
$\displaystyle \frac{\partial E}{\partial w_{j i}}=\sum_{n}\frac{\partial E_{n}}{\partial w_{j i}}$</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Backpropagation-Form-lecture-PDF">
<a class="anchor" href="#Backpropagation-Form-lecture-PDF" aria-hidden="true"><span class="octicon octicon-link"></span></a>Backpropagation Form lecture PDF<a class="anchor-link" href="#Backpropagation-Form-lecture-PDF"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It also has same points as per PRML but from different angle. But <strong>DIFFERENT TERMINOLOGY IS USED HERE SO BE CAREFUL</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br><br><br><br>
$$  
\mathbf{\text{Alert !!!}}
$$</p>
<p><br><br><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>SOME NOTATION IS WRONG IN THIS SECTION. DO NOT READ THIS SECTION, ALL CONCEPTS ARE ALREADY EXPLAINED</strong> <br><br><br><br><br><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Terminology <br>
$g$ is activation function <br>
$y=g(z)$ <br>
$E = (t_i-y_i)^2$ <br>
$z_i = a_j\times w_{ij}$</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Add a dimension for the observed error</li>
<li>Try to minimize your position on the "error surface"</li>
<li>Compute : <br>
$\text{Grad}_E = \left[ \frac{dE}{dW_1},\frac{dE}{dW_2},\dots , \frac{dE}{dW_n}  \right]$</li>
<li>Change $i_{th}$ weight by <br>
$\Delta W_i = -\alpha \frac{dE}{dW_i}$</li>
<li>We also use activation function at the end of every node.</li>
<li>consider $g(z)=y$ where $g$ is sigmoid activation function.</li>
<li>$g'(z)=g(z)\times (1-g(z))=y(1-y)$</li>
<li>Activation function must be continuous, differential, non-decreasing, and easy to compute.</li>
<li>We want activation function to be non-decreasing because, so that it should not increase value in some reason and decrease it in some other reason. </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Updating-Hidden-to-Output">
<a class="anchor" href="#Updating-Hidden-to-Output" aria-hidden="true"><span class="octicon octicon-link"></span></a>Updating Hidden-to-Output<a class="anchor-link" href="#Updating-Hidden-to-Output"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Updating Hidden-to-Output<br><br>
$\frac{\partial E}{\partial W_ij}= \frac{\partial E}{\partial y} \times \frac{\partial y}{\partial z} \times \frac{\partial z}{\partial W_ij}$<br><br>
$\Delta W_{ij} = \alpha \times \underbrace{(t_i -y_i)}_{\frac{\partial E}{\partial y} }  \times \underbrace{g'(z_i)}_{\frac{\partial y}{\partial z} }  \times \underbrace{a_j}_{\frac{\partial z}{\partial W_ij}} $ <br><br>
$\Delta W_{ij} = \overbrace{\alpha}^{\text{learning rate}}  \times \underbrace{(\overbrace{t_i}^{\text{Teacher supplied}}  -y_i)}_{\text{miss}}  \times \overbrace{g'(z_i)}^{\text{derivatve of acitvation function}} \times \underbrace{a_j}_{\text{previous layer output}} $<br><br>
$\Delta W_{ij} = \alpha \times (t_i -y_i) \times y_i \times (1-y_i)\times a_j$ <br><br>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Updating-interior-weights">
<a class="anchor" href="#Updating-interior-weights" aria-hidden="true"><span class="octicon octicon-link"></span></a>Updating interior weights<a class="anchor-link" href="#Updating-interior-weights"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Updating interior weights<br>
Layer $k$ units provide values to all layers $k+1$ units.<br>
"miss" is sum of the misses from all units on $k+1$<br><br>
$\displaystyle \text{miss}_j = \sum \left[ a_j(1-a_j)(t_i-a_j)w_{ji} \right]$<br><br>
$\displaystyle  \frac{\partial E}{\partial W_kj}= \left( \sum  \frac{\partial E}{\partial y_i} \times \frac{\partial y_i}{\partial z_i} \times \frac{\partial z_i}{\partial a_j}  \right) \times \frac{\partial a_j}{\partial l_j} \times \frac{\partial l_j}{\partial W_ij}$<br><br>
$\displaystyle  \frac{\partial E}{\partial W_kj}= \left( \sum  \underbrace{\frac{\partial E}{\partial y_i}}_{t_i-y_i}  \times \overbrace{\frac{\partial y_i}{\partial z_i}}^{y_i(1-y_i)}  \times \underbrace{\frac{\partial z_i}{\partial a_j}}_{w_ji}   \right) \times \underbrace{\frac{\partial a_j}{\partial l_j}}_{a_j(1-a_j)}  \times \overbrace{\frac{\partial l_j}{\partial W_ij}}^{l_k} $<br><br>
$\displaystyle  \frac{\partial E}{\partial W_kj} = \left(  \sum y_i  \times (1-y_i)  \times (t_i-y_i) \times  w_{ji}\right) \times  l_k  \times a_j  \times (1-a_j) $<br><br>
</li>
</ul>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="abhiyantaabhishek/MDS-IIT"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/MDS-IIT/foundations%20of%20machine%20learning/2022/09/24/CS5590-week5.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/MDS-IIT/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/MDS-IIT/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/MDS-IIT/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Home page contains all the posts date wise whereas Tag Page contains the same category wise.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/abhiyantaabhishek" target="_blank" title="abhiyantaabhishek"><svg class="svg-icon grey"><use xlink:href="/MDS-IIT/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/abhishek-kumar-dubey-585a86179" target="_blank" title="abhishek-kumar-dubey-585a86179"><svg class="svg-icon grey"><use xlink:href="/MDS-IIT/assets/minima-social-icons.svg#linkedin"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
