{
  
    
        "post0": {
            "title": "Linear Algebra CS6660 week 6",
            "content": "",
            "url": "https://abhiyantaabhishek.github.io/MDS-IIT/mathematical%20foundations%20of%20data%20science/2022/10/01/CS6660-week6.html",
            "relUrl": "/mathematical%20foundations%20of%20data%20science/2022/10/01/CS6660-week6.html",
            "date": " • Oct 1, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Neural Networks  CS5590 week 5",
            "content": "Deep Learning . Deep learning : A sub area of machine learning, that is today understood as representation learning. | Inspired by the human brain. | How do Neural Networks learn: We initialize the weights with random value. | Then present a trining patter to the network. | Feed it through tho get output. (feed forward) | compare with target output. | Adjust weights based on the error. | And so on ... | . | Deep learning models can learn complex decision boundaries. | . Perceptrons (Linear Models) . Consider the below pic: Mathematical formulation is given as below: $$ displaystyle z = left lbrace begin{array}{ccc} 1 &amp; text{if} &amp; displaystyle sum_{i=1}^n x_iw_i ge theta 0 &amp; text{if} &amp; displaystyle sum_{i=1}^n x_iw_i &lt; theta end{array} right.$$ | . Learn weight such that the objective function is maximized. | Loss calculation : $ Delta W_i = c(t-z)X_i$ where $W_i$ is the weight from input $i$ to perceptron node, $c$ is the learning rate, $t_j$ is the target for the current instance, $z$ is the current output, and $X_i$ is $i^{th}$ input | Least perturbation principle only change weights if there is an error | small $c$ sufficient to make current pattern corret | scale by $X_i$ | . | create a perceptron node with $n$ inputs. | Iteratively apply a pattern from the training set and apply the perceptron rule | Each iteration through the training set is an epoch | continue training until total training set error ceases to improve | Peceptron Convergence Theorem : Guaranteed to find a solution in finite time if a solution exists | . Multi Layer Perceptrons . MLP From PRML book . Network with inputs, one hidden unit and outputs: | . The output of the above network can be given as follows: $ displaystyle y_{k}({ bf x},{ bf w})= sigma left( sum_{j=1}^{M}w_{k j}^{(2)}h left( sum_{i=1}^{D}w_{j i}^{(1)}x_{i}+w_{j0}^{(1)} right)+w_{k0}^{(2)} right)$ This equation is also interpreted as forward propagation fo information through the newwork. It should be emphasized that these diagrams do not represent probabilistic graphical models, because the internal nodes represent deterministic variables rather than stochastic ones. | The above equation can be written as below if bias is absorbed into the set of weight by defining additional input variable $x_0$ whose value is clamped at $x_0=1$ $ displaystyle y_{k}({ bf x},{ bf w})= sigma left( sum_{j=0}^{M}w_{k j}^{(2)}h left( sum_{i=0}^{D}w_{j i}^{(1)}x_{i} right) right)$ | A key difference among neural network and perceptron, is that the neural network uses continuous sigmoidal non-linearities in the hidden units, whereas the perceptron uses step-function non-linearities. | If the activation functions of all the hidden units in a network are taken to be linear, then for any such network we can always find an equivalent network without hidden units. | . In principle, a network with sigmoidal hidden units can always mimic skip layer connections by using a sufficiently small first-layer weight that, over its operating range. | In practice, however, it may be advantageous to include skip-layer connections explicitly. | . MLP Form lecture PDF . Extension of perceptrons to multiple layers Initialize network with random weights | For all training cases ( called examples): present training inputs to network and calculate output | for all layers (starting with output layer, back to input layer): compare network output with correct putput | Adapt weight in current layer | . | . | | Method for Learning Weights in feed forward nets Can&#39;t use Perceptron Rule No teacher values (loss) are possible for hidden units. | . | Use Gradient decent to minimize the error Propagate the deltas to adjust for errors | Backward from outputs to hidden layers to inputs | The algorithm can be summarized as follows: Computes the error term for the output units using the observed error. | From output layer , repeat Propagating the error term back to the previous layer and updating the weights between the two layers until the earliest layer is reached. | . | . | . | . | . Algorithm in detail: Initialize weights (typically random) | Keep doing epoch For each example $e$ in the training set do Forward Pass to compute $y = $ neural new output (network , $e$) | miss = $(T-y)$ at each output unit | . | backward pass to calculate deltas to weights | update all weights | . | end | . | until tuning set error stops improving | . | . Error Backpropagation . Backpropagation From PRML book . Think of the N weights as a point in an N-dimensional space | . Many error functions fo practical interest, comprise a sum of terms, onw for each data point in training set, so that $ displaystyle E( mathbf{w})= sum_{n=1}^{N}E_{n}( mathbf{w})$ | Error function for one particular input patter $n$ takes the form $ displaystyle E_{n}={ frac{1}{2}} sum_{k}(y_{n k}-t_{n k})^{2}$ where $y_{n k}=y_{k}( mathbf{x}_{n}, mathbf{w})$ | The gradient of this error with respect to a weight $w_{ji}$ is given by $ displaystyle { frac{ partial E_{n}}{ partial w_{j i}}}=(y_{n j}-t_{n j})x_{n i}$ | . In a general feed-forward network, each unit computes a weighted sum of its inputs of the form: $ displaystyle a_{j}= sum_{i}w_{j i}z_{i}$ where $z_i$ is the activation of a unit, or input, that sends a connection to unit $j$, and $w_{ji}$ is the weight associated with that connection | . A non-linear activation function $h(.)$ transforms $a_j$ to produce $z_j$ of unit $j$ in the form $z_{j}=h(a_{j})$ Note $z_i$ in equation $a_{j}= sum_{i}w_{j i}z_{i}$ could be an input, and the unit $j$ in equation $z_{j}=h(a_{j})$ could be an output | . Now we consider to evaluate derivative of $E_n$ with respect to $w_{ji}$, $E_n$ depends on the weight $w_{ji}$ only via the summed input $a_j$ to unit $j$. Applying chain rule for partial derivatives we get $ displaystyle frac{ partial E_{n}}{ partial w_{j i}}= frac{ partial E_{n}}{ partial a_{j}} frac{ partial a_{j}}{ partial w_{j i}}$ | . Consider a useful notation $ displaystyle delta_{j} equiv frac{ partial{ E}_{n}}{ partial a_{j}}$ | we can find derivative of $a_j$ with respect to $w_{ji}$ using $ a_{j}= sum_{i}w_{j i}z_{i}$, we get $ displaystyle { frac{ partial a_{j}}{ partial w_{j i}}}=z_{i}$ | . Using above three bullet points we get $ displaystyle frac{ partial E_{n}}{ partial w_{j i}}= delta_{j}z_{i}$ This tells us required derivative is obtained simply by multiplying the value of $ delta$ for the unit at the output end of the weight by the value of $z$ for the unit at the input end of the weight. | . For output unit we have $ delta_k = y_k - t_k$ | For hidden units, we again make use of chain rule for partial derivatives $ displaystyle delta_{j} equiv frac{ partial E_{n}}{ partial a_{j}}= sum_{k} frac{ partial E_{n}}{ partial a_{k}} frac{ partial a_{k}}{ partial a_{j}}$ | . Now we know that $ displaystyle frac{ partial E_{n}}{ partial a_{k}} = delta _k$ also, $ a_{k}= sum_{j}w_{k j}z_{j}$ and $z_{j}=h(a_{j})$ so $ displaystyle frac{ partial a_{k}}{ partial a_{j}} = frac{ partial sum_{j}w_{k j}h(a_{j})}{ partial a_{j}} = w_{k j} h^ prime(a_{j}) $ putting value of $ frac{ partial E_{n}}{ partial a_{k}}$ and $ frac{ partial a_{k}}{ partial a_{j}} $ in the equation of above bullet point we get $ displaystyle delta_{j}=h^{ prime}(a_{j}) sum_{k}w_{k j} delta_{k}$ | . In short what we discussed till now $ displaystyle frac{ partial E_{n}}{ partial w_{j i}}= frac{ partial E_{n}}{ partial a_{j}} frac{ partial a_{j}}{ partial w_{j i}} $ $ displaystyle frac{ partial E_{n}}{ partial w_{j i}}= left( sum_{k}{ frac{ partial E_{n}}{ partial a_{k}}}{ frac{ partial a_{k}}{ partial a_{j}}} right) frac{ partial a_{j}}{ partial w_{j i}} $ $ displaystyle frac{ partial E_{n}}{ partial w_{j i}}= left( sum_{k}{ frac{ partial E_{n}}{ partial a_{k}}} left( w_{k j}h^{ prime}(a_{j}) right) right) frac{ partial a_{j}}{ partial w_{j i}} $ $ displaystyle frac{ partial E_{n}}{ partial w_{j i}}= h^{ prime}(a_{j}) left( sum_{k}{ frac{ partial E_{n}}{ partial a_{k}}}w_{k j} right) frac{ partial a_{j}}{ partial w_{j i}} $ $ displaystyle frac{ partial E_{n}}{ partial w_{j i}}= h^{ prime}(a_{j}) left( sum_{k}{ frac{ partial E_{n}}{ partial a_{k}}}w_{k j} right) z_i $ $ displaystyle frac{ partial E_{n}}{ partial w_{j i}}= h^{ prime}(a_{j}) z_i left( sum_{k}{ frac{ partial E_{n}}{ partial a_{k}}}w_{k j} right) $ | . In sort what we discussed till now with little more elaboration $ displaystyle y_{k}({ bf x},{ bf w})= overbrace{ sigma left( underbrace{ sum_{j=0}^{M}w_{k j}^{(2)} times overbrace{h left( underbrace{ sum_{i=0}^{D}w_{j i}^{(1)} times z_{i}}_{a_j} right)}^{z_j}}_{a_k} right)}^{z_k} $ $ displaystyle y_{k}({ bf x},{ bf w})= overbrace{ underbrace{ sigma}_{ text{activation fucntion at output }} left( underbrace{ sum_{j=0}^{M}w_{k j}^{(2)} times overbrace{ underbrace{h}_{ text{activation fucntion at hidden}} left( underbrace{ sum_{i=0}^{D}w_{j i}^{(1)} times overbrace{z_{i}}^{ text{from last layer or input }x_i} }_{a_j} right)}^{z_j}}_{a_k} right)}^{z_k} $ $ displaystyle E_{n}={ frac{1}{2}} sum_{k}(y_{k}-t_{k})^{2}$ Till now we had below expression : $ displaystyle frac{ partial E_{n}}{ partial w_{j i}}= left( sum_{k} frac{ partial E_{n}}{ partial a_{k}} { frac{ partial a_{k}}{ partial a_{j}}} right) frac{ partial a_{j}}{ partial w_{j i}} $ But if we consider activation function $ sigma $ at out layer and $y_k = z_k = sigma (a_k)$ we get : $ displaystyle frac{ partial E_{n}}{ partial w_{j i}}= left( sum_{k} frac{ partial E_{n}}{ partial z_{k}} { frac{ partial z_{k}}{ partial a_{k}}} { frac{ partial a_{k}}{ partial a_{j}}} right) frac{ partial a_{j}}{ partial w_{j i}} $ $ displaystyle frac{ partial E_{n}}{ partial w_{j i}}= left( sum_{k} frac{ partial E_{n}}{ partial z_{k}} { frac{ partial z_{k}}{ partial a_{k}}} { frac{ partial sum_{j}w_{k j}z_{j}}{ partial a_{j}}} right) frac{ partial a_{j}}{ partial w_{j i}} $ $ displaystyle frac{ partial E_{n}}{ partial w_{j i}}= left( sum_{k} frac{ partial E_{n}}{ partial z_{k}} { frac{ partial z_{k}}{ partial a_{k}}} { frac{ partial sum_{j}w_{k j}z_{j}}{ partial a_{j}}} right) frac{ partial a_{j}}{ partial w_{j i}} $ $ displaystyle frac{ partial E_{n}}{ partial w_{j i}}= left( sum_{k} frac{ partial E_{n}}{ partial z_{k}} { frac{ partial z_{k}}{ partial a_{k}}} { frac{ partial sum_{j}w_{k j}h(a_{j})}{ partial a_{j}}} right) frac{ partial a_{j}}{ partial w_{j i}} $ $ displaystyle frac{ partial E_{n}}{ partial w_{j i}}= left( sum_{k} frac{ partial E_{n}}{ partial z_{k}} { frac{ partial z_{k}}{ partial a_{k}}} w_{kj}h^ prime(a_j) right) frac{ partial a_{j}}{ partial w_{j i}} $ $ displaystyle frac{ partial E_{n}}{ partial w_{j i}}= left( sum_{k} frac{ partial E_{n}}{ partial z_{k}} { frac{ partial z_{k}}{ partial a_{k}}} w_{kj} right) h^ prime(a_j) frac{ partial a_{j}}{ partial w_{j i}} $ $ displaystyle frac{ partial E_{n}}{ partial w_{j i}}= left( sum_{k} frac{ partial E_{n}}{ partial z_{k}} { frac{ partial z_{k}}{ partial a_{k}}} w_{kj} right) h^ prime(a_j) frac{ partial sum_{i}w_{j i}z_{i}}{ partial w_{j i}} $ $ displaystyle frac{ partial E_{n}}{ partial w_{j i}}= left( sum_{k} frac{ partial E_{n}}{ partial z_{k}} { frac{ partial z_{k}}{ partial a_{k}}} w_{kj} right) h^ prime(a_j) z_i $ | . Since $z_k$ is same as $y_k$, we can replace $z_k$ with $y_k$ in above equation $ displaystyle frac{ partial E_{n}}{ partial w_{j i}}= left( sum_{k} { frac{ partial E_{n}}{ partial y_{k}}} { frac{ partial y_{k}}{ partial a_{k}}} w_{kj} right) h^ prime(a_j) z_i $ we know $y_k=z_k= sigma (a_k)$ so we get $ displaystyle frac{ partial E_{n}}{ partial w_{j i}}= left( sum_{k} { frac{ partial E_{n}}{ partial y_{k}}} { frac{ partial sigma (a_k)}{ partial a_{k}}} w_{kj} right) h^ prime(a_j) z_i $ $ displaystyle frac{ partial E_{n}}{ partial w_{j i}}= left( sum_{k} { frac{ partial E_{n}}{ partial y_{k}}} { frac{ partial sigma (a_k)}{ partial a_{k}}} w_{kj} right) h^ prime(a_j) z_i $ $ displaystyle frac{ partial E_{n}}{ partial w_{j i}}= underbrace{ left( sum_{k} underbrace{ frac{ partial E_{n}}{ partial y_{k}}}_{(y_k-t_k)} times overbrace{ frac{ partial sigma (a_k)}{ partial a_{k}}}^{z_k(1-z_k) text{ or }y_k(1-y_k)} times w_{kj} right)}_{ text{miss}} underbrace{h^ prime(a_j) }_{ z_j(1-z_j)} z_i $ $ displaystyle frac{ partial E_{n}}{ partial w_{j i}}= left( sum_{k} (y_k-t_k) y_k (1-y_k)w_{kj} right) z_j(1-z_j)z_i $ | . . Note: $ displaystyle y_{k}({ bf x},{ bf w})= overbrace{ underbrace{ sigma}_{ text{activation fucntion at output }} left( underbrace{ sum_{j=0}^{M}w_{k j}^{(2)} times overbrace{ underbrace{h}_{ text{activation fucntion at hidden}} left( underbrace{ sum_{i=0}^{D}w_{j i}^{(1)} times overbrace{z_{i}}^{ text{from last layer or input }x_i} }_{a_j} right)}^{z_j}}_{a_k} right)}^{z_k} $$ displaystyle frac{ partial E_{n}}{ partial w_{j i}}= left( sum_{k} (y_k-t_k) y_k (1-y_k)w_{kj} right) z_j(1-z_j)z_i $ . Error Backpropagation summery : Apply an input vector $x_n$ to the network and forward propagate through the network using below two equations to find the activations of all the hidden and output units. $ displaystyle a_{j}= sum_{i}w_{j i}z_{i}$ $z_j = h(a_j)$ | Evaluate the $ delta _k $ for all the output units using $ delta_K = y_k - t_k$ | Backpropagete all the $ delta$ using below equation to find $ delta _j$ for each hidden unit in the network $ displaystyle delta_{j}=h^{ prime}(a_{j}) sum_{k}w_{k j} delta_{k}$ | Use below equation to evaluate the required derivatives $ displaystyle frac{ partial E_{n}}{ partial w_{j i}}= delta_{j}z_{i}$ | | . For batch methods, the derivative of the total error $E$ can then be obtained by repeating the above steps for each pattern in the training set and then summing over all patterns: $ displaystyle frac{ partial E}{ partial w_{j i}}= sum_{n} frac{ partial E_{n}}{ partial w_{j i}}$ | . Backpropagation Form lecture PDF . It also has same points as per PRML but from different angle. But DIFFERENT TERMINOLOGY IS USED HERE SO BE CAREFUL . $$ mathbf{ text{Alert !!!}} $$ . . SOME NOTATION IS WRONG IN THIS SECTION. DO NOT READ THIS SECTION, ALL CONCEPTS ARE ALREADY EXPLAINED . Terminology $g$ is activation function $y=g(z)$ $E = (t_i-y_i)^2$ $z_i = a_j times w_{ij}$ | . Add a dimension for the observed error | Try to minimize your position on the &quot;error surface&quot; | Compute : $ text{Grad}_E = left[ frac{dE}{dW_1}, frac{dE}{dW_2}, dots , frac{dE}{dW_n} right]$ | Change $i_{th}$ weight by $ Delta W_i = - alpha frac{dE}{dW_i}$ | We also use activation function at the end of every node. | consider $g(z)=y$ where $g$ is sigmoid activation function. | $g&#39;(z)=g(z) times (1-g(z))=y(1-y)$ | Activation function must be continuous, differential, non-decreasing, and easy to compute. | We want activation function to be non-decreasing because, so that it should not increase value in some reason and decrease it in some other reason. | . Updating Hidden-to-Output . Updating Hidden-to-Output $ frac{ partial E}{ partial W_ij}= frac{ partial E}{ partial y} times frac{ partial y}{ partial z} times frac{ partial z}{ partial W_ij}$ $ Delta W_{ij} = alpha times underbrace{(t_i -y_i)}_{ frac{ partial E}{ partial y} } times underbrace{g&#39;(z_i)}_{ frac{ partial y}{ partial z} } times underbrace{a_j}_{ frac{ partial z}{ partial W_ij}} $ $ Delta W_{ij} = overbrace{ alpha}^{ text{learning rate}} times underbrace{( overbrace{t_i}^{ text{Teacher supplied}} -y_i)}_{ text{miss}} times overbrace{g&#39;(z_i)}^{ text{derivatve of acitvation function}} times underbrace{a_j}_{ text{previous layer output}} $ $ Delta W_{ij} = alpha times (t_i -y_i) times y_i times (1-y_i) times a_j$ | . Updating interior weights . Updating interior weights Layer $k$ units provide values to all layers $k+1$ units. &quot;miss&quot; is sum of the misses from all units on $k+1$ $ displaystyle text{miss}_j = sum left[ a_j(1-a_j)(t_i-a_j)w_{ji} right]$ $ displaystyle frac{ partial E}{ partial W_kj}= left( sum frac{ partial E}{ partial y_i} times frac{ partial y_i}{ partial z_i} times frac{ partial z_i}{ partial a_j} right) times frac{ partial a_j}{ partial l_j} times frac{ partial l_j}{ partial W_ij}$ $ displaystyle frac{ partial E}{ partial W_kj}= left( sum underbrace{ frac{ partial E}{ partial y_i}}_{t_i-y_i} times overbrace{ frac{ partial y_i}{ partial z_i}}^{y_i(1-y_i)} times underbrace{ frac{ partial z_i}{ partial a_j}}_{w_ji} right) times underbrace{ frac{ partial a_j}{ partial l_j}}_{a_j(1-a_j)} times overbrace{ frac{ partial l_j}{ partial W_ij}}^{l_k} $ $ displaystyle frac{ partial E}{ partial W_kj} = left( sum y_i times (1-y_i) times (t_i-y_i) times w_{ji} right) times l_k times a_j times (1-a_j) $ | .",
            "url": "https://abhiyantaabhishek.github.io/MDS-IIT/foundations%20of%20machine%20learning/2022/09/24/CS5590-week5.html",
            "relUrl": "/foundations%20of%20machine%20learning/2022/09/24/CS5590-week5.html",
            "date": " • Sep 24, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Linear Algebra CS6660 week 4",
            "content": "Gaussian Elimination . Gaussian Elimination are elementary transformation of subsystems fo linear equations, which transforms the equation systems into a simple form. | Elementary transformation that keep the solution set the same, but that transform the equation system. | Process Exchange fo two equations (rows in the matrix representing the system of equations) | Multiplication of an equation (row) with a constant $ lambda in mathbb{R}- {0 }$ | Addition of two equations (rows) | . | . Example : $ begin{array}{r r r r r r r r r r} -2x_1 &amp; + &amp; 4x_2 &amp; - &amp; 2x_3 &amp; - &amp; x_4 &amp; + &amp; 4x_5 &amp; = &amp; -3 4x_1 &amp; - &amp; 8x_2 &amp; + &amp; 3x_3 &amp; - &amp; 3x_4 &amp; + &amp; x_5 &amp; = &amp; 2 x_1 &amp; - &amp; 2x_2 &amp; + &amp; x_3 &amp; - &amp; x_4 &amp; + &amp; x_5 &amp; = &amp; 0 x_1 &amp; - &amp; 2x_2 &amp; &amp; &amp; - &amp; 3x_4 &amp; + &amp; 4x_5 &amp; = &amp; a end{array}$ Build the _augmented matrix (in the form of $ left[A mid b right]$ ) $ left[ begin{array}{r r r r r | r} -2 &amp; 4 &amp; -2 &amp; -1 &amp; 4 &amp; -3 4 &amp;-8 &amp; 3 &amp; -3 &amp; 1 &amp; 2 1 &amp;-2 &amp; 1 &amp; -1 &amp; 1 &amp; 0 1 &amp;-2 &amp; 0 &amp; -3 &amp; 4 &amp; a end{array} right] $ Exchange row $3$ and row $1$ for simplicity $ left[ begin{array}{r r r r r | r} 1 &amp;-2 &amp; 1 &amp; -1 &amp; 1 &amp; 0 4 &amp;-8 &amp; 3 &amp; -3 &amp; 1 &amp; 2 -2 &amp; 4 &amp; -2 &amp; -1 &amp; 4 &amp; -3 1 &amp;-2 &amp; 0 &amp; -3 &amp; 4 &amp; a end{array} right] $ Perform operations $ { R_2 leftarrow R_2 - 4R_1, R_3 leftarrow R_3+2R_1, R_4 leftarrow R_4-R_1 }$ $ left[ begin{array}{r r r r r | r} 1 &amp;-2 &amp; 1 &amp; -1 &amp; 1 &amp; 0 0 &amp; 0 &amp; -1 &amp; 1 &amp; -3 &amp; 2 0 &amp; 0 &amp; 0 &amp; -3 &amp; 6 &amp; -3 0 &amp; 0 &amp; -1 &amp; -2 &amp; 3 &amp; a end{array} right] $ Perform operations $ { R_4 leftarrow R_4 - R_2 -R_3 }$ $ left[ begin{array}{r r r r r | r} 1 &amp;-2 &amp; 1 &amp; -1 &amp; 1 &amp; 0 0 &amp; 0 &amp; -1 &amp; 1 &amp; -3 &amp; 2 0 &amp; 0 &amp; 0 &amp; -3 &amp; 6 &amp; -3 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; a end{array} right] $ Perform operations $ { R_2 leftarrow -R_2 , R_3 leftarrow - frac{1}{3}R_3 }$ $ left[ begin{array}{r r r r r | r} 1 &amp;-2 &amp; 1 &amp; -1 &amp; 1 &amp; 0 0 &amp; 0 &amp; 1 &amp; -1 &amp; 3 &amp; -2 0 &amp; 0 &amp; 0 &amp; 1 &amp; -2 &amp; 1 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; a+1 end{array} right] $ Above matrix is in row-echelon from Convert above matrix to normal equation form $ begin{array}{l l l l l l r} x_1 &amp;-2x_2 &amp; +x_3 &amp; -x_4 &amp; +x_5 &amp; = &amp; 0 &amp; &amp; +x_3 &amp; -x_4 &amp; +3x_5 &amp; = &amp; -2 &amp; &amp; &amp; +x_4 &amp; -2x_5 &amp; = &amp; 1 &amp; &amp; &amp; &amp; +0 &amp; = &amp; a+1 end{array} $ Only for $a=1$ this system can be solved, and can be solved using back substitution $ left[ begin{array}{r} x_1 x_2 x_3 x_4 x_5 end{array} right] = left[ begin{array}{r} 2 0 -1 1 0 end{array} right]$ General solution of the above equation can be found as explained earlier, and is given as below $ left {x in mathbb{R}^{5}:x= { left[ begin{array}{r}{2} {0} {-1} {1} 0 end{array} right]} + lambda_{1}{ left[ begin{array}{r}{2} {1} {0} {0} 0 end{array} right]} + lambda_{2}{ left[ begin{array}{r}{2} {0} {-1} {2} 1 end{array} right]}, ; ; lambda_{1}, lambda_{2} in mathbb{R} right }$ | . Row-Echelon Form . Example matrix $ left[ begin{array}{r r r r r | r} boxed 1 &amp;-2 &amp; 1 &amp; -1 &amp; 1 &amp; 0 0 &amp; 0 &amp; boxed 1 &amp; -1 &amp; 3 &amp; -2 0 &amp; 0 &amp; 0 &amp; boxed 1 &amp; -2 &amp; 1 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; a+1 end{array} right] $ | . Definition A matrix is in row-echelon form if All rows that contains only zeros are at the bottom ot the matrix; correspondingly, all rows that contains at least one nonzero element are on top of rows that contain only zeros. | Looking at nonzero rows only, the first nonzero number from the left ( also called the pivot or the leading coefficient ) is always strictly to the right of the pivot of the row above it. | . | . Basic and free variables : The variables corresponding to the pivots in the row-echelon form are called basic variables and the other variables are free variables. For example, $x_1; x_3; x_4 $ are basic variables, whereas $x_2;x_5$ are free variables in above example matrix. | . Reduced row Echelon Forms . Definition An equation system is in reduced row echelon form ( also: rwo reduced echelon form or row canonical from ) if It is in row echelon from. | Every pivot is 1. | The pivot is the only nonzero entry in its column. | . | . Solving $AX=0$ (Minus $1$ Trick) . Example Matrix $ left[ begin{array}{r r r r r} 1 &amp; 3 &amp; 0 &amp; 0 &amp; 3 0 &amp; 0 &amp; 1 &amp; 0 &amp; 9 0 &amp; 0 &amp; 0 &amp; 1 &amp; -4 end{array} right] $ For finding the solutions of $AX=0$ is to look at the non-pivot columns, which we will need to express as a (linear) combination of the pivot columns. $ left {x in mathbb{R}^{5}:x= lambda_{1}{ left[ begin{array}{r}3 -1 0 0 0 end{array} right]} + lambda_{2}{ left[ begin{array}{r}3 0 9 -4 -1 end{array} right]}, ; ; lambda_{1}, lambda_{2} in mathbb{R} right }$ Insert row in place of free variable ( $2^{ text{nd}}$ and $5^{ text{th}}$) with $ -1 $ value in place of dependent variable as shown below $ tilde{A} = left[ begin{array}{r r r r r} 1 &amp; 3 &amp; 0 &amp; 0 &amp; 3 boxed 0 &amp; boxed { mathbf {-1}} &amp; boxed 0 &amp; boxed 0 &amp; boxed 0 0 &amp; 0 &amp; 1 &amp; 0 &amp; 9 0 &amp; 0 &amp; 0 &amp; 1 &amp; -4 boxed 0 &amp; boxed 0 &amp; boxed 0 &amp; boxed 0 &amp; boxed { mathbf {-1}} end{array} right] $ Now the solution for $AX=0$ can be found by the columns of the the free variable which is $2^{ text{nd}}$ and $5^{ text{th}}$ column. | . Solving a System of Liner Equation . Inversion of matrices which are not square and non-invertible. $Ax=b$ is given as $x=A^{-1}b$ $Ax=b Longleftrightarrow A^TAx=A^Tb Longleftrightarrow x= (A^TA)^{-1}A^Tb $ This is called Moore-Penrose pseudo-inverse The disadvantage of this method is that it requires a lot of computation. | . Gaussian Elimination plays a key role in Computing determinants | Checking whether a set of vectors is linearly independent | Computing the rank of the matrix | Determining a basis of the vector space. | . | . Vector Space . A vector space $V$ is a set that is closed under finite vector addition and scalar multiplication. we two vectors are added or multiplied it should stay in the same vector space. | . | . Vector space properties In order for $V$ to be a vector space, the following condition must hold for all elements $X,Y,Z in V$ and any scalars $r,s in F$: Commutativity: $X+Y=Y+X$ | Associativity of vector addition: $(X+Y)+Z = X+(Y+Z)$ | Additive Identity: For all $X, ; ; 0+X = X+0=X$ | Existence of additive inverse: For any $X$, there exists a $-X$ such that $X+(-X) =0$ | Associativity of vector multiplication: $r(sX) = (rs)X$ | Distributivity of scalar sums: $(r+s)X = rX+sX$ | Scalar multiplication identity: $1X=X$ | . | . Subspace . Let $V$ be a vector space, and let $W$ be the subspace of $V$, if $W$ is a vector space with respect to the operation in $V$, then $W$ is called a subspace of $V$ | Let $V$ be the vector space, with operations $+$ and $ ; cdot ;$ and let $W$ be subset of $V$. Then $W$ is subspace of $V$ if and only if the following conditions hold. $W$ is non-empty: The zero vector belongs to $W$. | Closure under $+$: If $u$ and $v$ are any vectors in $W$, the $u+v$ is in $W$. | Closure under $ cdot ;$:if $v$ is any vector in $W$, and $c$ is nay real number, then $c cdot v$ si in $W$. | . | . Liner Combination . Consider a vector space $V$ and a finite number of vectors $x_1, dots , x_k in V$, then every $v in V$ of the form $ displaystyle v= lambda_1 x_1+ dots + lambda_kx_k = sum_{i=1}^k lambda_i x_i in V$ $ lambda_1, dots , lambda_k in mathbb{R}$ is a linear combination fo the vectors $x_1, dots ,x_k$ $ displaystyle 0= sum_{i=1}^k 0 x_i$ | . Linear Independence . Let us consider a vector space $V$ with $k in mathbb{N}$ and $x_1, dots ,x_k in V$. If there is a non trivial linear combination, such that $ 0= sum_{i=1}^k lambda_i x_i$ with at least one $ lambda _i ne 0,$ the vectors $x_1, dots , x_k$ are linearly independent. If only the trivial solution exists, i.e., $ lambda_1 = dots= lambda_k=0$ the vectors $x_1, dots , x_k$ are linearly independent. | If at-least one of the vectors $x_1, dots , x_k$ is $0$ then they are linearly dependent. The same holds if two vectors are identical. | . To find if a system is linear independent, perform gaussian elimination, if there is no non-pivot column then the vectors are independent. | .",
            "url": "https://abhiyantaabhishek.github.io/MDS-IIT/mathematical%20foundations%20of%20data%20science/2022/09/17/CS6660-week4_2.html",
            "relUrl": "/mathematical%20foundations%20of%20data%20science/2022/09/17/CS6660-week4_2.html",
            "date": " • Sep 17, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Probability Mass Function  CS6660 week 4",
            "content": "Bernoulli, Binomial . Definition: Suppose that $n$ independent trails are performed, each succeeding with probability $p$. Let $X$ count the number of success within the $n$ trails. Then $X$ has the Binomial distribution with parameters $n$ and $p$ of, in short $X~ mathrm{Binom} left(n,p right)$ $ displaystyle p left(i right)=P left lbrace X=i right rbrace = {n choose i} times p^i times { left(1-p right)}^{n-i} , ; ; ; ;i=0,1, ldotp ldotp ldotp ldotp ,n$ | . import matplotlib.pyplot as plt import math def get_Binom(n,p): P=[] for i in range(n+1): P.append( math.comb(n,i) * math.pow(p,i) * math.pow(1-p,n-i)) return P def plot_dist(n,p): P =get_Binom(n=n,p=p) plt.plot(range(len(P)),P,linestyle=&#39;--&#39;, marker=&#39;o&#39;,label=&quot;p={:.2f}&quot;.format(p)) plot_dist(n=30,p=0.5) plot_dist(n=30,p=0.3) plot_dist(n=30,p=0.7) plt.title(&quot;Binomial Distribution&quot;) plt.xlabel(&quot;$i$&quot;) plt.ylabel(&quot;$p(x=i)$&quot;); plt.legend(); . . Binomial : $ boxed{ displaystyle p left(i right)=P left lbrace X=i right rbrace = {n choose i} times p^i times { left(1-p right)}^{n-i}} $ $ ; ; ; ;i=0,1, ldotp ldotp ldotp ldotp ,n$ $ displaystyle mathrm{EX}= mathrm{np},$ and $ mathrm{VarX}= mathrm{np} left(1-p right)$ | Binomial mass function : $ displaystyle sum_{i=0}^n p left(i right)= sum_{i=0}^n P left lbrace X=i right rbrace = sum_{i=0}^n {n choose i} times p^i times { left(1-p right)}^{n-i} $ | . If we consider just one trail (event), it is called bernoulli, if we consider more trails (events) it is called binomial. | In particular, the Bernoulli $( rho)$ variable can take on values 0 or 1, with respective probabilities. $ rho(1) = p$ and $ rho(0) = 1-p$ | . Remarks : $ displaystyle sum_{i=0}^n p left(i right)= sum_{i=0}^n P left lbrace X=i right rbrace = sum_{i=0}^n {n choose i} times p^i times { left(1-p right)}^{n-i} =[p+(1-p)]^n=1$ | . Expectation $ displaystyle EX =np $ . $ boxed {EX = np}$ proof (using trick): Trick : $ displaystyle i= frac{d}{dt}^{i}{ Big|}_{t=1} ,$ $ displaystyle EX = sum_{i=0}^n {n choose i} times i times p^i times (1-p)^{n-i}$ using the trick we get $ displaystyle EX = frac{d}{dt} left( sum_{i=0}^n {n choose i} times t^{i} times p^i times (1-p)^{n-i} right) { Big|}_{t=1} $ $ displaystyle EX = frac{d}{dt} left( sum_{i=0}^n {n choose i} times {(tp)}^{i} times (1-p)^{n-i} right) { Big|}_{t=1} $ $ displaystyle EX = frac{d}{dt} left( tp+1-p right)^n { Big|}_{t=1} $ $ displaystyle EX =n left( tp+1-p right)^{n-1} cdot p { Big|}_{t=1} $ $ displaystyle EX =n p $ | Proof (normal way) : $ displaystyle EX = sum_{i=0}^n {n choose i} times i times p^i times (1-p)^{n-i}$ we can start the sum from $1$ as $0^{ text{th}}$ term will become zero $ displaystyle EX = sum_{i=1}^n {n choose i} times i times p^i times (1-p)^{n-i}$ we know that $ displaystyle i cdot {n choose i} = n cdot {n-1 choose i-1}$ so we get $ displaystyle EX = sum_{i=1}^n n times {n-1 choose i-1} times p^i times (1-p)^{n-i}$ $ displaystyle EX =np sum_{i=1}^n {n-1 choose i-1} times p^{i-1} times (1-p)^{n-i}$ $ displaystyle EX =np sum_{i=1}^n {n-1 choose i-1} times p^{i-1} times (1-p)^{(n-1)-(i-1)}$ Consider $n-1=m$ and $i-1=j$, we get $ displaystyle EX =np sum_{j=0}^n {m choose j} times p^{j} times (1-p)^{m-j}$ $ displaystyle EX =np underbrace{ sum_{j=0}^n {m choose j} times p^{j} times (1-p)^{m-j}}_{=1} $ $ displaystyle EX =np $ | . | . Variance $ displaystyle mathrm{Var} X = np(1-p)$ . $ boxed { mathrm{Var} X = np(1-p)}$ proof (using trick): Trick : $ displaystyle i(i-1)= frac{d}{dt^2}^{i}{ Big|}_{t=1} ,$ First find $E[X(X-1)]$ $ displaystyle E[X(X-1)] = sum_{i=0}^n {n choose i} times i times(i-1) times p^i times (1-p)^{n-i}$ $ displaystyle E[X(X-1)] = sum_{i=0}^n {n choose i} times frac{d}{dt^2}^{i}{ Big|}_{t=1} times p^i times (1-p)^{n-i} ; ; ;$ We got this using the trick $ displaystyle E[X(X-1)] = frac{d}{dt^2} left( sum_{i=0}^n {n choose i} times ^{i} times p^i times (1-p)^{n-i} right) { Big|}_{t=1} $ $ displaystyle E[X(X-1)] = frac{d}{dt^2} left( tp + (1-p) right)^n { Big|}_{t=1} $ $ displaystyle E[X(X-1)] = n(n-1) left( tp + 1-p right)^{n-2}p cdot p { Big|}_{t=1} $ $ displaystyle E[X(X-1)] = n(n-1)p^2$ Now, $ displaystyle mathrm{Var} X = E(X^2)-(EX)^2$ $ displaystyle mathrm{Var} X = left( E(X^2)-E(X) right)+ left(E(X)- (EX)^2 right)$ $ displaystyle mathrm{Var} X = E left[X^2-X right] + left(E(X)- (EX)^2 right)$ $ displaystyle mathrm{Var} X = E left[ X(X-1) right] + left(E(X)- (EX)^2 right)$ $ displaystyle mathrm{Var} X = n(n-1)p^2 + np- (np)^2 $ $ displaystyle mathrm{Var} X = (np)^2-np^2 + np- (np)^2 $ $ displaystyle mathrm{Var} X = -np^2 + np $ $ displaystyle mathrm{Var} X = np(1-p) $ | Proof (normal way) : $ displaystyle mathrm{Var} X = E(X^2)-(EX)^2$ First we find $E(X^2)$ $ displaystyle E(X^2) = sum_{i=0}^n {n choose i} times i^2 times p^i times (1-p)^{n-i}$ we can start the sum from $1$ as $0^{ text{th}}$ term will become zero $ displaystyle E(X^2) = sum_{i=1}^n {n choose i} times i^2 times p^i times (1-p)^{n-i}$ we know that $ displaystyle i cdot i cdot {n choose i} = n cdot i cdot{n-1 choose i-1}$ so we get $ displaystyle E(X^2) = sum_{i=1}^n n cdot i cdot{n-1 choose i-1} times p^i times (1-p)^{n-i}$ $ displaystyle E(X^2) =n cdot p sum_{i=1}^n i cdot{n-1 choose i-1} times p^{i-1} times (1-p)^{n-i}$ $ displaystyle E(X^2) =n cdot p sum_{i=1}^n i cdot{n-1 choose i-1} times p^{i-1} times (1-p)^{(n-1)-(i-1)}$ Consider $n-1=m$ and $i-1=j$, we get $ displaystyle E(X^2) =n cdot p sum_{j=0}^n left( j+1 right) cdot{m choose j} times p^{j} times (1-p)^{m-j}$ $ displaystyle E(X^2) =n cdot p underbrace{ sum_{j=0}^n j cdot{m choose j} times p^{j} times (1-p)^{m-j} }_{ text{same as we did in expectation,so it&#39;s }mp} ; ; ; ; ; ; ; ; ; ; ; ; +n cdot p underbrace{ sum_{j=0}^n {m choose j} times p^{j} times (1-p)^{m-j}}_{ text{sum of all probability }=1} $ $ displaystyle E(X^2) =n cdot p times m cdot p+n cdot p$ $ displaystyle E(X^2) =n cdot p times (n-1) cdot p+ n cdot p$ $ displaystyle E(X^2) =(np)^2-np^2+np$ $ displaystyle E(X^2) =(np)^2+np(1-p) $ Now, $ displaystyle mathrm{Var} X = E(X^2) - (EX)^2$ $ displaystyle mathrm{Var} X = (np)^2+np(1-p) -(np)^2$ $ displaystyle mathrm{Var} X = np(1-p) $ | Proof (yet another way, considering events are independent): if $Y$ is just a Bernoulli trail, $Y= left lbrace begin{array}{ll} 1, &amp; mathrm{with} ; mathrm{probability} ;P 0, &amp; mathrm{with} ; mathrm{probability} ;1-P end{array} right.$ $ mathrm{VarY}={ mathrm{EY}}^2 -{ left( mathrm{EY} right)}^2 = left(1^2 cdot P +0^2 cdot (1-P) right)-P^2 ; ; ; ; ; ; ; ; ; ;=P cdot left(1-P right)$ Let $X$ be multiple copies of independent $Y$ If all $Y$ are independent the we can say $ displaystyle mathrm{Var} left( sum_{i=1}^n Y_i right) = sum_{i=1}^n mathrm{Var} left( Y_i right) $ Proof of above statement: Consider only $2$ Events $Y_1$ and $Y_2$ for simplicity $ displaystyle mathrm{Var}(Y_1+Y_2) = E left[(Y_1+Y_2)^2 right] - left(E[Y_1+Y_2] right)^2 $ $ displaystyle = E left[Y_1^2 + 2Y_1Y_2 + Y_2^2 right] - left((EY_1)^2+2(EY_1)(EY_2)+(EY_2)^2 right) $ $ displaystyle = E(Y_1^2) + 2E(Y_1Y_2) + E(Y_2^2) -(EY_1)^2-2(EY_1)(EY_2)-(EY_2)^2 $ $ displaystyle = E(Y_1^2) -(EY_1)^2 + E(Y_2^2)-(EY_2)^2 + 2(EY_1)(EY_2) -2(EY_1)(EY_2) $ $ displaystyle = mathrm{Var}Y_1+ mathrm{Var}Y_2 $ In Above proof we used $2E(Y_1Y_2)=2(EY_1)(EY_2) $ We can do that if the 2 events are independent Proof of the same: $ displaystyle E(Y_1Y_2)= sum_{Y_1} sum_{Y_2} Y_1 Y_2 P(Y_1=y_1,Y_2=y_2)$ $ displaystyle E(Y_1Y_2)= sum_{Y_1} sum_{Y_2} Y_1 Y_2 P(Y_1=y_1)P(Y_2=y_2) ; ;$ as $Y_1$ and $Y2$ are independent $ displaystyle E(Y_1Y_2)= sum_{Y_1}Y_1 P(Y_1=y_1) sum_{Y_2} Y_2 P(Y_2=y_2)$ $ displaystyle E(Y_1Y_2)= E(Y_1) E(Y_2)$ Hence we can say, $ displaystyle mathrm{Var}X = mathrm{Var} left( sum_{i=1}^n Y_i right) = sum_{i=1}^n mathrm{Var} left( Y_i right) = np(1-p) ; ; ;$ Considering all $Y_i$ has variance of $p(1-p)$ | . | . Poisson . Definition Fix a positive real number $ lambda$. The random variable $X$ is Poisson distributed with parameter $ lambda$, in short $X ∼ mathrm{Poi}( lambda)$, if it is non-negative integer valued, and its mass function is $ displaystyle rho (i) = P left { X=i right } = frac{ lambda^i}{i!} e^{- lambda} ; ; ; ; ;i=0,1,2, dots$ | . import matplotlib.pyplot as plt import math def get_poisson(_lambda): P=[] for i in range(30): P.append((math.pow(_lambda,i)/math.factorial(i))* math.exp(-_lambda)) return P def plot_dist(_lambda): P =get_poisson(_lambda=_lambda) plt.plot(range(len(P)),P,linestyle=&#39;--&#39;, marker=&#39;o&#39;,label=&#39;lambda = &#39;+str(_lambda)) plot_dist(_lambda=1) plot_dist(_lambda=4) plot_dist(_lambda=10) plot_dist(_lambda=15) plt.title(&quot;poisson Distribution&quot;) plt.xlabel(&quot;$i$&quot;) plt.ylabel(&quot;$p(x=i)$&quot;); plt.legend(); . . Take $Y sim mathrm{Binom}(n, p)$ with large $n$, small $p$, such that $np simeq lambda$. Then $Y$ is approximately $ mathrm{Poisson}( lambda)$ distributed. | . Proof of Poisson formula Binomial formula: $ displaystyle p left(i right)=P left lbrace X=i right rbrace = {n choose i} times p^i times { left(1-p right)}^{n-i}$ we say that poisson is approximation of binomial when $n rightarrow infty$, and $ lambda =np Rightarrow p= frac{ lambda}{n}$ so we get $ displaystyle rho left(i right) = lim_{n rightarrow infty}{n choose i} times left( frac{ lambda}{n} right)^i times { left( 1- frac{ lambda}{n} right)}^{n-i}$ $ displaystyle rho left(i right) = lim_{n rightarrow infty} frac{n!}{i!(n-i)!} times left( frac{ lambda}{n} right)^i times { left( 1- frac{ lambda}{n} right)}^{n-i}$ $ displaystyle rho left(i right) = lim_{n rightarrow infty} frac{n cdot (n-1) cdot (n-2) dots cdot (n-i+1)!}{i!} times left( frac{ lambda}{n} right)^i times { left( 1- frac{ lambda}{n} right)}^{n-i}$ $ displaystyle rho left(i right) = lim_{n rightarrow infty} frac{n cdot (n-1) cdot (n-2) dots cdot (n-i+1)!}{i!} times left( frac{ lambda}{n} right)^i times { left( 1- frac{ lambda}{n} right)}^{n} { left( 1- frac{ lambda}{n} right)}^{i}$ $ displaystyle rho left(i right) = frac{ lambda^i}{i!} lim_{n rightarrow infty} frac{ overbrace{n cdot (n-1) cdot (n-2) dots cdot (n-i+1)!}^{i ; ; text{ terms}} }{n^i} times { left( 1- frac{ lambda}{n} right)}^{n} { left( 1- frac{ lambda}{n} right)}^{i}$ $ displaystyle rho left(i right) = frac{ lambda^i}{i!} lim_{n rightarrow infty} frac{n^i + dots}{n^i} times lim_{n rightarrow infty}{ left( 1- frac{ lambda}{n} right)}^{n} times lim_{n rightarrow infty}{ left( 1- frac{ lambda}{n} right)}^{i}$ We know that $ displaystyle ; ; lim_{n rightarrow infty}{ left( 1+ frac{a}{x} right)}^{x} = e^a$ $ displaystyle rho left(i right) = frac{ lambda^i}{i!} underbrace{ lim_{n rightarrow infty} frac{n^i + dots}{n^i}}_{=1} times underbrace{ lim_{n rightarrow infty}{ left( 1- frac{ lambda}{n} right)}^{n}}_{e^{- lambda}} times underbrace{ lim_{n rightarrow infty}{ left( 1- frac{ lambda}{n} right)}^{i}}_{=1, ;i text{ is very small } } $ $ boxed{ displaystyle rho left(i right) = frac{ lambda^i}{i!} e^{- lambda}} $ | . Expectation $ displaystyle EX = lambda $ . $ boxed {EX = lambda}$ $EX = displaystyle sum_{i=0}^ infty i times rho(i) = sum_{i=0}^ infty i times frac{ lambda^i}{i!} e^{- lambda}$ $EX = displaystyle sum_{i=1}^ infty i times frac{ lambda^i}{i!} e^{- lambda}$ $EX = displaystyle lambda sum_{i=1}^ infty i times frac{ lambda^{i-1}}{i!} e^{- lambda}$ $EX = displaystyle lambda sum_{i=1}^ infty frac{ lambda^{i-1}}{(i-1)!} e^{- lambda}$ consider $i-1=j$ $EX = displaystyle lambda underbrace{ sum_{j=0}^ infty frac{ lambda^{j}}{j!} e^{- lambda}}_{=1} $ $EX = displaystyle lambda$ | . Variance $ displaystyle mathrm{Var} X = lambda$ . $ boxed { mathrm{Var} X = lambda}$ $ displaystyle E left[X(X-1) right]= sum_{i=0}^n i(i-1) rho(i)$ $ displaystyle E left[X(X-1) right]= sum_{i=0}^n i(i-1) frac{ lambda ^i}{i!}e^{- lambda}$ $ displaystyle E left[X(X-1) right]= sum_{i=2}^n i(i-1) frac{ lambda ^i}{i!}e^{- lambda}$ $ displaystyle E left[X(X-1) right]= sum_{i=2}^n frac{ lambda ^i}{(i-2)!}e^{- lambda}$ $ displaystyle E left[X(X-1) right]= lambda ^2 sum_{i=2}^n frac{ lambda ^{i-2}}{(i-2)!}e^{- lambda}$ Consder $i-2=j$ $ displaystyle E left[X(X-1) right]= lambda ^2 underbrace{ sum_{j=0}^n frac{ lambda ^{j}}{j!}e^{- lambda}}_{=1} $ $ displaystyle E left[X(X-1) right]= lambda ^2$ Now, $ displaystyle mathrm{Var}X = E(X^2)-(EX)^2$ $ displaystyle mathrm{Var}X = E left[X(X-1) right] +EX-(EX)^2$ $ displaystyle mathrm{Var}X = lambda^2 + lambda- lambda^2$ $ displaystyle mathrm{Var}X = lambda$ | . Geometric . Definition Suppose that independent trials, each succeeding with probability $p$, are repeated until the first success. The total number $X$ of trials made has the $ mathrm{Geometric}(p)$ distribution (in short, $X sim mathrm{Geom}(p)$). $X$ can take on positive integers, with probabilities $ boxed{ displaystyle rho(i) = (1-p)^{i-1} cdot p}$ $ ; ; ; ; ; i=1,2,3, dots$ That is a function, we verify by $ rho(i) ge 0$ and $ displaystyle sum_{i=1}^ infty rho(i) = sum_{i=1}^ infty (1-p)^{i-1} cdot p = frac{p}{1-(1-p)}=1$ | Remarks For a Geometric$(p)$ random variable and any $k ge 1$ we have $P left { X ge k right }=(1-p)^{k-1}$ (We have at least $k-1$ failures). | Corollary The Geometric random variable is (discrete) memory-less :for every $k ge 1 , n ge 0$ $ displaystyle P left { X ge n + k mid X&gt;n right }=P left { X ge k right }$ Proof : $ displaystyle P left { X ge n + k mid X&gt;n right }= frac {P left { X ge n + k right } cap P left {X&gt;n right }}{P left {X&gt;n right }}$ if $X ge n+k$ then $X$ is also greater than $n$ as $k$ is at least $1$ $ displaystyle P left { X ge n + k mid X&gt;n right }= frac {P left { X ge n + k right } }{P left {X&gt;n right }}$ $ displaystyle P left { X ge n + k mid X&gt;n right }= frac {(1-p)^{n+k-1} }{(1-p)^n}= (1-p)^{k-1}=P left { X&gt;k right }$ | . Expectation $ displaystyle EX = frac{1}{p} $ . $ boxed {EX = frac{1}{p}}$ proof (using trick): Trick : $ displaystyle i= frac{d}{dt}^{i}{ Big|}_{t=1} ,$ $ displaystyle EX = sum_{i=1}^ infty i cdot (1-p)^{i-1} cdot p$ $ displaystyle EX = sum_{i=0}^ infty frac{d}{dt}{t}^{i}{ Big|}_{t=1} (1-p)^{i-1} cdot p$ $ displaystyle EX = frac{d}{dt} left( sum_{i=0}^ infty {t}^{i} (1-p)^{i-1} cdot p right) { Bigg|}_{t=1}$ $ displaystyle EX = frac{d}{dt} left( frac{p}{1-p} sum_{i=0}^ infty {t}^{i} (1-p)^{i} right) { Bigg|}_{t=1}$ $ displaystyle EX = frac{p}{1-p} cdot frac{d}{dt} left( frac{1}{1-t(1-p)} right) { Bigg|}_{t=1}$ $ displaystyle EX = frac{p}{1-p} cdot left( frac{1-p}{ left(1-t(1-p) right)^2} right) { Bigg|}_{t=1}$ $ displaystyle EX = frac{1}{p}$ | Proof (normal way): $ displaystyle EX = sum_{i=1}^ infty i cdot (1-p)^{i-1} cdot p$ $ displaystyle EX ; ; ; ; ; ; ; ; ; ; ;= p + 2 cdot (1-p) cdot p + 3 cdot (1-p)^2 cdot p ; dots ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; (1)$ $ displaystyle EX(1-p) = ; ; ; ; ; ; ; ; ; ; ; (1-p) cdot p + 2 cdot (1-p)^2 cdot p + 3 cdot (1-p)^3 cdot p ; dots ; ; ; ; ; (2)$ Subtracting equation $(1)$ from $(2)$ we get $ displaystyle p cdot EX = p + (1-p) cdot p + (1-p)^2 cdot p ; dots $ $ displaystyle EX = 1 + (1-p) + (1-p)^2 ; dots $ $ displaystyle EX = frac{1}{1-(1-p)}$ $ displaystyle EX = frac{1}{p}$ | . | . Variance $ displaystyle mathrm{Var} X = frac{1-p}{p^2} $ . $ boxed { mathrm{Var} X = frac{1-p}{p^2}}$ Proof (using trick); Trick : $ displaystyle i(i-1)= frac{d}{dt^2}^{i}{ Big|}_{t=1} ,$ First find $E[X(X-1)]$ $ displaystyle E[X(X-1)] = sum_{i=1}^ infty i times(i-1) times (1-p)^{i-1} times p $ $ displaystyle E[X(X-1)] = sum_{i=1}^ infty frac{d}{dt^2}{t}^{i}{ Big|}_{t=1} times (1-p)^{i-1} times p $ $ displaystyle E[X(X-1)] =p frac{d}{dt^2} left( sum_{i=1}^ infty{t}^{i}(1-p)^{i-1} right) { Bigg|}_{t=1} $ $ displaystyle E[X(X-1)] = frac{p}{1-p} cdot frac{d}{dt^2} left( sum_{i=1}^ infty{t}^{i}(1-p)^{i} right) { Bigg|}_{t=1} $ $ displaystyle E[X(X-1)] = frac{p}{1-p} cdot frac{d}{dt^2} left( frac{1}{1-t(1-p)} right) { Bigg|}_{t=1} $ $ displaystyle E[X(X-1)] = frac{p}{1-p} cdot left( frac{2(1-p)(1-p)}{( left( 1-t(1-p) right)^3 } right) { Bigg|}_{t=1} $ $ displaystyle E[X(X-1)] = frac{p}{1-p} cdot left( frac{2(1-p)(1-p)}{p^3 } right) $ $ displaystyle E[X(X-1)] = left( frac{2(1-p)}{p^2 } right) $ Now, $ displaystyle mathrm{Var}X = E(X^2)-(EX)^2$ $ displaystyle mathrm{Var}X = E left[X(X-1) right] +EX-(EX)^2$ $ displaystyle mathrm{Var}X = left( frac{2(1-p)}{p^2 } right) + frac{1}{p} - left( frac{1}{p} right)^2$ $ displaystyle mathrm{Var}X = left( frac{2-2p}{p^2 } right) + frac{1}{p} - left( frac{1}{p} right)^2$ $ displaystyle mathrm{Var}X = frac{2-2p+p-1}{p^2}$ $ displaystyle mathrm{Var}X = frac{1-p}{p^2}$ | proof (normal way): we first solve for $E(X^2)$ $ displaystyle E(X^2) = left( sum_{i=1}^ infty i^2 times (1-p)^{i-1} times p right)$ $ displaystyle E(X^2) ; ; ; ; ; ; ; ; ; ; ;= p + 4 cdot (1-p) cdot p + 9 cdot (1-p)^2 cdot p ; ; dots ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; (3)$ $ displaystyle (1-p) E(X^2) = ; ; ; ; ; ; ; ; ; ; ;(1-p) cdot p + 4 cdot (1-p)^2 cdot p + 9 cdot (1-p)^3 cdot p ; ; dots ; ; (4)$ subtracting eqution $(4)$ from equation $(3)$ $ displaystyle E(X^2) = p+3 cdot (1-p) + 5 cdot (1-p)^2 + 7 cdot (1-p)^3 ; dots$ This is AGP, the sum is given by $s_{ infty}= frac{a}{1-r}+ frac{dr}{(1-r)^2}, ; ; ; ;$ for $r&lt;1$ In our case $r=(1-p), ;a=1, ;d=2 ;$ so we get, $ displaystyle E(X^2) = frac{1}{1-(1-p)}+ frac{2 cdot (1-p)}{(1-(1-p))^2}$ $ displaystyle E(X^2) = frac{1}{p}+ frac{2-2p}{p^2}$ $ displaystyle E(X^2) = frac{p+2-2p}{p^2}$ $ displaystyle E(X^2) = frac{2-p}{p^2}$ $ displaystyle mathrm{Var}X = E(X^2) - (EX)^2$ $ displaystyle mathrm{Var}X = frac{2-p}{p^2} - left( frac{1}{p} right)^2$ $ displaystyle mathrm{Var}X = frac{1-p}{p^2} $ | . | .",
            "url": "https://abhiyantaabhishek.github.io/MDS-IIT/mathematical%20foundations%20of%20data%20science/2022/09/17/CS6660-week4_1.html",
            "relUrl": "/mathematical%20foundations%20of%20data%20science/2022/09/17/CS6660-week4_1.html",
            "date": " • Sep 17, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Support Vector Machines CS5590 week 4",
            "content": "Overview and History . It is a discriminative classifier. | Inspired by Statistical Learning. | Developed in 1992 by Vapnik, Guyon, Boser | Was one of the go-to methods in ML since mid 1990s (only recently displaced by deep learning.) | . Maximum Margin Classifier . Formulation $f( mathbf{X}, mathbf{W},b)= mathrm{sign} ( mathbf{W} cdot mathbf{X}+b)$ | Basic formulation of SVM can only handle two classes. | There are improvised method to handle more than tow class. | The Maximum margin classifier is the linear classifier with the maximum margin. This is the simplest kind of SVM ( called an LSVM). | . | . Estimate the Margin . The points those lies on the two margin lines are called support vector. | The model is immune to removal of any non-support-vector data points. | The equation of the line is given by $ mathbf{W}^T cdot mathbf{X}+b=0$ | $ mathbf{W}$ is always normal to the line $ mathbf{W}^T cdot mathbf{X}+b=0$ This can be proved by taking two vector $ mathbf{X_1}$ and $ mathbf{X_2}$ on line $ mathbf{W}^T cdot mathbf{X}+b=0$, Now if we subtract the two vector we get $ mathbf{W}^T( mathbf{X_1}- mathbf{X_2})= mathbf{0} Leftrightarrow ( mathbf{X_1}- mathbf{X_2}) perp mathbf{W}$. The same is explained on stack overflow. | Dotted line $ mathbf{X&#39;-X}$ is perpendicular to decision boundary so parallel to $ mathbf{W} $ let it&#39;s length (magnitude) be $r$ | The Unit vector along Dotted line $ mathbf{X&#39;-X}$ is given by $ frac{ mathit{ mathbf{W}}}{ left lVert mathit{ mathbf{W}} right rVert }$ | The equation of the dotted line $ mathbf{X&#39;-X}$ can be also given by magnitude multiplied by unit vector : $ displaystyle r cdot frac{ mathit{ mathbf{W}}}{ left lVert mathit{ mathbf{W}} right rVert }$ | But as the dotted line can be on any side of the main line so we need to multiply with $y$, as $y$ takes value of $1$ or $-1$ depending on the side: $ displaystyle mathbf{X&#39;-X} = yr cdot frac{ mathit{ mathbf{W}}}{ left lVert mathit{ mathbf{W}} right rVert }$ $ displaystyle mathbf{X&#39;} = mathbf{X} - yr cdot frac{ mathit{ mathbf{W}}}{ left lVert mathit{ mathbf{W}} right rVert }$ | Now since $ mathbf{X&#39;}$ lies on the line so we can write $ mathbf{W}^T cdot mathbf{X&#39;}+b=0$ | Substituting value of $ mathbf{X&#39;}$ in $ mathbf{W}^T cdot mathbf{X&#39;}+b=0$ we get: $ displaystyle mathbf{W}^T cdot left( mathbf{X} - yr cdot frac{ mathit{ mathbf{W}}}{ left lVert mathit{ mathbf{W}} right rVert } right)+b=0$ | substituting $ displaystyle left lVert mathit{ mathbf{W}} right rVert = sqrt{ mathit{ mathbf{W}}^T mathit{ mathbf{W}}}$ in above equation we get: $ displaystyle mathbf{W}^T cdot left( mathbf{X} - yr cdot frac{ mathit{ mathbf{W}}}{ sqrt{ mathit{ mathbf{W}}^T mathit{ mathbf{W}}}} right)+b=0$ $ displaystyle left( mathbf{W}^T mathbf{X} - yr cdot frac{ mathit{ mathbf{W}}^T mathit{ mathbf{W}}}{ sqrt{ mathit{ mathbf{W}}^T mathit{ mathbf{W}}}} right)+b=0$ $ displaystyle left( mathbf{W}^T mathbf{X} - yr cdot sqrt{ mathit{ mathbf{W}}^T mathit{ mathbf{W}}} right)+b=0$ $ displaystyle mathbf{W}^T mathbf{X} - yr cdot left lVert mathit{ mathbf{W}} right rVert +b=0$ $ displaystyle mathbf{W}^T mathbf{X} +b = yr cdot left lVert mathit{ mathbf{W}} right rVert$ $ displaystyle r = frac{ mathbf{W}^T mathbf{X} +b}{y cdot left lVert mathit{ mathbf{W}} right rVert}$ | Since $y$ takes value of only $1$ or $-1$, hence we can bring $y$ to numerator. . $ displaystyle r = y frac{ mathbf{W}^T mathbf{X} +b}{ left lVert mathit{ mathbf{W}} right rVert}$ . | . Since $ mathbf{W}^T cdot mathbf{X}+b=0$ and $c left( mathbf{W}^T cdot mathbf{X} right)+b=0$ define the same plane, we have the freedom to choose the normalization of $ mathbf{W}$ | Let us choose normalization such that $ mathbf{W}^T cdot mathbf{X}_+ + b = +1$ and $ mathbf{W}^T cdot mathbf{X}_- +b = -1$ for the positive and negative support vectors respectively. Hence, Margin now is: $ displaystyle left( +1 right) frac{ mathbf{W}^T mathbf{X_+} +b}{ left lVert mathit{ mathbf{W}} right rVert} + left( -1 right) frac{ mathbf{W}^T mathbf{X_-} +b}{ left lVert mathit{ mathbf{W}} right rVert}$ Since $ mathbf{W}^T mathbf{X_+} +b=+1$ and $ mathbf{W}^T mathbf{X_-} +b=-1$, substituting these in above equation we get: $ displaystyle left( +1 right) frac{ mathbf{W}^T mathbf{X_+} +b}{ left lVert mathit{ mathbf{W}} right rVert} + left( -1 right) frac{ mathbf{W}^T mathbf{X_-} +b}{ left lVert mathit{ mathbf{W}} right rVert}= displaystyle left( +1 right) frac{+1}{ left lVert mathit{ mathbf{W}} right rVert} + left( -1 right) frac{-1}{ left lVert mathit{ mathbf{W}} right rVert}= frac{2}{ left lVert mathbf{W} right rVert}$ | . . Tip: Margin between the two support vector is given by: $$ displaystyle frac{2}{ left lVert mathbf{W} right rVert}$$ . Maximize the Margin . Now we know the margin between the two support vector. | We need to maximize the margin in such a way that $+1$ class points lies on one side of the margin and $-1$ class points lies on the other side of the margin. | We can formulate this as the quadratic optimization problem: Find $ mathbf{W}$ such that $ displaystyle rho = frac{2}{ left lVert mathbf{W} right rVert } $ is maximized; and for all $ left { left( mathbf{X}_i, mathbf{y}_i right) right }$ and $ mathbf{W}^T cdot mathbf{X}_i+b ge 1$ if $y_i=+1$ and $ mathbf{W}^T cdot mathbf{X}_i+b le -1$ if $y_i=-1$ | . A better formulation is to minimize inverse of $ rho$ instead of maximizing it. . We know that $ displaystyle max frac{2}{ left lVert mathbf{W} right rVert } = min frac{ left lVert mathbf{W} right rVert}{2} = min frac{ sqrt{ mathbf{W}^T mathbf{W}}}{2} $ | Instead of minimizing $ displaystyle frac { left lVert mathbf{W} right rVert}{2}$ we minimize $ displaystyle frac { left lVert mathbf{W} right rVert^2}{2} = displaystyle frac{ mathbf{W}^T mathbf{W}}{2} $ as both (with or without square) are equivalent. we select square one as math (derivative) becomes easy. . Tip: Maximization problem can be written in terms of minimization as follows:Find $ mathbf{W}$ and $b$ such that$ displaystyle frac{ mathbf{W}^T mathbf{W}}{2}$ is minimized.and for all $ left { left( mathbf{X}_i, mathbf{y}_i right) right } : displaystyle y_i left( mathbf{W}^T mathbf{X}_i +b right) ge 1$ | . Using Lagrange Multipliers . Basics of Lagrange Multipliers . Optimization problem: Minimize : $ displaystyle f left( overrightarrow{x} right)$ Such that for all $i,$ $ displaystyle g_i left( overrightarrow{x} right) le 0$ | To solve the above problem we create augmented Lagrange function: $ displaystyle L left( overrightarrow{x}, overrightarrow{ lambda} right):=f left( overrightarrow{x} right)+ sum_{i=1}^{n} lambda_ig_i left( overrightarrow{x} right)$ $ displaystyle underbrace{L left( overrightarrow{x}, overrightarrow{ lambda} right)}_{ text{lagrange function}} :=f left( overrightarrow{x} right)+ sum_{i=1}^{n} underbrace{ lambda_i}_{ text{lagrange variable or dual varialbe }}g_i left( overrightarrow{x} right)$ | Observation: For any feasible $x$ and all $ lambda_i ge 0$, $ displaystyle L left( overrightarrow{x}, overrightarrow{ lambda} right):=f left( overrightarrow{x} right)+ overbrace{ sum_{i=1}^{n} overbrace{ lambda_i}^{ text{this is positve}} underbrace{g_i left( overrightarrow{x} right)}_{ text{this is negative}}}^{ text{This is negative}} $ Hence , $ displaystyle L left( overrightarrow{x}, overrightarrow{ lambda} right) le f left( overrightarrow{x} right)$ $ displaystyle Longrightarrow max_{ lambda_i ge 0} L left( overrightarrow{x}, overrightarrow{ lambda} right) le f left( overrightarrow{x} right)$ | So, the optimal value to the constrained optimization: $ displaystyle p^*:= min_{ overrightarrow{x} } max_{ lambda_i ge 0} L left( overrightarrow{x}, overrightarrow{ lambda} right) $ We can see that now problem becomes unconstrained in $x$ Also $p^*$ is called The primal problem | Observation: consider a function: $ displaystyle min_{ overrightarrow{x} } L left( overrightarrow{x}, overrightarrow{ lambda} right) $ Since $p^*$ is solution for maximum possible $ lambda$ so for any feasible $x$ and all $ lambda_i ge 0$ $ displaystyle p^* ge min_{ overrightarrow{x} } L left( overrightarrow{x}, overrightarrow{ lambda} right) $ Thus: $ displaystyle d^*:= max_{ lambda_i ge 0} min_{ overrightarrow{x} } L left( overrightarrow{x}, overrightarrow{ lambda} right) le p^*$ Also $d^*$ is called The dual problem | . In short: . . Note: Optimization problem:Minimize : $ displaystyle f left( overrightarrow{x} right)$ Such that for all $i,$ $ displaystyle g_i left( overrightarrow{x} right) le 0$ Lagrange Function : $ displaystyle L left( overrightarrow{x}, overrightarrow{ lambda} right):=f left( overrightarrow{x} right)+ sum_{i=1}^{n} lambda_ig_i left( overrightarrow{x} right)$ Primal: $ displaystyle p^*:= min_{ overrightarrow{x} } max_{ lambda_i ge 0} L left( overrightarrow{x}, overrightarrow{ lambda} right) $ Dual: $ displaystyle d^*:= max_{ lambda_i ge 0} min_{ overrightarrow{x} } L left( overrightarrow{x}, overrightarrow{ lambda} right) $ . Theorem (weak Lagrangian duality): $d^* le p^*$ This is also called as minimax inequality $p^*-d^*$ is called duality gap . | There are certain condition when duality gap becomes zero, for that we need to understand convexity . A function $f: mathbb{R}^d rightarrow mathbb{R} $ is called convex iff for any two point $x$ and $x&#39;$ and $ beta in left[ 0,1 right]$ $f left( beta overrightarrow{x}+ left( 1- beta right) overrightarrow{x} right) le beta f left( overrightarrow{x} right)+ left( 1- beta right)f left( overrightarrow{x} right)$ | A set $S subset mathbb{R}^d $ is called conved iff for any tow points $x, x&#39; in S$ and any $ beta in left[ 0,1 right]$ $ beta overrightarrow{x}+ left( 1- beta right) overrightarrow{x} in S $ | Convex Optimization problem $ displaystyle min_{ overrightarrow{x} in mathbb{R}^d } f left( overrightarrow{x} right)$ subject to: $ displaystyle g_i left( overrightarrow{x} right) le 0$ for $1 le i le n$ is called convex optimization problem if: The objective function $f left( overrightarrow{x} right)$ is convex function, and | the feasible set induced by the constraints $g_i$ is a convex set. | . | . | Theorem (strong Lagrangian duality): if $f$ is convex and for a feasible point $x^*$ $g_i left( overrightarrow{x^*} right)&lt;0$, or $g_i left( overrightarrow{x^*} right) le 0$ when $g$ is affine Then $d^*=p^*$ This is called Slater&#39;s condition. | . SVM standard (primal) form . $ displaystyle min_{w,b} frac{1}{2} left lVert overrightarrow{w} right rVert^2 $ such that: $ forall$ $i,$ $y_i left( overrightarrow{w} cdot overrightarrow{x_i} +b right) ge 1$ . Observations : Objective function is convex | the constraints are affine, inducing a polytope constraint set. | . | So SVM is a convex optimization problem (in fact a quadratic program) | Moreover, strong duality holds. | . Lagrangian for SVM For Lagrangian the constraint should always written as less than $0$ format: $y_i left( overrightarrow{w} cdot overrightarrow{x_i} +b right) -1 ge 0$ $- y_i left( overrightarrow{w} cdot overrightarrow{x_i} +b right) +1 le 0$ $ 1 - y_i left( overrightarrow{w} cdot overrightarrow{x_i} +b right) le 0$ | Now the Lagrangian for SVM can be written as: $ displaystyle L left( overrightarrow{w},b, overrightarrow{ alpha} right)= frac{1}{2} left lVert overrightarrow{w} right rVert^2 + underbrace{ sum_{i = 1}^{n} alpha_i left( 1-y_i left( overrightarrow{w} cdot overrightarrow{x_i}+b right) right)}_{ text{appears like a hinge loss}} $ | . | . SVM Dual . Primal $ displaystyle min_{ overrightarrow{w},b } max_{ overrightarrow{ alpha } ge 0 } frac{1}{2} left lVert overrightarrow{w} right rVert^2 + sum_{i = 1}^{n} alpha_i left( 1-y_i left( overrightarrow{w} cdot overrightarrow{x_i}+b right) right) $ | Dual $ displaystyle max_{ overrightarrow{ alpha } ge 0 } min_{ overrightarrow{w},b } frac{1}{2} left lVert overrightarrow{w} right rVert^2 + sum_{i = 1}^{n} alpha_i left( 1-y_i left( overrightarrow{w} cdot overrightarrow{x_i}+b right) right) $ | Slater&#39;s condition from convex optimization guarantees that these two optimization problems are equivalent! | . Solving using KKT condition . KKT stands for Karush-Kuhn-Tucker Condition . We solve Dual problem: $ displaystyle max_{ overrightarrow{ alpha } ge 0 } min_{ overrightarrow{w},b } frac{1}{2} left lVert overrightarrow{w} right rVert^2 + sum_{i = 1}^{n} alpha_i left( 1-y_i left( overrightarrow{w} cdot overrightarrow{x_i}+b right) right) $ | . We can solve for optimal $w$, $b$ as function of $ alpha$ $ displaystyle frac{ partial L }{ partial overrightarrow{w}}= w - sum_{i} alpha_iy_i overrightarrow{x}_i=0 Rightarrow w = sum_{i} alpha_iy_i overrightarrow{x}_i $ $ displaystyle frac{ partial L }{ partial b}= sum_{i} alpha_iy_i=0 Rightarrow sum_{i} alpha_iy_i =0$ | . substituting these values back in Dual we get: $ displaystyle max_{ overrightarrow{ alpha } ge 0 } frac{1}{2} left( sum_{i} alpha_iy_i overrightarrow{x}_i right) cdot left( sum_{j} alpha_jy_j overrightarrow{x}_j right) + sum_{i = 1}^{n} alpha_i left( 1-y_i left( sum_{j} alpha_jy_j overrightarrow{x}_j cdot overrightarrow{x_i}+b right) right) $ $ displaystyle max_{ overrightarrow{ alpha } ge 0 } frac{1}{2} sum_{i,j} alpha_i alpha_jy_iy_j overrightarrow{x}_i cdot overrightarrow{x} _j + sum_{i = 1}^{n} alpha_i- sum_{i = 1}^{n} left( alpha_iy_i left( sum_{j} alpha_jy_j overrightarrow{x} _j cdot overrightarrow{x_i}+b right) right) $ $ displaystyle max_{ overrightarrow{ alpha } ge 0 } frac{1}{2} sum_{i,j} alpha_i alpha_jy_iy_j overrightarrow{x}_i cdot overrightarrow{x} _j + sum_{i = 1}^{n} alpha_i- sum_{i = 1}^{n} alpha_iy_i sum_{j} alpha_jy_j overrightarrow{x} _j cdot overrightarrow{x_i}+ sum_{i = 1}^{n} alpha_iy_ib $ $ displaystyle max_{ overrightarrow{ alpha } ge 0 } - frac{1}{2} sum_{i,j} alpha_i alpha_jy_iy_j overrightarrow{x}_i cdot overrightarrow{x} _j + sum_{i = 1}^{n} alpha_i + sum_{i = 1}^{n} alpha_iy_ib $ $ displaystyle max_{ overrightarrow{ alpha } ge 0 } sum_{i = 1}^{n} alpha_i - frac{1}{2} sum_{i,j} alpha_i alpha_jy_iy_j overrightarrow{x}_i cdot overrightarrow{x} _j $ | . The above equation can also be written as: $ displaystyle max sum_{k = 1}^{R} alpha_k - frac{1}{2} sum_{k=1}^{R} sum_{l = 1}^{R} alpha_k alpha_l Q_{kl}, $ where $ displaystyle Q_{kl} =y_ky_l left( mathbf{X_k} cdot mathbf{X_l} right)$ subject to constrains:$ alpha_k ge 0,$ and $ forall k , $ $ displaystyle sum_{k=1}^{R} alpha_ky_k=0$ | . Above problem can be solved using SMO (Sequential minimal optimization) or any other quadratic programming or gradient descent. | Once we solve we get optimum $ alpha^*$ | Using $ alpha^*$ we can get $w^*$ as below: $ displaystyle w^* = sum_{i} alpha_i^*y_i overrightarrow{x}_i $ | $b^*$ can be calculated as follows: $y_i left( overrightarrow{w}^* cdot overrightarrow{x_i} +b right) = 1$ $y_i left( overrightarrow{w}^* cdot overrightarrow{x_i} right) + y_ib = 1$ Multiplying $y_i$ both the sides: $y_i y_i left( overrightarrow{w}^* cdot overrightarrow{x_i} right) + y_i y_ib = y_i$ $y_i y_ib = y_i -y_i y_i left( overrightarrow{w}^* cdot overrightarrow{x_i} right) $ $y_i y_i b$ can be written as $b$ because $y_i$ can be only $ pm 1$ so in either case $y_i y_i =1$ $b = y_i left( 1- y_i left( overrightarrow{w}^* cdot overrightarrow{x_i} right) right)$ $b^* = - y_i left( y_i left( overrightarrow{w}^* cdot overrightarrow{x_i} right)- 1 right)$ | . Now we can classify with: $f( mathbf{X}, mathbf{W}^*,b^*)= mathrm{sign} ( mathbf{W}^* cdot mathbf{X}+b^*)$ | . Soft Margin SVM . Till now what we discussed is called Hard margin SVM. | In practice data is not always separable | We allow misclassification of the data point in soft margin SVM. | Soft margin SVM is also called as C-SVM | . Now our Lagrangian is formulated as below: $ displaystyle min_{w,b, varepsilon} frac{1}{2} left lVert overrightarrow{w} right rVert^2 + c sum_{j=1}^{N} varepsilon_j $ such that: $ forall$ $i,$ $y_i left( overrightarrow{w} cdot overrightarrow{x_i} +b right) ge 1- varepsilon _i$, and $ varepsilon _i ge0$ . | Significance of $ varepsilon$ . $ displaystyle min_{w,b, varepsilon} frac{1}{2} left lVert overrightarrow{w} right rVert^2 + overbrace{c}^{ text{Controls amount of misclassification}} underbrace{ sum_{j=1}^{N} varepsilon_j }_{ text{Minimize } varepsilon text{ also } } $ such that: $ forall$ $i,$ $y_i left( overrightarrow{w} cdot overrightarrow{x_i} +b right) ge 1- underbrace{ varepsilon _i}_{ text{to allow misclassification}} ,$ and $ overbrace{ varepsilon _i ge0}^{ text{keep } varepsilon text{ positive} } $ | $ varepsilon_i ge 1 Longleftrightarrow y_i left( overrightarrow{w} cdot overrightarrow{x_i} +b right) &lt; 0,$ i.e., misclassification. | $0&lt; varepsilon_i &lt; 1 Longleftrightarrow x_i$ is correctly classified, but lies inside the margin | $ varepsilon_i =0 Longleftrightarrow x_i$ is correctly classified, and lies outside of margin | $ sum_{j=1}^{N} varepsilon_j$ is an upper bound on training errors. | . | . Lagrangian for Soft Margin SVM . We need to Solve: $ displaystyle min_{w,b, varepsilon} frac{1}{2} left lVert overrightarrow{w} right rVert^2 + c sum_{i=1} varepsilon_i $ such that: $ forall$ $i,$ $y_i left( overrightarrow{w} cdot overrightarrow{x_i} +b right) ge 1- varepsilon _i$, and $ varepsilon _i ge0$ | For Lagrangian the constraint should always written as less than $0$ condition: $y_i left( overrightarrow{w} cdot overrightarrow{x_i} +b right) ge 1- varepsilon _i$, and $ varepsilon _i ge0$ $y_i left( overrightarrow{w} cdot overrightarrow{x_i} +b right) - 1 + varepsilon _i ge 0$, and $- varepsilon _i le 0$ $1 - varepsilon _i -y_i left( overrightarrow{w} cdot overrightarrow{x_i} +b right) le 0$, and $- varepsilon _i le 0$ | Lagrangian formulation of above problem: $ displaystyle L left( overrightarrow{w},b, overrightarrow{ alpha}, overrightarrow{ beta}, overrightarrow{ varepsilon } right)= frac{1}{2} left lVert overrightarrow{w} right rVert^2 +c sum_{i} varepsilon_i + sum_{i = 1}^{n} alpha_i left( 1- varepsilon _i - y_i left( overrightarrow{w} cdot overrightarrow{x_i}+b right) right) - sum_{i} beta_i varepsilon_i $ | . Primal $ displaystyle min_{ left( overrightarrow{w},b, overrightarrow{ varepsilon } right)} max_{ left( overrightarrow{ alpha } ge 0 , overrightarrow{ beta} ge 0 right)} frac{1}{2} left lVert overrightarrow{w} right rVert^2 +c sum_{i} varepsilon_i + sum_{i = 1}^{n} alpha_i left( 1- varepsilon _i - y_i left( overrightarrow{w} cdot overrightarrow{x_i}+b right) right) - sum_{i} beta_i varepsilon_i $ | Dual $ displaystyle max_{ left( overrightarrow{ alpha } ge 0 , overrightarrow{ beta} ge 0 right)} min_{ left( overrightarrow{w},b, overrightarrow{ varepsilon } right)} frac{1}{2} left lVert overrightarrow{w} right rVert^2 +c sum_{i} varepsilon_i + sum_{i = 1}^{n} alpha_i left( 1- varepsilon _i - y_i left( overrightarrow{w} cdot overrightarrow{x_i}+b right) right) - sum_{i} beta_i varepsilon_i $ | . Same as Hard margin SVM we use KKT condition and solve minimization of dual: $ displaystyle frac{ partial L }{ partial overrightarrow{w}}= w - sum_{i} alpha_iy_i overrightarrow{x}_i=0 Rightarrow w = sum_{i} alpha_iy_i overrightarrow{x}_i $ $ displaystyle frac{ partial L }{ partial b}= sum_{i} alpha_iy_i=0 Rightarrow sum_{i} alpha_iy_i =0$ $ displaystyle frac{ partial L }{ partial varepsilon _i}= c - alpha_i - beta_i = 0 Rightarrow c= beta_i+ alpha_i$ Observation: $ overbrace{c}^{ text{Upper bound of } alpha text{ and } beta} = underbrace{ beta_i}_{ text{always +ve}} + underbrace{ alpha_i}_{ text{always +ve}}$ so we can say: $0 le alpha_i le c$ $ forall i$ | . | . substituting these values back in Dual we get: $ displaystyle Rightarrow max_{ overrightarrow{ alpha } ge 0 , overrightarrow{ beta} ge 0} frac{1}{2} left( sum_{i} alpha_iy_i overrightarrow{x}_i right) cdot left( sum_{j} alpha_jy_j overrightarrow{x}_j right) + sum_{i} varepsilon_i left( beta_i+ alpha_i right) + sum_{i = 1}^{n} alpha_i left( 1- varepsilon _i - y_i left( sum_{j} alpha_jy_j overrightarrow{x}_j cdot overrightarrow{x_i}+b right) right) - sum_{i} beta_i varepsilon_i $ $ displaystyle Rightarrow max_{ overrightarrow{ alpha } ge 0 , overrightarrow{ beta} ge 0} frac{1}{2} left( sum_{i} alpha_iy_i overrightarrow{x}_i right) cdot left( sum_{j} alpha_jy_j overrightarrow{x}_j right) + sum_{i} varepsilon_i beta_i+ sum_{i} varepsilon_i alpha_i + sum_{i = 1}^{n} alpha_i - sum_{i = 1}^{n} alpha_i varepsilon _i - sum_{i = 1}^{n} alpha_i y_i left( sum_{j} alpha_jy_j overrightarrow{x}_j cdot overrightarrow{x_i}+b right) - sum_{i} beta_i varepsilon_i $ $ displaystyle Rightarrow max_{ overrightarrow{ alpha } ge 0 , overrightarrow{ beta} ge 0} frac{1}{2} sum_{i} alpha_i alpha_j y_i y_j overrightarrow{x}_i overrightarrow{x}_j + sum_{i} varepsilon_i beta_i+ sum_{i} varepsilon_i alpha_i + sum_{i = 1}^{n} alpha_i - sum_{i = 1}^{n} alpha_i varepsilon _i - sum_{i = 1}^{n} alpha_i y_i sum_{j} alpha_jy_j overrightarrow{x}_j cdot overrightarrow{x_i}+ sum_{i = 1}^{n} alpha_i y_ib - sum_{i} beta_i varepsilon_i $ $ displaystyle Rightarrow max_{ overrightarrow{ alpha } ge 0 } frac{1}{2} sum_{i} alpha_i alpha_j y_i y_j overrightarrow{x}_i overrightarrow{x}_j + sum_{i = 1}^{n} alpha_i - sum_{i = 1}^{n} alpha_i y_i sum_{j} alpha_jy_j overrightarrow{x}_j cdot overrightarrow{x_i}+ sum_{i = 1}^{n} alpha_i y_ib $ $ displaystyle Rightarrow max_{ overrightarrow{ alpha } ge 0 } frac{1}{2} sum_{i} alpha_i alpha_j y_i y_j overrightarrow{x}_i overrightarrow{x}_j + sum_{i = 1}^{n} alpha_i - sum_{i=1,j=1}^{n} alpha_i alpha_j y_i y_j overrightarrow{x_i} cdot overrightarrow{x_j}+ sum_{i = 1}^{n} alpha_i y_ib $ $ displaystyle Rightarrow max_{ overrightarrow{ alpha } ge 0 } sum_{i = 1}^{n} alpha_i - frac{1}{2} sum_{i} alpha_i alpha_j y_i y_j overrightarrow{x}_i overrightarrow{x}_j $ | . Notice neither $ overrightarrow{ beta}$ nor $ overrightarrow{ varepsilon }$ appears in the above equation. | . The above equation can also be written as: $ displaystyle max sum_{k = 1}^{R} alpha_k - frac{1}{2} sum_{k=1}^{R} sum_{l = 1}^{R} alpha_k alpha_l Q_{kl}, $ where $ displaystyle Q_{kl} =y_ky_l left( mathbf{X_k} cdot mathbf{X_l} right)$ subject to constrains:$0 le alpha_k le c,$ and $ forall k , $ $ displaystyle sum_{k=1}^{R} alpha_ky_k=0$ | . . Note: One of the constraint of soft margin SVM is $0 le alpha_k le c$ which is different for hard margin SVM constraint $ alpha_k ge 0$ . Multi-class Classification with SVMs . SVM can handle only tow-class outputs. | what to do for multi-class case: one vs all SVM Learn N SVMs | SVM 1 learns class $1$ vs not class $1$ | SVM 2 learns class $2$ vs not class $2$ and so on. | Then to predict the output for a new point, just predict with each SVM and fond out which one puts the prediction the furthest into the positive region. | . | Other approaches: pair-wise SVM | Tree-structured SVM | . | . | . Kernel Trick . Why do we require the Kernel Trick . We found that after solving minimization problem of dual of SVM we get following: $ displaystyle max sum_{k = 1}^{R} alpha_k - frac{1}{2} sum_{k=1}^{R} sum_{l = 1}^{R} alpha_k alpha_l Q_{kl}, $ where $ displaystyle Q_{kl} =y_ky_l left( mathbf{X_k} cdot mathbf{X_l} right)$ subject to constrains:$0 le alpha_k le c,$ and $ forall k , $ $ displaystyle sum_{k=1}^{R} alpha_ky_k=0$ | . But if the data can&#39;t be separated linearly we transform the data to higher dimension space using the transformation $ phi$. So that the data can be separated using a hyper-plane in higher dimension space. In that case the above equation changes as below : $ displaystyle max sum_{k = 1}^{R} alpha_k - frac{1}{2} sum_{k=1}^{R} sum_{l = 1}^{R} alpha_k alpha_l Q_{kl}, $ where $ displaystyle Q_{kl} =y_ky_l underbrace{ left( mathbf{ Phi} left( mathbf{X}_k right) cdot mathbf{ Phi} left( mathbf{X}_l right) right)}_{ text{Notice the term } Phi } $ subject to constrains:$0 le alpha_k le c,$ and $ forall k , $ $ displaystyle sum_{k=1}^{R} alpha_ky_k=0$ | . Then compute : $ displaystyle mathbf{W} = sum_{ text{k s.t } alpha_k &gt;0 } alpha_k^*y_k mathbf{ Phi} left( mathbf{X}_k right)$ | Then classify with $ displaystyle f( mathbf{X},w,b)= mathrm{sign} left( mathbf{W} cdot mathbf{ Phi}( mathbf{X})+b right)$ | . Most important change : $ mathbf{X} rightarrow mathbf{ Phi}( mathbf{X})$ | . Notice that in the term $ displaystyle Q_{kl} =y_ky_l left( mathbf{ Phi} left( mathbf{X}_k right) cdot mathbf{ Phi} left( mathbf{X}_l right) right) $ we must do $ frac{R^2}{2}$ dot products to get this matrix ready. Assuming a quadratic polynomial kernel, each dot product requires $ frac{m^2}{2}$ addition and multiplication ( where $m$ is the dimension of $X$) The whole thing costs $ frac{R^2m^2}{4}$ This is the reason we require a trick so that we need not do this large computation. | . How do we do the kernel Trick . To understand we create a data in circular fashion as shown below: . import matplotlib.pyplot as plt import numpy as np import random import math def get_points(rl,rh): npoints = 1000 # points to chose from r = np.random.uniform(low=rl, high=rh, size=npoints) t = np.linspace(0, 2*np.pi, npoints, endpoint=False) x = r * np.cos(t) y = r * np.sin(t) return x,y fig = plt.figure(figsize=(4,4)) x11,x21=get_points(2,4) plt.scatter(x11,x21); x12,x22=get_points(6,8) plt.scatter(x12,x22); plt.xlabel(&#39;x1&#39;) plt.ylabel(&#39;x2&#39;); . . The two circle can&#39;t be separated by a line. | Now we transform the data to 3 dimension using below function. $ phi left( mathbf{X} right) = phi left( left( begin{array}{c} x_1 x_2 end{array} right) right) = left( begin{array}{c} x_1^2 sqrt{2}x_1x_2 x_2^2 end{array} right)$ | Python implementation of the same is shown below: | . def transform(x1,x2): return np.square(x1),np.sqrt(2)*x1*x2,np.square(x2) . We can see in below pic that how data becomes sparable in 3 dimensional space. | . from mpl_toolkits import mplot3d import matplotlib.pyplot as plt fig = plt.figure(figsize=(8,8)) ax = plt.axes(projection=&#39;3d&#39;) x11,x21,x31 = transform(x11,x21) x12,x22,x32 = transform(x12,x22) ax.scatter3D(x11,x21,x31); ax.scatter3D(x12,x22,x32); ax.set_xlabel(&#39;x1&#39;) ax.set_ylabel(&#39;x2&#39;) ax.set_zlabel(&#39;x3&#39;); ax.view_init(10, 80); . . In case of SVM we need the dot product of the transformed data, not the transformed data itself. | . Consider two vectors $ mathbf{a}$ and $ mathbf{b}$, we will apply following transformation on it : $ phi left( mathbf{X} right) = phi left( left( begin{array}{c} x_1 x_2 end{array} right) right) = left( begin{array}{c} x_1^2 sqrt{2}x_1x_2 x_2^2 end{array} right)$ We get below result: $ phi left( mathbf{a} right)^T phi left( mathbf{b} right)= left( begin{array}{c} a_1^2 sqrt{2}a_1a_2 a_2^2 end{array} right)^T cdot left( begin{array}{c} b_1^2 sqrt{2}b_1b_2 b_2^2 end{array} right)= a_1^2 b_1^2 + 2 a_1 b_1 a_2 b_2 +a_2^2 b_2^2= left( a_1b_1+a_2b_2 right)^2= left( left( begin{array}{c} a_1 a_2 end{array} right)^T cdot left( begin{array}{c} b_1 b_2 end{array} right) right)^2 = left( mathbf{a}^T cdot mathbf{b} right)^2$ | . The kernel function here is polynomial function. | we can see that we don&#39;t even need to use $ phi$ we can get the result just from $ left( mathbf{a}^T cdot mathbf{b} right)^2$ | So we never need to transformed the data to higher domain still we get the same benefit at the less computation cost. | The same is explained below: | . fig = plt.figure(figsize=(16,16)) x11,x21=get_points(2,4) x12,x22=get_points(6,8) same_domain_result = np.square(x11*x12+x21*x22) ax = fig.add_subplot(2, 2, 1) ax.scatter(x11,x21); ax.scatter(x12,x22); ax.set_xlabel(&#39;x1&#39;) ax.set_ylabel(&#39;x2&#39;); ax.set_title(&#39; Original domain&#39;) x11,x21,x31 = transform(x11,x21) x12,x22,x32 = transform(x12,x22) transformed_domain_result = x11*x12+x21*x22+x31*x32 ax = fig.add_subplot(2, 2, 2, projection=&#39;3d&#39;) ax.scatter3D(x11,x21,x31); ax.scatter3D(x12,x22,x32); ax.set_xlabel(&#39;x1&#39;) ax.set_ylabel(&#39;x2&#39;) ax.set_zlabel(&#39;x3&#39;); ax.view_init(10, 80); ax.set_title(&#39;Transformed domain &#39;) ax = fig.add_subplot(2, 2, 3) ax.scatter(range(len(same_domain_result)),same_domain_result) ax.set_title(&#39;Result in original domain $( mathbf{a}^T cdot mathbf{b} )^2$&#39;) ax = fig.add_subplot(2, 2, 4) ax.scatter(range(len(transformed_domain_result)),transformed_domain_result) ax.set_title(&#39;Result in transformed domain $ phi( mathbf{a})^T phi( mathbf{b})$ &#39;); . . From above figure we can see that we get exact same result in original domain without transforming data to the higher dimensional space | Not all functions are kernel functions. Need to be decomposable: $K(a,b)= phi(a) cdot phi(b)$ | . | Mercer&#39;s condition : To expand kernel function $K(X,Y)$ into a dot product, i.e. $K(x,y)= Phi(x). Phi(y)$, $K(x,y)$ has to be positive semi-definite function, i.e., for any function $f(X)$ whose $ displaystyle int f^2(x)dx$ is finite, the following inequality holds: $$ displaystyle int dx dy f(x) K(x,y) f(y) ge 0$$ | . It is not easy to select the kernel function which will work best for the given data. | RBF kernels are considered good in general, especially for images (and other smooth functions/data). | For discrete data, chi-square kernel is preferred. | we can also do Multiple Kernel learning. | If still it doesn&#39;t work we can use cross-validation to select a kernel function from some basic options. | Same kernel trick can also be applied to other methods including: Kernel k-NN | Kernel Perceptron | Kernelized Linear Regression | etc. | . | .",
            "url": "https://abhiyantaabhishek.github.io/MDS-IIT/foundations%20of%20machine%20learning/2022/09/10/CS5590-week4.html",
            "relUrl": "/foundations%20of%20machine%20learning/2022/09/10/CS5590-week4.html",
            "date": " • Sep 10, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Linear Algebra CS6660 week 3",
            "content": "system of Linear equations . Let us see some Systems of linear equations A company produce products $ N_1, dots ,N_n $ for which resources $R_1, dots ,R_m$ are required. To produce a unit of porduct $N_j, a_{ij}$ units of resources $R_i$ are needed, where $i=1, dots ,m$ and $j=1, dots ,n$. The objective is to find an optimal production plan, i.e., a plan of how many units $x_j$ of product $N_j$ should be produced if a total of $b_i$ units of resource $R_i$ are available and (ideally) no resources are left over. write system of equation of the same $$ left lbrace begin{array}{c} a_{11}x_1 + dots +a_{1n}x_n =b_1 vdots a_{m1}x_1+ dots +a_{mn}x_n=b_n end{array} right.$$ | John received an inheritance of $ $12000$ that he divided in three parts and invested in three ways: in a money market fund paying $3 %$ annual interest; in municipal bonds paying $4 %$ of annual interest; and in mutual fund paying $7 %$ of interest, john invested $ $4,000$ more in mutual funds than in municipal bonds. He earned $ $670$ in interest the first year.How much did john invest in each type of found $$ left lbrace begin{array}{c} x+y+z=12000 0.03x+0.04y+0.07z=670 -x+z=4000 end{array} right. $$ | solution of above question: $x=3500,y=1000,z=7500$ | . | . . If there are more variables than equations than there might be infinitely many solutions. | If $AX=b$ has infinitely many solutions then to find other solutions we find $Y$ such that $AY=b$, then we can write $A(X+Y)=b$ as $A(X+Y)=AX+AY=AX$ because $AY=0$ here, $X+Y$ is another solution here. If $AX=b$ and $AY=0$ then $A(X+Y)=b$ | . Particular and general solutions $1$. Find a particular solution to $AX=b$ | $2$. Find all the solutions to $AX=0$ | $3$. Combine the solution from step $1$ and $2$ to get the general solution | . | . For Example, consider ${ left[ begin{array}{l l l}{1}&amp;{0}&amp;{8}&amp;{-4} {0}&amp;{1}&amp;{2}&amp;{12} end{array} right]} { left[ begin{array}{l}{x_1} {x_2} end{array} right]}={ left[ begin{array}{l}{42} {8} end{array} right]}$ solution for $AX=b$ (particular solution) ${ left[ begin{array}{l l l}{1}&amp;{0}&amp;{8}&amp;{-4} {0}&amp;{1}&amp;{2}&amp;{12} end{array} right]} { left[ begin{array}{l}{x_1} {x_2} end{array} right]}={ left[ begin{array}{l}{48} {8} end{array} right]}$ is given by $ { left[ begin{array}{l}{x_1} {x_2} end{array} right]}={ left[ begin{array}{l}{42} {8} end{array} right]}$ Now we find solution for $AX=0$ (general solution ) ${ left[ begin{array}{l l l}{1}&amp;{0}&amp;{8}&amp;{-4} {0}&amp;{1}&amp;{2}&amp;{12} end{array} right]} { left[ begin{array}{l}{x_1} {x_2} end{array} right]}={ left[ begin{array}{l}{0} {0} end{array} right]}$ We can find that there are two vectors which satisfy it $ { left[ begin{array}{l}{x_1} {x_2} {x_3} {x_4} end{array} right]}={ left[ begin{array}{l}{8} {2} {-1} {0} end{array} right]}$ $ { left[ begin{array}{l}{x_1} {x_2} {x_3} {x_4} end{array} right]}={ left[ begin{array}{l}{-4} {12} {0} {-1} end{array} right]}$ We can represent the final solution by combining particular and general solution $ left {x in mathbb{R}^{4}:x= { left[ begin{array}{r}{42} {8} {0} {0} end{array} right]} + lambda_{1}{ left[ begin{array}{r}{8} {2} {-1} {0} end{array} right]} + lambda_{2}{ left[ begin{array}{r}{-4} {12} {0} {-1} end{array} right]}, ; ; lambda_{1}, lambda_{2} in mathbb{R} right }$ | .",
            "url": "https://abhiyantaabhishek.github.io/MDS-IIT/mathematical%20foundations%20of%20data%20science/2022/09/03/CS6660-week3.html",
            "relUrl": "/mathematical%20foundations%20of%20data%20science/2022/09/03/CS6660-week3.html",
            "date": " • Sep 3, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Decision Trees, Naive Bayes  CS5590  week 3",
            "content": "Decision Trees . An efficient nonparametric method. | A hierarchical model. | Divide and conquer strategy. | Internal decision nodes Univariate : It uses a single attribute $X_i$ Numeric $X_i$: If numeric data perform Binary split:$X_i&gt;w_m$ | . | Discrete $X_i$: For discrete data perform n- way split for n possible values | . | . | Multivariate: It uses more than one attributes, $X$ | . | Leaves Classification : Class labels, or proportions | Regression : Numeric, r average, or local fit | . | Learning is greedy; find the best split recursively. | For node $m$, $N_m$ instances reach $m$, $N_m^i$ belong to $C_i$ $ hat P(C_i|X,m) equiv p_m^i = frac{N_m^i}{N_m} $ | Node $m$ is pure if $p_m^i$ is $0$ or $1$ | Measure if impurity is entropy $ displaystyle I_m=- sum_{i=1}^kp_m^i log_2p_m^i$ | . compare probability distributions vs entropy . from matplotlib import pyplot import matplotlib.pyplot as plt import numpy as np # calculate entropy def entropy(events, ets=1e-15): return -sum([p * np.log2(p + ets) for p in events]) # define probabilities probs = np.arange(0.0001,1,0.001) # create probability distribution dists = [[p, 1.0 - p] for p in probs] # calculate entropy for each distribution ents = [entropy(d) for d in dists] # plot probability distribution vs entropy plt.plot(probs, ents) plt.title(&#39;Probability Distribution vs Entropy for 2 class problem&#39;) plt.xlabel(&#39;Probability Distribution&#39;) plt.ylabel(&#39;Entropy (bits)&#39;) plt.show() . . Entropy in information theory specifies the average (expected) amount of information derived from observing an event . . How to generate decision tree . Select a root not which divides the data best based on impurity measures. | If node is pure, generate a leaf and stop, otherwise split and continue recursively. | Impurity after split: It is probability weighted entropy given by: $ displaystyle I_m^{ prime }=- sum_{j=1}^{n} frac{N_{mj}}{N_m} sum_{i=1}^kp_{mj}^i log_2p_{mj}^i$, here, $N_{mj}$ is $j^{th}$ branch of $N_m$ and $N_{mj}^i$ belongs to $i^{th}$ class. | . | . | Information gain: Expected reduction in impurity measure after split. Chose the attribute with maximum information gain. | Other impurity measure method - Gini impurity/index : $ displaystyle 1- sum_{j=1}^cp_j^2$ | . Overfitting and generalization . Noisy training example or if only small number of samples are associated leaf nodes can cause overfitting. | Using Pruning for better generalization Pruning is the process of removing subtree. Pre-pruning: Early stopping, after a predetermined performance. | Post-pruning: Grow the whole then prune the subtree which overfit on the pruning set | . | Pre-pruning is faster, post-pruning is more accurate. | . | . Occam&#39;s Razor Principle . When multiple hypotheses can solve the problem chose the simplest one . Select best tree . Measure performance over training and separate validation data set | Minimum Description Length : Minimize size(tree)+size(miscalssifications(tree)) | . Rule Extraction from Trees . Convert tree to equivalent set of rules.(&quot;if else&quot; condition for example). | Prune each rules independently of others, by removing any pre-conditions that result in improving its estimates accuracy. | Sort final rules into desired sequence for use. | . from sklearn.datasets import load_iris from sklearn import tree import matplotlib.pyplot as plt iris = load_iris() X, y = iris.data, iris.target clf = tree.DecisionTreeClassifier() clf = clf.fit(X, y) . fig, ax = plt.subplots(1, 1, figsize=(10, 10)) tree = tree.plot_tree(clf,ax=ax) . Naive Bayes . . In Naive Bayes classifier goal is to learn function $f:X rightarrow y$, where $y$ is one of $k$ classes and $X=X_1,...,X_n$: values of attributes (numeric or categorical) | It is a probabilistic classification most probable class given observation: $ displaystyle hat{y}= arg max_y P(y|x)$ | Bayesian probability of a class: $ displaystyle P left( Y|X right)= frac{P left( X|Y right)P left( Y right)}{ sum_{y&#39;}P left( X|Y&#39; right)P left( Y&#39; right)}$ | . | . Formulation . consider a record with attributes $A_1,A_2, dots ,A_n$ | Goal is to predict class $C$ | Specifically, we want to find the value of $C$ that maximizes $P(C|A_1,A_2, dots,A_n)$ | . . what is Naive about Naive Bayes? The attributes are considered independent of each other, this is Naive in Naive Bayes. | . | . AS we assume independence among attributes $A_i$ so we can write: $P(A_1,A_2, dots ,A_n|C_j)=P(A_i|C_j)P(A_2|C_j) dots P(A_n|C_j)$ | New point is classified to $C_j$ if $P(C_j) prod_{j}P(A_i|C_j)=P(C_j)P(A_i|C_j)(A_2|C_j) dots P(A_n|C_j) $ is maximal . | Assume that all hypotheses (classes) are equally probable a priori, i.e., $P(C_i)=P(C_j)$ for all $i,j$ | This is called assuming a uniform prior. It simplifies computing the posterior: $ displaystyle C_{ML}= arg max_c P(A_1,A_2, dots A_n|C)$ | This hypothesis is called the maximum likelihood hypothesis . | . Example . Given a data as shown in as shown in below data frame, Find if tennis will be played for a scenario given by $X$: $X=( mathrm{ Outlook = Sunny, Temperature= Cool, Humidity =High, Wind= Strong})$ . import pandas as pd Attributes =[&#39;Day&#39;, &#39;Outlook&#39;, &#39;Temperature&#39;, &#39;Humidity&#39;, &#39;Wind&#39;, &#39;Play Tennis&#39;] data =[[&#39;D1&#39;, &#39;Sunny&#39;, &#39;Hot&#39;, &#39;High&#39;, &#39;Weak&#39;, &#39;No&#39; ], [&#39;D2&#39;, &#39;Sunny&#39;, &#39;Hot&#39;, &#39;High&#39;, &#39;Strong&#39;, &#39;No&#39; ], [&#39;D3&#39;, &#39;Overcast&#39;, &#39;Hot&#39;, &#39;High&#39;, &#39;Weak&#39;, &#39;Yes&#39; ], [&#39;D4&#39;, &#39;Rain&#39;, &#39;Mild&#39;, &#39;High&#39;, &#39;Weak&#39;, &#39;Yes&#39; ], [&#39;D5&#39;, &#39;Rain&#39;, &#39;Cool&#39;, &#39;Normal&#39;, &#39;Weak&#39;, &#39;Yes&#39; ], [&#39;D6&#39;, &#39;Rain&#39;, &#39;Cool&#39;, &#39;Normal&#39;, &#39;Strong&#39;, &#39;No&#39; ], [&#39;D7&#39;, &#39;Overcast&#39;, &#39;Cool&#39;, &#39;Normal&#39;, &#39;Strong&#39;, &#39;Yes&#39; ], [&#39;D8&#39;, &#39;Sunny&#39;, &#39;Mild&#39;, &#39;High&#39;, &#39;Weak&#39;, &#39;No&#39; ], [&#39;D9&#39;, &#39;Sunny&#39;, &#39;Cool&#39;, &#39;Normal&#39;, &#39;Weak&#39;, &#39;Yes&#39; ], [&#39;D10&#39;, &#39;Rain&#39;, &#39;Mild&#39;, &#39;Normal&#39;, &#39;Weak&#39;, &#39;Yes&#39; ], [&#39;D11&#39;, &#39;Sunny&#39;, &#39;Mild&#39;, &#39;Normal&#39;, &#39;Strong&#39;, &#39;Yes&#39; ], [&#39;D12&#39;, &#39;Overcast&#39;, &#39;Mild&#39;, &#39;High&#39;, &#39;Strong&#39;, &#39;Yes&#39; ], [&#39;D13&#39;, &#39;Overcast&#39;, &#39;Hot&#39;, &#39;Normal&#39;, &#39;Weak&#39;, &#39;Yes&#39; ], [&#39;D14&#39;, &#39;Rain&#39;, &#39;Mild&#39;, &#39;High&#39;, &#39;Strong&#39;, &#39;No&#39; ]] df = pd.DataFrame(columns=Attributes,data=data) df . Day Outlook Temperature Humidity Wind Play Tennis . 0 D1 | Sunny | Hot | High | Weak | No | . 1 D2 | Sunny | Hot | High | Strong | No | . 2 D3 | Overcast | Hot | High | Weak | Yes | . 3 D4 | Rain | Mild | High | Weak | Yes | . 4 D5 | Rain | Cool | Normal | Weak | Yes | . 5 D6 | Rain | Cool | Normal | Strong | No | . 6 D7 | Overcast | Cool | Normal | Strong | Yes | . 7 D8 | Sunny | Mild | High | Weak | No | . 8 D9 | Sunny | Cool | Normal | Weak | Yes | . 9 D10 | Rain | Mild | Normal | Weak | Yes | . 10 D11 | Sunny | Mild | Normal | Strong | Yes | . 11 D12 | Overcast | Mild | High | Strong | Yes | . 12 D13 | Overcast | Hot | Normal | Weak | Yes | . 13 D14 | Rain | Mild | High | Strong | No | . from IPython.display import display_html all_tables=&quot;&quot; for col in df.columns[1:-1]: table=pd.DataFrame() for c in df[&#39;Play Tennis&#39;].unique(): for r in df[col].unique(): n = df.loc[(df[col]==r) &amp; (df[&#39;Play Tennis&#39;]==c) ,[col]].count().to_numpy()[0] d = df.loc[(df[&#39;Play Tennis&#39;]==c) ,[col]].count().to_numpy()[0] table.loc[col+&#39;_&#39;+str(r),&#39;Play_Tennis&#39;+&#39;_&#39;+c]=&quot;{}/{}&quot;.format(n,d) table_styler = table.style.set_table_attributes(&quot;style=&#39;display:inline&#39;&quot;).set_caption(col) all_tables=all_tables+table_styler._repr_html_() display_html (all_tables,raw=True) . Outlook &nbsp; Play_Tennis_No Play_Tennis_Yes . Outlook_Sunny 3/5 | 2/9 | . Outlook_Overcast 0/5 | 4/9 | . Outlook_Rain 2/5 | 3/9 | . Temperature &nbsp; Play_Tennis_No Play_Tennis_Yes . Temperature_Hot 2/5 | 2/9 | . Temperature_Mild 2/5 | 4/9 | . Temperature_Cool 1/5 | 3/9 | . Humidity &nbsp; Play_Tennis_No Play_Tennis_Yes . Humidity_High 4/5 | 3/9 | . Humidity_Normal 1/5 | 6/9 | . Wind &nbsp; Play_Tennis_No Play_Tennis_Yes . Wind_Weak 2/5 | 6/9 | . Wind_Strong 3/5 | 3/9 | . Above table shows conditional probabilities For example $p( mathrm{outlook}= mathrm{sunny} mid mathrm{play Tennis} = mathrm{no})$ is given by row outlook_Sunny and column Play_Tennis_No of table Outlook and $p( mathrm{Temperature}= mathrm{Cool} mid mathrm{play Tennis} = mathrm{Yes})$ is given by row Temperature_Cool and column Play_Tennis_Yes of table Temperature Also we can calculate below 2 probabilities of the two classes: $P( mathrm{Play}= mathrm{Yes})= frac{9}{14}$ $P( mathrm{Play}= mathrm{No})= frac{5}{14}$ . Form look up table: $P left( mathrm{Outlook}= mathrm{Sunny} mid mathrm{Play} = mathrm{Yes} right) = frac{2}{9}$ $P left( mathrm{Outlook}= mathrm{Sunny} mid mathrm{Play} = mathrm{No} right) = frac{3}{5}$ $P left( mathrm{Temperature}= mathrm{Cool} mid mathrm{Play} = mathrm{Yes} right) = frac{3}{9}$ $P left( mathrm{Temperature}= mathrm{Cool} mid mathrm{Play} = mathrm{No} right) = frac{1}{5}$ $P left( mathrm{Humidity}= mathrm{High } mid mathrm{Play} = mathrm{Yes} right) = frac{3}{9}$ $P left( mathrm{Humidity}= mathrm{High } mid mathrm{Play} = mathrm{No} right) = frac{4}{5}$ $P left( mathrm{Wind}= mathrm{Strong } mid mathrm{Play} = mathrm{Yes} right) = frac{3}{9}$ $P left( mathrm{Wind}= mathrm{Strong } mid mathrm{Play} = mathrm{No} right) = frac{3}{5}$ . | MAP Rule: $P left( mathrm{Yes} mid X right)= P left( mathrm{Outlook}= mathrm{Sunny} mid mathrm{Play} = mathrm{Yes} right) times P left( mathrm{Temperature}= mathrm{Cool} mid mathrm{Play} = mathrm{Yes} right) times P left( mathrm{Humidity}= mathrm{High } mid mathrm{Play} = mathrm{Yes} right) times P left( mathrm{Wind}= mathrm{Strong } mid mathrm{Play} = mathrm{Yes} right) times P( mathrm{Play}= mathrm{Yes}) =0.0053 $ $P left( mathrm{No} mid X right)= P left( mathrm{Outlook}= mathrm{Sunny} mid mathrm{Play} = mathrm{No} right) times P left( mathrm{Temperature}= mathrm{Cool} mid mathrm{Play} = mathrm{No} right) times P left( mathrm{Humidity}= mathrm{High } mid mathrm{Play} = mathrm{No} right) times P left( mathrm{Wind}= mathrm{Strong } mid mathrm{Play} = mathrm{No} right) times P( mathrm{Play}= mathrm{No}) = 0.0206$ Since $P( mathrm{Play}= mathrm{Yes}) &lt; P( mathrm{Play}= mathrm{No})$ so $X$ shall be labeled to be &quot;No&quot;, i.e. Given scenario $X$ tennis will not be played. . | . Pros and Cons . Combines prior knowledge and observed data | Output is not only a classification but a probability distribution over all classes | Robust to isolated noise points. | Handle missing values by ignoring instance during probability estimate calculation. | Robust to irrelevant attributes. | with each training example, the prior and the likelihood can be updated dynamically | Independence assumption may not hold always. | . Practical Issues . Discretize the range into bins One ordinal attribute per bin | Violates independence assumption | . | Two way split : (A &lt; v) or (A &lt; v) choose only one of the two splits as new attribute. | . | Probability density estimation: Assume attribute follows a parametrized distribution, e.g. normal distribution. | Use data to estimate parameters of distribution, e.g. mean and standard deviation using maximum likelihood estimation. | Once probability distribution is known, can use it to estimate the conditional probability, $P(A_i mid c)$ | . | . Bayesian Belief network ( Bayesian net ) . Describe conditional independence among subset of variables (attributes) : combining prior knowledge about dependencies among variables with observed training data. For example consider below graph: . . Here Age, Occupation and Income determine if customer will byt this product, Given that customer buys product, whether there is interest in insurance is now independent of Age, Occupation, Income. $P left( mathrm{Age,Occ,Inc,Buy,Ins} right)=P( mathrm{Age})P( mathrm{Occ})P left( mathrm{Inc} right)P left( mathrm{Buy} mid mathrm{Age, Occ, Inc} right)P left( mathrm{Int} mid mathrm{Buy} right)$ . Naive Bayes Classifier category . It is an Inductive Learning. | It is a generative Modeling. | It can be Parametric or Non-parametric Models. | It can be online or offline Models. | .",
            "url": "https://abhiyantaabhishek.github.io/MDS-IIT/foundations%20of%20machine%20learning/2022/08/27/CS5590-week3.html",
            "relUrl": "/foundations%20of%20machine%20learning/2022/08/27/CS5590-week3.html",
            "date": " • Aug 27, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Probability CS6660 Quiz 1",
            "content": "Question 1 . Suppose X is a random variable that takes values from the set {0, 1}. If $E(X) = 0.3$, then what is $E(X^2)$? . Answer:we know for Indicator random variable $E left(X right)=0 cdot p left(0 right)+1 cdot p left(1 right)$ so substituting values we get : $0 ldotp 3=1 ldotp p left(1 right)$ $ Rightarrow p left(1 right)=0 ldotp 3 ;,p left(0 right)=0 ldotp 7$ Now: $E left(X^2 right)=0^2 cdot p left(0 right)+1^2 cdot p left(1 right)$ $ Rightarrow E left(X^2 right)=1 cdot p left(1 right)=0 ldotp 3$ . Question 2 . Suppose $A$ and $B$ are two mutually exclusive events with $P(A) = 0.3$ and $P(B) = 0.4$. Then $P(A|B) =?$ and $P(A|B^C) =?$ . Answer:$p left(A|B right)= frac{p left(A cap B right)}{p left(B right)}$ $p left(A cap B right)=0$, mutually exclusive $ Rightarrow p left(A|B right)=0$ Now: $p left(A|B^c right)= frac{p left(A cap B^c right)}{p left(B^c right)}$ $ Rightarrow p left(A|B^c right)= frac{p left(A right)-p left(A cap ;B right)}{p left(B^c right)}$ $ Rightarrow p left(A|B^c right)= frac{0 ldotp 3-0}{1-0 ldotp 4}=0 ldotp 5$ . Question 3 . Suppose $X$ and $Y$ are random variables such that $E(X) = 10, Var(X) = 50, E(Y) = 5,$ and $Var(Y) = 40.$ Then what is $E(2X+Y)?$ . Answer:$E left(2X+Y right)=E left(2X right)+E left(Y right)=2E left(X right)+E left(Y right)$ $ Rightarrow 2 times 10+5=25$ . Question 4 . Suppose $A$ and $B$ are events such that $P(A) = 0.3, P(B) = 0.2$ and $P(A cap B) = 0.1$. Then what is $P(A^c cap B^C)$? . Answer:$P left(A^c cap B^c right)={P left(A cup ;B right)}^c$ $P left(A cup B right)=P left(A right)+P left(B right)-P left(A cap B right)$ $ Rightarrow P left(A cup B right)=0 ldotp 3+0 ldotp 2-0 ldotp 1$ $ Rightarrow P left(A cup B right)=0 ldotp 4$ ${P left(A cup B right)}^c =1-P left(A cup B right)=1-0 ldotp 4=0 ldotp 6$ . Question 5 . Suppose $A$ and $B$ are independent events such that $P(A) = 0.2$ and $P(B) = 0.5$. Then what is $P(A cup B)$? . Answer:$P left(A cup B right)=P left(A right)+P left(B right)-P left(A cap B right)$ $P left(A cap ;B right)=P left(A right) cdot P left(B right)$, Because $A$ and $B$ are independent here. Hence, $P left(A cup B right)=P left(A right)+P left(B right)-P left(A right) cdot P left(B right)$ $ Rightarrow P left(A cup B right)=0 ldotp 2+0 ldotp 5-0 ldotp 2*0 ldotp 5=0 ldotp 6$ . Question 6 . Suppose $X$ is a random variable such that $E(X) = 10$ and $E(X^2) = 150$. What is the variance of $X$? . Answer:$ mathrm{Var} left(X right)=E left(X^2 right)-{ left(E left(X right) right)}^2$ $ Rightarrow mathrm{Var} left(X right)=150-{10}^2 =50$ . Question 7 . Suppose $A$ and $B$ are events such that $P(A) = 0.8, P(B) = 0.6,$ and $P(A^c cap B^c) = 0.2.$ Then which of the following is true? . Neither A nor B is contained in the other | A and B are mutually exclusive | None of the choices can be inferred | A is a subset of B | B is a subset of A | . Answer:$P left(A^c cap B^c right)={P left(A cup B right)}^c$ $P left(A cup B right)=1-{P left(A cup B right)}^c =1-0 ldotp 2=0 ldotp 8$ $P left(A cup B right)=P left(A right)+P left(B right)+P left(A cap B right)$ $ Rightarrow 0 ldotp 8=0 ldotp 8+0 ldotp 6+P left(A cap B right)$ $ Rightarrow P left(A cap B right)=0 ldotp 6$ Notice: $P left(A right)=0 ldotp 8,P left(B right)=0 ldotp 6 ;,P left(A cap B right)=0 ldotp 6$ Hence B is subset of A . Question 8 . Suppose you roll two fair dice and look at the sum of the two results. The probability of getting a value of at least (greater than or equal to) 10 is ? . Answer:The sum value of 10, 11 and 12 qualifies for at least greater than or equal to 10 so the events are ${(6,4),(5,5),(4,6),(6,5),(5.6),(6,6)}$ so the probability is $ frac{6}{36}= frac{1}{6} $ . Question 9 . Let $X$ be a random variable which takes values from $ left lbrace 1,2,3 right rbrace$ with equal probability. What is the variance of $X$? . Answer:$ mathrm{Var} left(X right)=E left(X^2 right)-{ left(E left(X right) right)}^2$ $E left(X right)= frac{1+2+3}{3}=2$ $E left(X^2 right)= frac{1^2 +2^2 +3^2 }{3}= frac{14}{3}$ $ Rightarrow mathrm{Var} left(X right)= frac{14}{3}-4= frac{2}{3}$ . Question 10 . Let $X$ be a random variable which takes values from $ left lbrace 10,20,30 right rbrace$ with equal probability. What is the variance of $X$? . Answer:The answer can be calculated as in question 9, but we can notice that the values is scaled by $10$ as compared to question 9, so the variance will be scaled by $10^2$, hence the answer is $ frac{200}{3}$ . Question 11 . Suppose the probability of a random person of the population having covid is $0.1$. Given that the person has covid, the probability of the person having cough is $0.9$. Given that the person does not have covid, the probability of having cough is $0.01$. Suppose a random person from the population has cough. What is the probability that he/she has covid? . Answer:Given: $P left( mathrm{covid} right)=0 ldotp 1$ $p left( mathrm{cough}| mathrm{covid} right)=0 ldotp 9$ $p left( mathrm{cough}|{ mathrm{covid}}^c right)=0 ldotp 01$ Find: $p left( mathrm{covid}| mathrm{cough} right)=?$ calculate probability of not covid $P left({ mathrm{covid}}^c right)=1-P left( mathrm{covid} right)=1-0 ldotp 1=0 ldotp 9$ Use Bayes theorem $p left( mathrm{covid}| mathrm{cough} right)= frac{p left( mathrm{cough}| mathrm{covid} right) times p left( mathrm{covid} right)}{p left( mathrm{cough}| mathrm{covid} right) times p left( mathrm{covid} right)+p left( mathrm{cough}|{ mathrm{covid}}^c right) times p left({ mathrm{covid}}^c right)}$ $ Rightarrow p left( mathrm{covid}| mathrm{cough} right)= frac{0 ldotp 9 times 0 ldotp 1}{0 ldotp 9 times 0 ldotp 1+0 ldotp 01 times 0 ldotp 9}= frac{10}{11}$ . Question 12 . Let $A$ be an event, and let $P(A)$ denote the probability of the event $A$. Which of the following options is correct? . $P(A)$ is at most $1$ but has no lower bound (unbounded) | $P(A)$ is at least $0$ and at most $1$ | $P(A)$ is at least $0$ but has no upper bound (unbounded) | $P(A)$ has neither an upper bound nor a lower bound | . Answer:P(A) is at least 0 and at most 1 . Question 13 . Let $A$ and $B$ be two events, such that $P(A) = 1$ and $P(B) &gt; 0$. Which of the following is true? . A and B are nether independent nor mutually exclusive | A and B are independent and mutually exclusive | A and B are mutually exclusive but not independent | A and B are independent, but not mutually exclusive | Cannot conclude any of the choices from the information given. | . Answer: . Question 14 . Suppose there is a meeting of 5 friends. The probability that a given member of the population is covid positive is 0.1. Which of the following is an upper bound on the probability that at least one in the meeting is covid positive? . 0.05 | 0.00001 | 0.25 | 0.5 | 0.3 | . Answer:$p left( mathrm{covid} right)=0 ldotp 1$ $ mathrm{At} ; mathrm{least} ; mathrm{one}= mathrm{complement} ; mathrm{of} ; mathrm{none}$ $p left( mathrm{at} ; mathrm{least} ; mathrm{one} ; mathrm{covid} ; mathrm{positive} right)={p left( mathrm{none} ; mathrm{covid} ; mathrm{positive} right)}^c$ $p left( mathrm{none} ; mathrm{covid} ; mathrm{positive} right)=0 ldotp 9 times 0 ldotp 9 times 0 ldotp 9 times 0 ldotp 9 times 0 ldotp 9=0 ldotp 59$ $p left( mathrm{at} ; mathrm{least} ; mathrm{one} ; mathrm{covid} ; mathrm{positive} right)=1-0 ldotp 59=0 ldotp 41$ So the upper bound is 0.5 .",
            "url": "https://abhiyantaabhishek.github.io/MDS-IIT/quizzes%20and%20assignments%20-%20mathematical%20foundations%20of%20data%20science/2022/08/20/CS6660-Quiz-1.html",
            "relUrl": "/quizzes%20and%20assignments%20-%20mathematical%20foundations%20of%20data%20science/2022/08/20/CS6660-Quiz-1.html",
            "date": " • Aug 20, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Machine Learning Model Evaluation Measures and KNN CS5590  week 2",
            "content": "Evaluation Measures . Classification It is a measure of being right/wrong,0-1, eg: hinge loss, cross entropy loss | . | Regression loss It is a measure if how close we are to target, eg: MEA, MES | . | Ranking/search It is a measure of top K search | . | Clustering How well we have described the data ( not straight forward) | . | . Is accuracy adequate . Accuracy may not not be useful in cases where: . There is a large class skew. | There are differential misclassification cost, say getting a positive wrong costs more than getting a negative wrong. | we are most interested in a subset of high confidence predictions. | . Classification Error . . . Tip: Precision = How many retrieved items are relevant? Recall = How many relevant items are retrieved? . . Tip: sensitivity = Probability of positive test given a patient has a disease. Specificity = Probability of a negative test given a patient is well.Specificity = 1 - False Alarm . Utility and cost . Detection Cost: cost = $C_{ mathrm{FP}} times mathrm{FP}+C_{ mathrm{FN}} times mathrm{FN}$ | . | Fmeasure $F1= frac{2 times left( mathrm{Recall} times mathrm{Precision} right)}{ mathrm{Recall}+ mathrm{Precision}}$ | . | . ROC curve . Receiver Operative Curve | Plot between True positive rate on y axis and False positive rate on x axis | AUC : Area under the curve, higher the area, better the performance | . A Receiver Operating Characteristic (ROC) curve plots the TP rate vs. the FP rate as a threshold on the confidence of an instance being positive is varied : . Precision Recall Curve . Plot between Precision on y axis and recall (TPR) on x axis. It is used for class imbalanced dataset mostly, It is also used when we can not calculate True Negative. | . A precision/recall curve plots the precision vs. recall (TP rate) as a threshold on the confidence of an instance being positive is varied. . A nice way to define precision recall etc. . Consider $Y$ as true label and $ hat{Y}$ as predicted label. . precision $=P left(Y=1| ; hat{Y} =1 right)$ | Recall (TPR) $=P left( hat{Y} =1| ;Y=1 right)$ | False positive Rate (FPR) $=P left( hat{Y} =1| ;Y=0 right)$ | True Negative Rate (TNR)= $=P left( hat{Y} =0| ;Y=0 right)$ . Note: Notice that TPR and FPR which makes ROC curve is conditioned on the true value, and precision which is used in PR curve is conditioned on predicted label. This is the reason, PR curve is used for class imbalanced dataset, or where positive class is more interesting then negative class. If question is: &quot;How meaningful is a positive result from my classifier given the baseline probabilities of my problem?&quot;, use a PR curve. If question is, &quot;How well can this classifier be expected to perform in general, at a variety of different baseline probabilities?&quot;, go with a ROC curve. | . ROC curve vs PR curve . Consider a dataset having 100 positive cases and 10,000 negative cases. Now consider 2 classifiers $A$ and $B$. $A$ predicts 9 as true positive, 40 as false positive whereas $B$ predicts 9 as true positive 1000 as false positive. We can observe that as both the classifier predicts 9 out of 10 as true positive so both has same recall value, also FPR is small(as it can be seen in below picture), so the ROC curve which is drawn between recall (TPR) and FPR, will not differentiate among the two classifier. But PR curve which is drawn between precision and recall, will look totally different here as both has different precision. . import numpy as np from sklearn.metrics import confusion_matrix import seaborn as sns import numpy as np import matplotlib.pyplot as plt import seaborn as sns def get_conf_matrix_labels(cf_matrix): group_names = [&#39;True Neg&#39;,&#39;False Pos&#39;,&#39;False Neg&#39;,&#39;True Pos&#39;] group_counts = [&#39;{0:0.0f}&#39;.format(value) for value in cf_matrix.flatten()] group_percentages = [&#39;{0:.2%}&#39;.format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)] labels = [f&#39;{v1} n{v2} n{v3}&#39; for v1, v2, v3 in zip(group_names,group_counts,group_percentages)] labels = np.asarray(labels).reshape(cf_matrix.shape[0],cf_matrix.shape[1]) accuracy = np.trace(cf_matrix) / float(np.sum(cf_matrix)) precision = cf_matrix[1,1] / sum(cf_matrix[:,1]) recall = cf_matrix[1,1] / sum(cf_matrix[1,:]) fpr = cf_matrix[0,1] / sum(cf_matrix[0,:]) stats_text = &quot; n nAccuracy={:0.3f} nPrecision={:0.3f} nRecall={:0.3f} nFPR={:0.4f}&quot;.format( accuracy,precision,recall,fpr) return labels,stats_text YTrue = np.hstack([np.ones(10),np.zeros(100000)]) # data with 100 positive and 10,000 negative cases yPredA = np.hstack([np.ones(9),np.zeros(1),np.ones(40),np.zeros(99960)]) # A predicts 9 True positive, 40 False Positive, yPredB = np.hstack([np.ones(9),np.zeros(1),np.ones(1000),np.zeros(99000)]) # B predicts 9 True positive, 1000 False positive cf_matrixA = confusion_matrix(YTrue, yPredA) cf_matrixB = confusion_matrix(YTrue, yPredB) fig, ax = plt.subplots(1, 2, figsize=(10, 5)) axis_labels = [&#39;Negative Class&#39;,&#39;Positive Class&#39;] labels,stats_text=get_conf_matrix_labels(cf_matrixA) sns.heatmap(cf_matrixA, annot=labels, fmt=&#39;&#39;, cmap=&#39;Blues&#39;,ax=ax[0],cbar=False,xticklabels=axis_labels, yticklabels=axis_labels) ax[0].set_title(&#39;Classifier A&#39;) ax[0].set_xlabel(&#39;Predicted label&#39;+stats_text) ax[0].set_ylabel(&#39;True label&#39;) labels,stats_text=get_conf_matrix_labels(cf_matrixB) sns.heatmap(cf_matrixB, annot=labels, fmt=&#39;&#39;, cmap=&#39;Blues&#39;,ax=ax[1],cbar=False,xticklabels=axis_labels, yticklabels=axis_labels) ax[1].set_title(&#39;Classifier B&#39;) ax[1].set_xlabel(&#39;Predicted label&#39;+stats_text) ax[1].set_ylabel(&#39;True label&#39;); . . Accuracy also is not a good measure in above case. As both the classifier has similar accuracy. . Other performance measures . Kullback-Leibler Diverfence : $D_{ mathrm{KL}} left(P |Q right)= sum_i P left(i right) log frac{P left(i right)}{Q left(i right)}$ | Gini Statistic : $2 times mathrm{AUC}-1$ | F1 score: $ frac{2 times left( mathrm{Recall} times mathrm{Precision} right)}{ mathrm{Recall}+ mathrm{Precision}}$ | Akaike Information Criterion (AIC): $2k-2 ln left(L right)$, here $k$ is number of model parameters, L is max value of the Likelihood function for the model | . Important points . Randomization of data is essential so that held-aside test data can be really representative of new data. | Test set should never be used in any way for normalization, hyper parameter tuning etc. | Any preprocessing done over entire data set ( feature selection, parameter tuning, threshold selection ) must not use labels from test set. | . K-Nearest Neighbors . basic idea . If it walks like a duck, quacks like a duck , then it&#39;s probably a duck. | If data points are represented well then KNN works well. | choosing K is important, if K is too small then it becomes sensitive to noise point. If k is too large, neighborhood may incudes points from other class. | Euclidean distance between two instance $d left(X_i ,X_j right)= sqrt{ sum_{r=1}^n { left(a_r left(X_i right)-a_r{ left(X_j right)} right)}^2 }$ here $a_i left(X right) ;$ denotes features. | In case of continuous valued target function, Mean value of K nearest training examples is taken | . How to determinke K . experiment with different value of K starting form 1 on test set to validate the error, in case of binary classification use odd number for k to avoid ties. | . KNN is a transductive method, there is no training involved , it is refereed as Lazy learning, Learning is just storing all the training instances Similar Keywords: KNN, Memory Based Reasoning, Example Based Reasoning, Instance Based Learning, Case Based Reasoning, Lazy Learning Voronoi Diagram: Decision surface formed by the training Examples for 1 nearest neighbors classifier . Improvements . Distance weighted Nearest Neighbors | Scaling (normalization) attributes for fair computation fo distances | Measure &quot;closeness&quot; differently | Finding &quot;close&quot; example in large training set quickly , eg Efficient memory indexing using kd-tree | . Pros . Highly effective transductive inference method for noisy training data and complex target functions. | Target function for a whole space may be described as a combinations of less complex local approximations | Trains very fast (Lazy Learner) | . Cons . Curse of dimensionality | Storage: all training example are saved in memory | slow at query time, can be overcome by pre sorting and indexing training samples. | . Convergence of 1-NN . $P left( mathrm{KNNError} right)=2 left( mathrm{Bayes} ; mathrm{Optimal} ; mathrm{Error} ; mathrm{Rate} right)$ Probability of K NN error is at most twice the bayes optimal error, bayes optimal error is best(least) error we can get using machine learning It is Possible to show that: as the size of training data set approaches infinity, the one nearest neighbor classifier guarantees an error rate of no worse than twice the bayes error rate ( the minimum achievable error rate given the distribution of the data). . Density Estimation using KNN . Non parametric Density Estimation using KNN. nstead of fixing bin width h and counting the mumber of instances, fix the instances(neighbors) k and check bin width $ hat{p} left(X right)= frac{k}{2{ mathrm{Nd}}_k left(X right)}$ here $d_k left(X right)$ is the $k_{ mathrm{th}}$ closest distance to to $X$ , This is also known as Parzen density estimation. . KNN example using sklearn . Import libraries . import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap from sklearn import neighbors, datasets iris = datasets.load_iris() . Print shapes and class information . print(&#39;features: &#39;,iris.feature_names) print(&#39;target: &#39;,iris.target) print(&#39;classess: &#39;,iris.target_names) X = iris.data y = iris.target print(&#39;input data shape:&#39;,X.shape) print(&#39;target shape: &#39;,y.shape) . features: [&#39;sepal length (cm)&#39;, &#39;sepal width (cm)&#39;, &#39;petal length (cm)&#39;, &#39;petal width (cm)&#39;] target: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] classess: [&#39;setosa&#39; &#39;versicolor&#39; &#39;virginica&#39;] input data shape: (150, 4) target shape: (150,) . Plot the data . plt.scatter(iris.data[:,1],iris.data[:,2],c=iris.target, cmap=plt.cm.Paired) plt.xlabel(iris.feature_names[1]) plt.ylabel(iris.feature_names[2]) plt.show() plt.scatter(iris.data[:,0],iris.data[:,3],c=iris.target, cmap=plt.cm.Paired) plt.xlabel(iris.feature_names[0]) plt.ylabel(iris.feature_names[3]) plt.show() . split the dataset into test set and train set . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0) . Fit the classifier with different values of k . from sklearn.neighbors import KNeighborsClassifier from sklearn import metrics k_range = range(1,26) scores = {} scores_list = [] for k in k_range: knn = KNeighborsClassifier(n_neighbors=k) knn.fit(X_train,y_train) y_pred=knn.predict(X_test) scores[k] = metrics.accuracy_score(y_test,y_pred) scores_list.append(metrics.accuracy_score(y_test,y_pred)) . Plot the scores: . plt.plot(k_range,scores_list) plt.xlabel(&#39;Value of K for KNN&#39;) plt.ylabel(&#39;Testing Accuracy&#39;); . chose the best value of K for final model. . Tip: In KNN, finding the value of k is not easy. A small value of k means that noise will have a higher influence on the result and a large value make it computationally expensive. Data scientists usually choose as an odd number if the number of classes is 2 and another simple approach to select k is set k=sqrt(n). There is one more widely used method called Elbow Method which is also used to find the value of K, here we plot error rate vs k value and chose k value at elbow point. . knn = KNeighborsClassifier(n_neighbors=5) knn.fit(X_train,y_train); . from sklearn.metrics import confusion_matrix print(&#39;confusion matrix on test data: &#39;) print(confusion_matrix(y_test, y_pred)) . confusion matrix on test data: [[11 0 0] [ 0 13 0] [ 0 0 6]] . from sklearn.metrics import classification_report print(&#39;classification report on test data:&#39;) print(classification_report(y_test, y_pred, target_names=iris.target_names)) . classification report on test data: precision recall f1-score support setosa 1.00 1.00 1.00 11 versicolor 1.00 1.00 1.00 13 virginica 1.00 1.00 1.00 6 accuracy 1.00 30 macro avg 1.00 1.00 1.00 30 weighted avg 1.00 1.00 1.00 30 .",
            "url": "https://abhiyantaabhishek.github.io/MDS-IIT/foundations%20of%20machine%20learning/2022/08/20/CS5590-week2.html",
            "relUrl": "/foundations%20of%20machine%20learning/2022/08/20/CS5590-week2.html",
            "date": " • Aug 20, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "FoML CS5590 Assignment 0",
            "content": "This homework is intended to review basic pre-requisites in the following topics: . Linear Algebra, Probability, Python programming | . Practice Questions . The questions below are only for your practice - no submission required. (The exam could include questions from this list though!) . 1. Numpy . The following questions should be done using numpy operations. Each exercise can be solved with a maximum of 4-5 lines of Python code. Please avoid the use of iterative constructs (such as for loops) to the extent possible, and use matrix/vector operations to achieve the objectives. Please use https://docs.python.org/3/tutorial/ for any reference required. . (a) Import the numpy package under the name np. Print the numpy version and the configuration. . import numpy as np np.set_printoptions(precision=3) print(&#39;Version: n&#39;,np.__version__) # print(&#39; nconfiguration: n&#39;,np.show_config()) . Version: 1.21.5 . (b) Create a null vector of size 10, and output the vector to the terminal. . null_vector = np.zeros(10) print(null_vector) . [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] . (c) Create a null vector of size 10 but the fifth value which is 1. Output the vector to the terminal. . null_vector = (np.arange(10) == 4).astype(int) print(null_vector) . [0 0 0 0 1 0 0 0 0 0] . (d) Reverse a vector (first element becomes last). . vector=np.array([1,2,3,4,5]) print(vector[::-1]) . [5 4 3 2 1] . (e) Create an $n × n$ array with checkerboard pattern of zeros and ones. . n=5 checkerboard = np.ones(shape=(5,5)) checkerboard[::2,::2]=0 checkerboard[1::2,1::2]=0 print(checkerboard) . [[0. 1. 0. 1. 0.] [1. 0. 1. 0. 1.] [0. 1. 0. 1. 0.] [1. 0. 1. 0. 1.] [0. 1. 0. 1. 0.]] . (f) Given an $n × n$ array, sort the rows of array according to $m^{th}$ column of array. . n=5 m=2 # sort by column random_array = np.random.randint(1,100,size=(n,n)) print(&#39;Original array: n&#39;,random_array) print(&#39;sorted array: n&#39;,random_array[random_array[:,m].argsort()]) . Original array: [[88 22 64 73 98] [ 7 63 33 96 69] [22 22 32 79 55] [33 18 76 85 44] [17 93 71 87 88]] sorted array: [[22 22 32 79 55] [ 7 63 33 96 69] [88 22 64 73 98] [17 93 71 87 88] [33 18 76 85 44]] . (g) Create an $n × n$ array with $(i + j)^{th}$-entry equal to $i + j$. . n=5 print(np.add.outer(range(n),range(n))) . [[0 1 2 3 4] [1 2 3 4 5] [2 3 4 5 6] [3 4 5 6 7] [4 5 6 7 8]] . (h) Consider a $ left(6,7,8 right)$ shape array, what is the index $ left(x,y,z right)$ of the $100^{th}$ element (of the entire structure)? . array = np.arange(0,6*7*8).reshape((6,7,8)) n=100 x = int(n/(7*8)) n=n%(7*8) y = int(n/(8)) z=n%(8) print(&#39;Answer: (x,y,z) = ({},{},{})&#39;.format(x,y,z)) assert array.flatten()[x*7*8+y*8+z] == array[x,y,z] # OR print (&#39;Answer: (x,y,z) = ({},{},{})&#39;.format(*np.unravel_index(100, (6,7,8)))) . Answer: (x,y,z) = (1,5,4) Answer: (x,y,z) = (1,5,4) . (i) Multiply a $5 × 3$ matrix by a $3 × 2$ matrix (real matrix product). . matA = np.random.randint(9,size=(5,3)) matB = np.random.randint(9,size=(3,2)) prod = np.matmul(matA,matB) print(&#39;matA: n&#39;,matA) print(&#39;matB: n&#39;,matB) print(&#39;product: n&#39;,prod) . matA: [[3 2 6] [6 1 6] [8 1 8] [4 8 4] [5 0 2]] matB: [[3 7] [5 4] [5 1]] product: [[49 35] [53 52] [69 68] [72 64] [25 37]] . (j) Create random vector of size $10$ and replace the maximum value by $0$. . mat = np.random.randint(9,size=10) print(&#39;mat: n&#39;,mat) mat[mat.argmax()]=0 print(&#39;New mat: n&#39;,mat) . mat: [7 8 4 3 8 2 0 3 7 0] New mat: [7 0 4 3 8 2 0 3 7 0] . (k) How to find the closest value (to a given scalar) in an array? . mat = np.random.randint(9,size=10) scalar = 5 print(&#39;mat: n&#39;,mat) closest = mat[np.absolute(mat-scalar).argmin()] print(&#39;closest value to scalar={} is {}&#39;.format(scalar,closest)) . mat: [4 5 3 7 8 6 3 4 0 4] closest value to scalar=5 is 5 . (l) Subtract the mean of each row from each corresponding row of a matrix. . mat = np.random.randint(9,size=(4,3)) print(&#39;mat: n&#39;,mat) mean = mat.mean(axis=1) new_mat = mat-mean.T[...,None] print(&#39;After subtracting mean from each row: n&#39;,new_mat) . mat: [[8 6 4] [5 2 7] [5 1 4] [8 0 1]] After subtracting mean from each row: [[ 2. 0. -2. ] [ 0.333 -2.667 2.333] [ 1.667 -2.333 0.667] [ 5. -3. -2. ]] . (m) Consider a given vector, how to add 1 to each element indexed by a second vector (be careful with repeated indices - you should consider it only once)? . mat = np.random.randint(9,size=4) indexes = np.array([0,0,3]) print(&#39;mat: n&#39;,mat) mat[np.unique(indexes)]+=1 print(&#39;mat: n&#39;,mat) . mat: [4 5 6 7] mat: [5 5 6 8] . (n) How to find the most frequent value in an array? . mat = np.random.randint(3,size=5) print(&#39;mat: n&#39;,mat) value,count = np.unique(mat,return_counts=True) print(&#39;most frequent value = {}&#39;.format(value[count.argmax()])) . mat: [0 2 1 0 0] most frequent value = 0 . (o) Extract all the contiguous $3 × 3$ blocks from a random $10 × 10$ matrix. . mat = np.random.randint(0,9,(10,10)) n=3 i=1+(mat.shape[0]-n) j=1+(mat.shape[1]-n) res = np.lib.stride_tricks.as_strided(mat,shape=(i,j,n,n),strides=mat.strides+mat.strides) . (p) Compute the rank, trace and determinant of a matrix. . mat = np.random.randint(0,9,(4,4)) print(&#39;Matrix : n{}&#39;.format(mat)) print(&#39;Matrix Rank :{}&#39;.format(np.linalg.matrix_rank(mat))) print(&#39;Matrix Determinant :{:.2f}&#39;.format(np.linalg.det(mat))) . Matrix : [[2 2 2 0] [5 2 5 0] [7 3 5 4] [8 5 2 1]] Matrix Rank :4 Matrix Determinant :-132.00 . 2. Linear Algebra . (a) Prove or disprove: Empty set is a vector space. . Answer:Axioms of vector spaces: A real vector space is a set $X$ with a special element $0$, and three operations: Addition: Given two elements $x$, $y$ in $X$, one can form the sum $x+y$, which is also an element of $X$. | Inverse: Given an element $x$ in $X$, one can form the inverse $-x$, which is also an element of $X$. | Scalar multiplication: Given an element $x$ in $X$ and a real number $c$, one can form the product $cx$, which is also an element of $X$. | . | As Empty set doesn&#39;t contain element $0$ Hence we can say that Empty set is NOT a vector space | . | . (b) Show that the inverse of $M=I+ left({ mathbf{uv}}^T right)$ is of the type $I+ alpha left({ mathbf{uv}}^T right)$, where $ mathit{ mathbf{u}}, mathit{ mathbf{v}} in { mathbb{R}}^n ,{ mathit{ mathbf{v}}}^T mathit{ mathbf{u}} not= 0$ . Continuing from previous question, find $ alpha$. | For what $u$ and $v$ is $M$ singular? | Find the null space of $M$, if it is singular. | Answer: | . (c) . Consider the 2 × 2 matrix: $A= left lbrack begin{array}{cc} -2 &amp; 2 -6 &amp; 5 end{array} right rbrack$ There exists vectors such that when the matrix $A$ acts on those vectors, the subspace of the vectors does not change. Mathematically, $Ax = lambda I$. The vectors $x$ are called eigenvectors and the values $ lambda$ are the corresponding eigenvalues. Find the eigenvalues and corresponding eigenvectors for the matrix $A$. | Consider a diagonal matrix $ Lambda$ which has the eigenvalues of $A$ as its diagonal entries. Find the matrix $U$ such that the equation $AU = U Lambda$ holds. | Note that we can write the matrix $A$ as $A$ = $U Lambda U^{−1}$. Find the inverse of the matrix $U$ computed in the previous question and verify. | Answer: | . $A= left lbrack begin{array}{cc} -2 &amp; 2 -6 &amp; 5 end{array} right rbrack$ $ mathrm{Ax}= lambda x$ $ Rightarrow mathrm{Ax}- lambda ;x=0 Rightarrow left(A- lambda I right)x=0$ $ left lbrack begin{array}{cc} -2 &amp; 2 -6 &amp; 5 end{array} right rbrack - lambda left lbrack begin{array}{cc} 1 &amp; 0 0 &amp; 1 end{array} right rbrack =0$ $ left lbrack begin{array}{cc} -2 &amp; 2 -6 &amp; 5 end{array} right rbrack - left lbrack begin{array}{cc} lambda &amp; 0 0 &amp; lambda end{array} right rbrack =0$ $ left lbrack begin{array}{cc} -2- lambda &amp; 2 -6 &amp; 5- lambda end{array} right rbrack =0$ $ left(-2- lambda right) left(5- lambda right)-2 left(-6 right)=0$ $-10+2 lambda -5 lambda_{ ;} + lambda^2 +12=0$ $ lambda^2 -3 lambda +2=0$ $ left( lambda -2 right) left( lambda -1 right)=0$ $ lambda =2, lambda =1$ for $ lambda =2$ put $ lambda$ value in $ left(A- lambda I right)x=0$ $ left lbrack begin{array}{cc} -2- lambda &amp; 2 -6 &amp; 5- lambda end{array} right rbrack left lbrack begin{array}{c} x y end{array} right rbrack = left lbrack begin{array}{c} 0 0 end{array} right rbrack$ $ left lbrack begin{array}{cc} -2-2 &amp; 2 -6 &amp; 5-2 end{array} right rbrack left lbrack begin{array}{c} x y end{array} right rbrack = left lbrack begin{array}{c} 0 0 end{array} right rbrack$ $ left lbrack begin{array}{cc} -4 &amp; 2 -6 &amp; 3 end{array} right rbrack left lbrack begin{array}{c} x y end{array} right rbrack = left lbrack begin{array}{c} 0 0 end{array} right rbrack$ We get below 2 equations: $-4x+2y=0$ $-6x+3y=0$ After solving we get : $ left lbrack begin{array}{c} x y end{array} right rbrack = left lbrack begin{array}{c} 1 2 end{array} right rbrack$ Similarly for $ lambda$=1, we get: $ left lbrack begin{array}{cc} -2- lambda &amp; 2 -6 &amp; 5- lambda end{array} right rbrack left lbrack begin{array}{c} x y end{array} right rbrack = left lbrack begin{array}{c} 0 0 end{array} right rbrack$ $ left lbrack begin{array}{cc} -2-1 &amp; 2 -6 &amp; 5-1 end{array} right rbrack left lbrack begin{array}{c} x y end{array} right rbrack = left lbrack begin{array}{c} 0 0 end{array} right rbrack$ $ left lbrack begin{array}{cc} -3 &amp; 2 -6 &amp; 4 end{array} right rbrack left lbrack begin{array}{c} x y end{array} right rbrack = left lbrack begin{array}{c} 0 0 end{array} right rbrack$ $-3x+2y=0$ $-6x+4y=0$ After solving above 2 equations we get: $ left lbrack begin{array}{c} x y end{array} right rbrack = left lbrack begin{array}{c} 2 3 end{array} right rbrack$ | $ mathrm{AU}=U Lambda$ holds when $U$ is a the matrix with all the eigen vectors. Hence $U= left lbrack begin{array}{cc} 2 &amp; 1 3 &amp; 2 end{array} right rbrack$ | $A$ = $U Lambda U^{−1}$ $U= left lbrack begin{array}{cc} 2 &amp; 1 3 &amp; 2 end{array} right rbrack$ $U^{-1} = frac{1}{|U|} cdot mathrm{Adj} ;U$ $ Rightarrow U^{-1} = frac{1}{4-3} left lbrack begin{array}{cc} 2 &amp; -1 -3 &amp; 2 end{array} right rbrack$ $ Rightarrow U^{-1} = left lbrack begin{array}{cc} 2 &amp; -1 -3 &amp; 2 end{array} right rbrack$ $U^{ ;} Lambda U^{-1} = left lbrack begin{array}{cc} 2 &amp; 1 3 &amp; 2 end{array} right rbrack left lbrack begin{array}{cc} 1 &amp; 0 0 &amp; 2 end{array} right rbrack left lbrack begin{array}{cc} 2 &amp; -1 -3 &amp; 2 end{array} right rbrack$ $ Rightarrow U^{ ;} Lambda U^{-1} = left lbrack begin{array}{cc} -2 &amp; 2 -6 &amp; 5 end{array} right rbrack$ Which is ame as $A$, hence proved. | Below is the solution of above question using Numpy, The Eigen Vector seems to be different because it&#39;s normalized one. . import numpy as np m = np.array([[-2, 2], [-6, 5]]) eigValue,eigVector = np.linalg.eig(m) print(&#39;eigen Value: n&#39;,eigValue) print(&#39;eigen Vector: n&#39;,eigVector) . eigen Value: [1. 2.] eigen Vector: [[-0.555 -0.447] [-0.832 -0.894]] . U = np.array([[2, 1], [3, 2]]) Lambda = np.array([[1, 0], [0, 2]]) U_inv = np.linalg.inv(U) print(U.dot(Lambda).dot(U_inv)) . [[-2. 2.] [-6. 5.]] . (c) Show that for any square matrix $A$, the eigenvectors of $A$ are also eigenvectors of $A^2$. What are the eigenvalues for $A^2$? .",
            "url": "https://abhiyantaabhishek.github.io/MDS-IIT/quizzes%20and%20assignments%20-%20foundations%20of%20machine%20learning/2022/08/20/CS5590-Assignment-0.html",
            "relUrl": "/quizzes%20and%20assignments%20-%20foundations%20of%20machine%20learning/2022/08/20/CS5590-Assignment-0.html",
            "date": " • Aug 20, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Discrete Random variable  CS6660 week 2",
            "content": "A random variable is a function from a sample space $ Omega$ to the real numbers $ mathbb{R}$ A random variable $X$ that can take on finitely or countably infinitely many possible values is called discrete. . Indicator random variable . $X= left lbrace begin{array}{ll} 1, &amp; mathrm{if} ; mathrm{event} ;E ; mathrm{occurs} 0, &amp; mathrm{if} ; mathrm{event} ;E^c ; mathrm{occurs} end{array} right.$ $ mathrm{EX}=0 cdot p left(0 right)+1 cdot p left(1 right)=P left lbrace E right rbrace$ $ mathrm{VarX}={ mathrm{EX}}^2 -{ left( mathrm{EX} right)}^2 = left(1^2 cdot P left lbrace E right rbrace +0^2 cdot P left lbrace E^c right rbrace right)-{ left(P left lbrace E right rbrace right)}^2 ; ; ; ; ; ; ; ; ; ;=P left lbrace E right rbrace cdot left(1-P left lbrace E right rbrace right)$ $ mathrm{SD} ;X= sqrt{P left lbrace E right rbrace cdot left(1-P left lbrace E right rbrace right)}$ . Mass function . Let $X$ be a discrete random variable with possible values $X_1 ,X_2 , ldotp ldotp ldotp$ The probability mass function (pmf), or distribution of a random variable tells us the probabilities of these possible values: $p_X left(x_i right)=P left lbrace X=x_i right rbrace$ For any discrete random variable $X$ $p left(X_i right) ge 0,$ and $ sum_i p left(X_i right)=1$ . Expectation . The expection, or mean, or expected value of a discrete random variable $X$ is defined as $EX= sum_i X_i cdot p left(X_i right)$, provided that this sum exists | Expected value is not necessarily a possible value | Expected value can be infinity | Expected value many not exist | $E left( mathrm{aX}+b right)=a cdot mathrm{EX}+b$ proof: $E left( mathrm{aX}+b right)= sum_i left( mathrm{aX}+b right) cdot p left(i right)=a sum_i X cdot p left(i right)+b sum_i p left(i right)=a cdot mathrm{EX}+b$ | . | ${ mathrm{EX}}^n =E left(X^n right) not= { left( mathrm{EX} right)}^n$ | . Variance . The variance and the standard deviation of a random variable are defined as $ mathrm{VarX}:={E left(X- mathrm{EX} right)}^2$ and $ mathrm{SDX}:= sqrt{ ; mathrm{VarX}}$ . Properties of the Variance . $ mathrm{VarX}={ mathrm{EX}}^2 -{ left( mathrm{EX} right)}^2$ proof : $ begin{array}{l} mathrm{VarX}:={E left(X- mathrm{EX} right)}^2 =E left( left(X^2 -2 cdot X cdot mathrm{EX}+{ left( mathrm{EX} right)}^2 right) right) =E left(X^2 right)-E left(2 cdot X cdot mathrm{EX} right)+{E left({ left( mathrm{EX} right)}^2 right)}^{ ;} =E left(X^2 right)-2 cdot mathrm{EX} cdot mathrm{EX}+{ left( mathrm{EX} right)}^2 ={E left({ mathrm{X}}^2 right)}^{ ;} -{ left( mathrm{EX} right)}^2 end{array}$ Here $EX$ is a constant so can come out of $E left(2 cdot X cdot mathrm{EX} right)$ and it becomes $2 cdot mathrm{EX} cdot mathrm{EX}$ also expectation has no effect on ${E left({ left( mathrm{EX} right)}^2 right)}^{ ;}$ for the same reason so it becomes ${ left( mathrm{EX} right)}^2$ | corollary: for any $X,{ mathrm{EX}}^2 ge { left( mathrm{EX} right)}^2$ here equality hold only if $X=$constant a.s. (almost always means probability one) | . | $ mathrm{Var} left( mathrm{aX}+b right)=a^2 cdot mathrm{VarX}$ proof $ begin{array}{l} mathrm{Var} left( mathrm{aX}+b right)={E left( mathrm{aX}+b right)}^2 -{ left(E left( mathrm{aX}+b right) right)}^2 =E left(a^2 X^2 +2 mathrm{abX}+b^2 right)-{ left( mathrm{aEX}+b right)}^2 =a^2 { cdot mathrm{EX}}^2 +2 mathrm{ab} cdot mathrm{EX}+b^2 -a^2 cdot { left( mathrm{EX} right)}^2 -2 mathrm{ab} cdot mathrm{EX}-b^2 =a^2 left({ mathrm{EX}}^2 -{ left( mathrm{EX} right)}^2 right)=a^2 mathrm{VarX} end{array}$ | . | $ mathrm{Var} left(X+b right)= mathrm{VarX}= mathrm{Var} left(-X right)$ | . . Tip: $ mathrm{VarX}={ mathrm{EX}}^2 -{ left( mathrm{EX} right)}^2$$ mathrm{Var} left( mathrm{aX}+b right)=a^2 cdot mathrm{VarX}$$ mathrm{Var} left(X+b right)= mathrm{VarX}= mathrm{Var} left(-X right)$ . Bernoulli, Binomial . Suppose that $n$ independent trails are performed, each succeeding with probability $p$. Let $X$ count the number of success within the $n$ trails. Then $X$ has the Binomial distribution with parameters $n$ and $p$ of, in short $X~ mathrm{Binom} left(n,p right)$ . Binomial Function . $p left(i right)=P left lbrace X=i right rbrace = {n choose i} p^i { left(1-p right)}^{n-i} , ; ; ; ;i=0,1, ldotp ldotp ldotp ldotp ,n$ $ mathrm{EX}= mathrm{np},$ and $ mathrm{VarX}= mathrm{np} left(1-p right)$ .",
            "url": "https://abhiyantaabhishek.github.io/MDS-IIT/mathematical%20foundations%20of%20data%20science/2022/08/13/CS6660-week2.html",
            "relUrl": "/mathematical%20foundations%20of%20data%20science/2022/08/13/CS6660-week2.html",
            "date": " • Aug 13, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "Probability Theory CS6660 week 1",
            "content": "Basics . The Union and intersection . The union $E cup F ;$ of events $E$ and $F$ always means $E$ OR $F$ , The intersection $E cap F$ of events $E$ and $F$ always means $E$ AND $F$ . Tip: The union $ bigcup_i E_{i ;}$ of events $E_{i ; ;}$ always means at least one of the $E_i$&#8217;s, The intersection $ bigcap_i E_i$ of events $E_{i ;}$ always means each of the $E_i$&#8217;s . Complementary Events . The complement of an event is $E^c = bar{E} =E^* := Omega -E$ . Simple Properties of events . commutativity: $ begin{array}{l} E cup F=F cup E E cap F=F cap E end{array}$ | Associativity: $ begin{array}{l} E cup left(F cup G right)= left(E cup F right) cup G=E cup F cup G E cap left(F cap G right)= left(E cap F right) cap G=E cap F cap G end{array}$ | Distributivity: $ begin{array}{l} left(E cup F right) cap G= left(E cap G right) cup left(F cap G right) left(E cap F right) cup G= left(E cup G right) cap left(F cup G right) ; end{array}$ | De Morgan&#39;s Law: $ begin{array}{l} { left(E cup F right)}^c =E^{c ;} cap F^{c ;} { left(E cap F right)}^{c ;} =E^c cup F^c end{array}$ Similarly $ begin{array}{l} { left({ bigcup_{ ; ;} }_i E_{i ;} right)}^c ={ bigcap_{ ;} }_i E_{i ;}^{c ;} { left({ bigcap_{ ;} }_i E_{i ;} right)}^{c ;} ={ bigcup_{ ;} }_i E_i^{c ;} end{array}$ | . Definition (axioms of probability) . The probability $P$ on a sample space $ Omega$ assigns numbers to events $ Omega$ of in such a way that . The probability of any event is non-negative : $P left lbrace E right rbrace ge 0$ | The probability if the sample space is one : $P left lbrace Omega ; right rbrace =1$ | For any finitely or countably infinitely many manually exclusive events $E_{1,} E_2 , ldotp ldotp ldotp$ , $P left lbrace { bigcup_{i ;} E_{i ;} }_{ ;} right rbrace = sum_i P left lbrace E_i right rbrace ;$ | A few simple facts . Inclusion-exclusion principle: . For any events $E$ and $F$, $P left lbrace E cup F right rbrace =P left lbrace E right rbrace +P left lbrace F right rbrace -P left lbrace E cap F right rbrace$ | For any events $E$, $F$ and $G$: $P left lbrace E cup F cup G right rbrace =P left lbrace E right rbrace +P left lbrace F right rbrace +P left lbrace G right rbrace -P left lbrace E cap F right rbrace -P left lbrace E cap G right rbrace -P left lbrace F cap G right rbrace +P left lbrace E cap F cap G right rbrace$ | Generally: $p left lbrace E_1 cup E_2 cup E_3 cup ldotp ldotp ldotp cup E_n right rbrace = sum_{1 le i le n} P left lbrace E_i right rbrace - sum_{1 le i_1 le i_2 le n} P left lbrace E_{i_1 } cap E_{i_2 } right rbrace + sum_{1 le i_1 le i_2 le i_{3 ;} le n} left lbrace P left lbrace E_{i_1 } cap E_{i_2 } cap E_{i_3 } right rbrace right rbrace - ldotp ldotp ldotp ;+{ left(-1 right)}^{n+1} P left lbrace E_1 cap E_2 cap E_3 right rbrace$ | . Boole&#39;s inequality . For any events $E_1 ,E_2 , ldotp ldotp ldotp E_n$ $P left lbrace bigcup_{i=1}^n E_i right rbrace le sum_{i=1}^n P left lbrace E_i right rbrace$ | . Example . Out of $n$ people, what is the probability that there are no coinciding birthdays? $| Omega |={365}^n$ $|E|=365 ldotp 364 ldotp ldotp ldotp left(365-n+1 right)= frac{365!}{ left(365-n right)!}$ $P left lbrace E right rbrace = frac{|E|}{| Omega |}= frac{365!}{ left(365-n right)!{365}^n }$ . Conditional Probability . Let $F$ be an Event with $P left lbrace F right rbrace &gt;0$ . then the conditional probability $E$ of given $F$ is defined as: $P left lbrace E|F right rbrace := frac{P left lbrace E cap F right rbrace }{P left lbrace F right rbrace }$ . Note: Conditional Probability can be interpreted as:&quot;In what proportion of case in $F$ will also $E$ occur?&quot; or &quot;How does the probability of both $E$ and $F$ compare to the probability of $F$ only?&quot; conditional probability is a proper probability and it satisfies the axioms: . The conditional probability of any event is non-negative :$P left lbrace E|F right rbrace ge 0$ | The conditional probability if the sample space is one :$P left lbrace Omega |F ; right rbrace =1$ | For any finitely or countably infinitely many manually exclusive events $E_{1,} E_2 , ldotp ldotp ldotp$ , $P left lbrace { bigcup_{i ;} E_{i ;} |F}_{ ;} right rbrace = sum_i P left lbrace E_i |F right rbrace ;$ | Corollary . $P left lbrace E^c |F right rbrace =1-P left lbrace E|F right rbrace$ | $P left lbrace phi |F right rbrace =0$ | $P left lbrace E|F right rbrace =1-P left lbrace E^c |F right rbrace le 1$ | $P left lbrace left(E cup G right)|F right rbrace =P left lbrace E|F right rbrace +P left lbrace G|F right rbrace -P left lbrace E cap F|F right rbrace$ proof: $ begin{array}{l} P left lbrace left(E cup G right)|F right rbrace = frac{P left lbrace left(E cup G right) cap F right rbrace }{P left lbrace F right rbrace }= frac{P left lbrace left(E cap F right) cup left(G cap F right) right rbrace }{P left lbrace F right rbrace } = frac{P left(E cap F right)+P left(G cap F right)-P left lbrace left(E cap F right) cap ; left(G cap F right) right rbrace }{P left lbrace F right rbrace }= frac{P left(E cap F right)+P left(G cap F right)-P left lbrace E cap G cap F right rbrace }{P left lbrace F right rbrace } =P left lbrace E|F right rbrace +P left lbrace G|F right rbrace -P left lbrace E cap F|F right rbrace end{array}$ | if $E subseteq G$ then $P left lbrace left(G-E right)|F right rbrace =P left lbrace G|F right rbrace -P left lbrace E|F right rbrace$ proof: $ begin{array}{l} P left lbrace G|F right rbrace -P left lbrace E|F right rbrace = frac{ ;P left lbrace G cap F right rbrace }{P left lbrace mathrm{F} right rbrace }- frac{ ;P left lbrace E cap F right rbrace }{P left lbrace mathrm{F} right rbrace } = frac{P left lbrace left(G cap F right)- left(E cap F right) right rbrace ;}{P left lbrace mathrm{F} right rbrace }= frac{P left lbrace left(G cap F right) cap { left(E cap F right)}^c right rbrace ;}{P left lbrace mathrm{F} right rbrace } = frac{P left lbrace left(G cap F right) cap { left(E^{c ;} cup F^{c ;} right)}^{ ;} right rbrace ;}{P left lbrace mathrm{F} right rbrace }= frac{P left lbrace G cap left(F cap { left(E^{c ;} cup F^{c ;} right)}^{ ;} right) right rbrace ;}{P left lbrace mathrm{F} right rbrace } = frac{P left lbrace G cap left({ left({F cap ;E}^{c ;} right) cup left({F cap ;F}^{c ;} right)}^{ ;} right) right rbrace ;}{P left lbrace mathrm{F} right rbrace }= frac{P left lbrace G cap {F cap ;E}^{c ;} right rbrace ;}{P left lbrace mathrm{F} right rbrace } = frac{P left lbrace G cap { ;E}^{c ;} cap F right rbrace ;}{P left lbrace mathrm{F} right rbrace }= frac{P left lbrace left(G-{ ;E}^{ ;} right) cap F right rbrace ;}{P left lbrace mathrm{F} right rbrace }=P left lbrace left(G-E right)|F right rbrace end{array}$ if $A subseteq B$ then $P left(B right)-P left(A right)=P left(B_{ ;} -A right)$ Here as $E subseteq G$ so $ left(E cap F right) subseteq left(G cap F right)$ so we can write $P left lbrace G cap F right rbrace - ;P left lbrace E cap F right rbrace=P left lbrace left(G cap F right)- left(E cap F right) right rbrace$ | . . if $E subseteq G$ then $P left lbrace E|F right rbrace le P left lbrace G|F right rbrace$ | . Multiplication Rule . For $E_1 ,E_2 , ldotp ldotp ldotp E_n$ events: $P left lbrace E_1 cap E_2 cap ldotp ldotp ldotp cap E_n right rbrace =P left lbrace E_1 right rbrace ldotp P left lbrace E_2 |E_1 right rbrace ldotp P left lbrace E_3 |E_1 cap E_2 right rbrace ldotp ldotp ldotp ldotp P left lbrace E_n |E_1 cap E_2 cap ldotp ldotp ldotp cap E_{n-1} right rbrace$ . The law of total probability . This is also known as partition theorem For any events $E$ and $F$ $P left lbrace E right rbrace =P left lbrace E|F right rbrace ldotp P left lbrace E right rbrace +P left lbrace E|F^c right rbrace ldotp P left lbrace F^c right rbrace$ $P left lbrace E right rbrace = sum_i P left lbrace E|F_i right rbrace ldotp P left lbrace F_i right rbrace$ . Bayes&#39; Theorem . For any events $E$ and $F$ $P left lbrace F|E right rbrace = frac{P left lbrace E|F right rbrace ldotp P left lbrace F right rbrace }{P left lbrace E|F right rbrace ldotp P left lbrace F right rbrace +P left lbrace E|F^c right rbrace ldotp P left lbrace F^c right rbrace }$ . Important: if ${ left lbrace F_i right rbrace }_i$ is a complete system of events, then $$P left lbrace F_i |E right rbrace = frac{ ;P left lbrace E|F_i right rbrace ldotp P left lbrace F_i right rbrace }{ sum_j ;P left lbrace E|F_j right rbrace ldotp P left lbrace F_j right rbrace }$$ . Independence . Event $E$ and $F$ are independent if $P left lbrace E|F right rbrace =P left lbrace E right rbrace$ or $P left lbrace E cap F right rbrace =P left lbrace E right rbrace cdot P left lbrace F right rbrace$ . Important: Mutually exclusive events are necessarily also dependent events because one&#8217;s existence depends on the other&#8217;s non-existence.Dependent events are not necessarily mutually exclusive . If $A$ and $B$ are independent then $A^c$ and $B$ are also also independent Proof: $P left(A^c |B right)= frac{P left(A^c cap B right)}{P left(B right)}= frac{P left(B right)-P left(A^{ ;} cap ; ;B right)}{P left(B right)}=1-P left(A|B right)=1-P left(A right)=P left(A^c right)$ | . Three events $E$, $F$ and $G$ are (mutually) independent if . $P left lbrace E cap F right rbrace =P left lbrace E right rbrace ldotp P left lbrace F right rbrace$ | $P left lbrace E cap G right rbrace =P left lbrace E right rbrace ldotp P left lbrace G right rbrace$ | $P left lbrace F cap G right rbrace =P left lbrace F right rbrace ldotp P left lbrace G right rbrace$ | $P left lbrace E cap F cap G right rbrace =P left lbrace E right rbrace ldotp P left lbrace F right rbrace ldotp P left lbrace G right rbrace$ | .",
            "url": "https://abhiyantaabhishek.github.io/MDS-IIT/mathematical%20foundations%20of%20data%20science/2022/08/06/CS6660-week1.html",
            "relUrl": "/mathematical%20foundations%20of%20data%20science/2022/08/06/CS6660-week1.html",
            "date": " • Aug 6, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "Machine Learning Introduction CS5590  week 1",
            "content": "Types of ML . Supervised Learning classification, regression | . | Unsupervised learning | Other settings of ML Reinforcement learning | Semi-supervised learning | Active learning,Transfer learning,Structured learning | . | Dimensionality Reduction (unsupervised Learning) Large sample size is required for high dimensional data | Query accuracy and efficiency degrade rapidly as the dimension increases | strategies: Feature reduction, Feature selection, Manifold learning, Kernel learning | . | . | . . IID Assumption . Identically independently distributed : This is the assumption that the training data and testing data comes from the same distribution | . Types of Models . Induction : Model Learns by Induction, ( creating it&#39;s own rules for example, if we do extensive research while buying mobile, we create set rules, it is called induction). | Transductions: Model learns from references ( for example, if we ask our friends about mobile and we buy according to their suggestion, it is called Transduction). | Online : data could be a stream, data keeps coming over time. | Offline: data is already acquired and trained offline. | Generative: Learns the distribution, the model learns joint probability distribution. | Discriminative:Learns to discriminate without learning distribution. | Parametric : The model have parameters like $ mu$ and $ sigma$ | Non parametric: The model doesn&#39;t have parameters, as in K nearest neighbor (KNN) | . Classifier evaluation . . Training Error Not very useful | Relatively easy to obtain low error | $E_{ mathrm{train}} = frac{1}{n} sum_{i=1}^n mathrm{error} left(f_D left(X_i right),y_i right)$ | . | Generalization Error Measure of how well do we do on unseen data | $E_{ mathrm{gen}} = int mathrm{error} left(f_D left(X right),y right)p left(y,X right) mathrm{dX}$ | . | . Stratified sampling . First stratify instances by class, then randomly select instances from each class proportionally | It ensures that the proportion of each class remains same in training and validation set. | . Model Selection . Re-Substitution : not useful as it suggests to re- substitute the train data for validation as well | K - Fold cross-validation : Divide the data in K fold using stratified sampling, and the select some set for training and some for validation in each iteration. | Leave-one-out N-fold cross-validation | . | .",
            "url": "https://abhiyantaabhishek.github.io/MDS-IIT/foundations%20of%20machine%20learning/2022/08/06/CS5590-week1.html",
            "relUrl": "/foundations%20of%20machine%20learning/2022/08/06/CS5590-week1.html",
            "date": " • Aug 6, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am an M.tech Student at IIT Hyderabad, Also working as Senior Data Scientist at Bosch. . Your browser does not support PDFs viewer. Download the PDF . . &lt;/object&gt;",
          "url": "https://abhiyantaabhishek.github.io/MDS-IIT/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://abhiyantaabhishek.github.io/MDS-IIT/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}