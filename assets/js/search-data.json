{
  
    
        "post0": {
            "title": "Foundations of Machine Learning  week",
            "content": "",
            "url": "https://abhiyantaabhishek.github.io/MDS-IIT/foundations%20of%20machine%20learning/2022/08/20/CS5590-week2.html",
            "relUrl": "/foundations%20of%20machine%20learning/2022/08/20/CS5590-week2.html",
            "date": " • Aug 20, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Discrete Random variable",
            "content": "A random variable is a function from a sample space $ Omega$ to the real numbers $ mathbb{R}$ A random variable $X$ that can take on finitely or countably infinitely many possible values is called discrete. . Indicator random variable . $X= left lbrace begin{array}{ll} 1, &amp; mathrm{if} ; mathrm{event} ;E ; mathrm{occurs} 0, &amp; mathrm{if} ; mathrm{event} ;E^c ; mathrm{occurs} end{array} right.$ $ mathrm{EX}=0 cdot p left(0 right)+1 cdot p left(1 right)=P left lbrace E right rbrace$ $ mathrm{VarX}={ mathrm{EX}}^2 -{ left( mathrm{EX} right)}^2 = left(1^2 cdot P left lbrace E right rbrace +0^2 cdot P left lbrace E^c right rbrace right)-{ left(P left lbrace E right rbrace right)}^2 =P left lbrace E right rbrace cdot left(1-P left lbrace E right rbrace right)$ $ mathrm{SD} ;X= sqrt{P left lbrace E right rbrace cdot left(1-P left lbrace E right rbrace right)}$ . Mass function . Let $X$ be a discrete random variable with possible values $X_1 ,X_2 , ldotp ldotp ldotp$ The probability mass function (pmf), or distribution of a random variable tells us the probabilities of these possible values: $p_X left(x_i right)=P left lbrace X=x_i right rbrace$ For any discrete random variable $X$ $p left(X_i right) ge 0,$ and $ sum_i p left(X_i right)=1$ . Expectation . The expection, or mean, or expected value of a discrete random variable $X$ is defined as $EX= sum_i X_i cdot p left(X_i right)$, provided that this sum exists | Expected value is not necessarily a possible value | Expected value can be infinity | Expected value many not exist | $E left( mathrm{aX}+b right)=a cdot mathrm{EX}+b$ proof: $E left( mathrm{aX}+b right)= sum_i left( mathrm{aX}+b right) cdot p left(i right)=a sum_i X cdot p left(i right)+b sum_i p left(i right)=a cdot mathrm{EX}+b$ | . | ${ mathrm{EX}}^n =E left(X^n right) not= { left( mathrm{EX} right)}^n$ | . Variance . The variance and the standard deviation of a random variable are defined as $ mathrm{VarX}:={E left(X- mathrm{EX} right)}^2$ and $ mathrm{SDX}:= sqrt{ ; mathrm{VarX}}$ . Properties of the Variance . $ mathrm{VarX}={ mathrm{EX}}^2 -{ left( mathrm{EX} right)}^2$ proof : $ begin{array}{l} mathrm{VarX}:={E left(X- mathrm{EX} right)}^2 =E left( left(X^2 -2 cdot X cdot mathrm{EX}+{ left( mathrm{EX} right)}^2 right) right) =E left(X^2 right)-E left(2 cdot X cdot mathrm{EX} right)+{E left({ left( mathrm{EX} right)}^2 right)}^{ ;} =E left(X^2 right)-2 cdot mathrm{EX} cdot mathrm{EX}+{ left( mathrm{EX} right)}^2 ={E left({ mathrm{X}}^2 right)}^{ ;} -{ left( mathrm{EX} right)}^2 end{array}$ Here $EX$ is a constant so can come out of $E left(2 cdot X cdot mathrm{EX} right)$ and it becomes $2 cdot mathrm{EX} cdot mathrm{EX}$ also expectation has no effect on ${E left({ left( mathrm{EX} right)}^2 right)}^{ ;}$ for the same reason so it becomes ${ left( mathrm{EX} right)}^2$ | corollary: for any $X,{ mathrm{EX}}^2 ge { left( mathrm{EX} right)}^2$ here equality hold only if $X=$constant a.s. (almost always means probability one) | . | $ mathrm{Var} left( mathrm{aX}+b right)=a^2 cdot mathrm{VarX}$ proof $ begin{array}{l} mathrm{Var} left( mathrm{aX}+b right)={E left( mathrm{aX}+b right)}^2 -{ left(E left( mathrm{aX}+b right) right)}^2 =E left(a^2 X^2 +2 mathrm{abX}+b^2 right)-{ left( mathrm{aEX}+b right)}^2 =a^2 { cdot mathrm{EX}}^2 +2 mathrm{ab} cdot mathrm{EX}+b^2 -a^2 cdot { left( mathrm{EX} right)}^2 -2 mathrm{ab} cdot mathrm{EX}-b^2 =a^2 left({ mathrm{EX}}^2 -{ left( mathrm{EX} right)}^2 right)=a^2 mathrm{VarX} end{array}$ | . | $ mathrm{Var} left(X+b right)= mathrm{VarX}= mathrm{Var} left(-X right)$ | . . Tip: $ mathrm{VarX}={ mathrm{EX}}^2 -{ left( mathrm{EX} right)}^2$$ mathrm{Var} left( mathrm{aX}+b right)=a^2 cdot mathrm{VarX}$$ mathrm{Var} left(X+b right)= mathrm{VarX}= mathrm{Var} left(-X right)$ . Bernoulli, Binomial . Suppose that $n$ independent trails are performed, each succeeding with probability $p$. Let $X$ count the number of success within the $n$ trails. Then $X$ has the Binomial distribution with parameters $n$ and $p$ of, in short $X~ mathrm{Binom} left(n,p right)$ . Mass Function . $p left(i right)=P left lbrace X=i right rbrace = {n choose k} p^i { left(1-p right)}^{n-i} , ; ; ; ;i=0,1, ldotp ldotp ldotp ldotp ,n$ $ mathrm{EX}= mathrm{np},$ and $ mathrm{VarX}= mathrm{np} left(1-p right)$ .",
            "url": "https://abhiyantaabhishek.github.io/MDS-IIT/probability/2022/08/13/CS6660-week2.html",
            "relUrl": "/probability/2022/08/13/CS6660-week2.html",
            "date": " • Aug 13, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Probability",
            "content": "Basics . The Union and intersection . The union $E cup F ;$ of events $E$ and $F$ always means $E$ OR $F$ , The intersection $E cap F$ of events $E$ and $F$ always means $E$ AND $F$ . Tip: The union $ bigcup_i E_{i ;}$ of events $E_{i ; ;}$ always means at least one of the $E_i$&#8217;s, The intersection $ bigcap_i E_i$ of events $E_{i ;}$ always means each of the $E_i$&#8217;s . Complementary Events . The complement of an event is $E^c = bar{E} =E^* := Omega -E$ . Simple Properties of events . commutativity: $ begin{array}{l} E cup F=F cup E E cap F=F cap E end{array}$ | Associativity: $ begin{array}{l} E cup left(F cup G right)= left(E cup F right) cup G=E cup F cup G E cap left(F cap G right)= left(E cap F right) cap G=E cap F cap G end{array}$ | Distributivity: $ begin{array}{l} left(E cup F right) cap G= left(E cap G right) cup left(F cap G right) left(E cap F right) cup G= left(E cup G right) cap left(F cup G right) ; end{array}$ | De Morgan&#39;s Law: $ begin{array}{l} { left(E cup F right)}^c =E^{c ;} cap F^{c ;} { left(E cap F right)}^{c ;} =E^c cup F^c end{array}$ Similarly $ begin{array}{l} { left({ bigcup_{ ; ;} }_i E_{i ;} right)}^c ={ bigcap_{ ;} }_i E_{i ;}^{c ;} { left({ bigcap_{ ;} }_i E_{i ;} right)}^{c ;} ={ bigcup_{ ;} }_i E_i^{c ;} end{array}$ | . Definition (axioms of probability) . The probability $P$ on a sample space $ Omega$ assigns numbers to events $ Omega$ of in such a way that . The probability of any event is non-negative : $P left lbrace E right rbrace ge 0$ | The probability if the sample space is one : $P left lbrace Omega ; right rbrace =1$ | For any finitely or countably infinitely many manually exclusive events $E_{1,} E_2 , ldotp ldotp ldotp$ , $P left lbrace { bigcup_{i ;} E_{i ;} }_{ ;} right rbrace = sum_i P left lbrace E_i right rbrace ;$ | A few simple facts . Inclusion-exclusion principle: . For any events $E$ and $F$, $P left lbrace E cup F right rbrace =P left lbrace E right rbrace +P left lbrace F right rbrace -P left lbrace E cap F right rbrace$ | For any events $E$, $F$ and $G$: $P left lbrace E cup F cup G right rbrace =P left lbrace E right rbrace +P left lbrace F right rbrace +P left lbrace G right rbrace -P left lbrace E cap F right rbrace -P left lbrace E cap G right rbrace -P left lbrace F cap G right rbrace +P left lbrace E cap F cap G right rbrace$ | Generally: $p left lbrace E_1 cup E_2 cup E_3 cup ldotp ldotp ldotp cup E_n right rbrace = sum_{1 le i le n} P left lbrace E_i right rbrace - sum_{1 le i_1 le i_2 le n} P left lbrace E_{i_1 } cap E_{i_2 } right rbrace + sum_{1 le i_1 le i_2 le i_{3 ;} le n} left lbrace P left lbrace E_{i_1 } cap E_{i_2 } cap E_{i_3 } right rbrace right rbrace - ldotp ldotp ldotp ;+{ left(-1 right)}^{n+1} P left lbrace E_1 cap E_2 cap E_3 right rbrace$ | . Boole&#39;s inequality . For any events $E_1 ,E_2 , ldotp ldotp ldotp E_n$ $P left lbrace bigcup_{i=1}^n E_i right rbrace le sum_{i=1}^n P left lbrace E_i right rbrace$ | . Example . Out of $n$ people, what is the probability that there are no coinciding birthdays? $| Omega |={365}^n$ $|E|=365 ldotp 364 ldotp ldotp ldotp left(365-n+1 right)= frac{365!}{ left(365-n right)!}$ $P left lbrace E right rbrace = frac{|E|}{| Omega |}= frac{365!}{ left(365-n right)!{365}^n }$ . Conditional Probability . Let $F$ be an Event with $P left lbrace F right rbrace &gt;0$ . then the conditional probability $E$ of given $F$ is defined as: $P left lbrace E|F right rbrace := frac{P left lbrace E cap F right rbrace }{P left lbrace F right rbrace }$ . Note: Conditional Probability can be interpreted as:&quot;In what proportion of case in $F$ will also $E$ occur?&quot; or &quot;How does the probability of both $E$ and $F$ compare to the probability of $F$ only?&quot; conditional probability is a proper probability and it satisfies the axioms: . The conditional probability of any event is non-negative :$P left lbrace E|F right rbrace ge 0$ | The conditional probability if the sample space is one :$P left lbrace Omega |F ; right rbrace =1$ | For any finitely or countably infinitely many manually exclusive events $E_{1,} E_2 , ldotp ldotp ldotp$ , $P left lbrace { bigcup_{i ;} E_{i ;} |F}_{ ;} right rbrace = sum_i P left lbrace E_i |F right rbrace ;$ | Corollary . $P left lbrace E^c |F right rbrace =1-P left lbrace E|F right rbrace$ | $P left lbrace phi |F right rbrace =0$ | $P left lbrace E|F right rbrace =1-P left lbrace E^c |F right rbrace le 1$ | $P left lbrace left(E cup G right)|F right rbrace =P left lbrace E|F right rbrace +P left lbrace G|F right rbrace -P left lbrace E cap F|F right rbrace$ proof: $ begin{array}{l} P left lbrace left(E cup G right)|F right rbrace = frac{P left lbrace left(E cup G right) cap F right rbrace }{P left lbrace F right rbrace }= frac{P left lbrace left(E cap F right) cup left(G cap F right) right rbrace }{P left lbrace F right rbrace } = frac{P left(E cap F right)+P left(G cap F right)-P left lbrace left(E cap F right) cap ; left(G cap F right) right rbrace }{P left lbrace F right rbrace }= frac{P left(E cap F right)+P left(G cap F right)-P left lbrace E cap G cap F right rbrace }{P left lbrace F right rbrace } =P left lbrace E|F right rbrace +P left lbrace G|F right rbrace -P left lbrace E cap F|F right rbrace end{array}$ | if $E subseteq G$ then $P left lbrace left(G-E right)|F right rbrace =P left lbrace G|F right rbrace -P left lbrace E|F right rbrace$ proof: $ begin{array}{l} P left lbrace G|F right rbrace -P left lbrace E|F right rbrace = frac{ ;P left lbrace G cap F right rbrace }{P left lbrace mathrm{F} right rbrace }- frac{ ;P left lbrace E cap F right rbrace }{P left lbrace mathrm{F} right rbrace } = frac{P left lbrace left(G cap F right)- left(E cap F right) right rbrace ;}{P left lbrace mathrm{F} right rbrace }= frac{P left lbrace left(G cap F right) cap { left(E cap F right)}^c right rbrace ;}{P left lbrace mathrm{F} right rbrace } = frac{P left lbrace left(G cap F right) cap { left(E^{c ;} cup F^{c ;} right)}^{ ;} right rbrace ;}{P left lbrace mathrm{F} right rbrace }= frac{P left lbrace G cap left(F cap { left(E^{c ;} cup F^{c ;} right)}^{ ;} right) right rbrace ;}{P left lbrace mathrm{F} right rbrace } = frac{P left lbrace G cap left({ left({F cap ;E}^{c ;} right) cup left({F cap ;F}^{c ;} right)}^{ ;} right) right rbrace ;}{P left lbrace mathrm{F} right rbrace }= frac{P left lbrace G cap {F cap ;E}^{c ;} right rbrace ;}{P left lbrace mathrm{F} right rbrace } = frac{P left lbrace G cap { ;E}^{c ;} cap F right rbrace ;}{P left lbrace mathrm{F} right rbrace }= frac{P left lbrace left(G-{ ;E}^{ ;} right) cap F right rbrace ;}{P left lbrace mathrm{F} right rbrace }=P left lbrace left(G-E right)|F right rbrace end{array}$ if $A subseteq B$ then $P left(B right)-P left(A right)=P left(B_{ ;} -A right)$ Here as $E subseteq G$ so $ left(E cap F right) subseteq left(G cap F right)$ so we can write $P left lbrace G cap F right rbrace - ;P left lbrace E cap F right rbrace=P left lbrace left(G cap F right)- left(E cap F right) right rbrace$ | . . if $E subseteq G$ then $P left lbrace E|F right rbrace le P left lbrace G|F right rbrace$ | . Multiplication Rule . For $E_1 ,E_2 , ldotp ldotp ldotp E_n$ events: $P left lbrace E_1 cap E_2 cap ldotp ldotp ldotp cap E_n right rbrace =P left lbrace E_1 right rbrace ldotp P left lbrace E_2 |E_1 right rbrace ldotp P left lbrace E_3 |E_1 cap E_2 right rbrace ldotp ldotp ldotp ldotp P left lbrace E_n |E_1 cap E_2 cap ldotp ldotp ldotp cap E_{n-1} right rbrace$ . The law of total probability . This is also known as partition theorem For any events $E$ and $F$ $P left lbrace E right rbrace =P left lbrace E|F right rbrace ldotp P left lbrace E right rbrace +P left lbrace E|F^c right rbrace ldotp P left lbrace F^c right rbrace$ $P left lbrace E right rbrace = sum_i P left lbrace E|F_i right rbrace ldotp P left lbrace F_i right rbrace$ . Bayes&#39; Theorem . For any events $E$ and $F$ $P left lbrace F|E right rbrace = frac{P left lbrace E|F right rbrace ldotp P left lbrace F right rbrace }{P left lbrace E|F right rbrace ldotp P left lbrace F right rbrace +P left lbrace E|F^c right rbrace ldotp P left lbrace F^c right rbrace }$ . Important: if ${ left lbrace F_i right rbrace }_i$ is a complete system of events, then $$P left lbrace F_i |E right rbrace = frac{ ;P left lbrace E|F_i right rbrace ldotp P left lbrace F_i right rbrace }{ sum_j ;P left lbrace E|F_j right rbrace ldotp P left lbrace F_j right rbrace }$$ . Independence . Event $E$ and $F$ are independent if $P left lbrace E|F right rbrace =P left lbrace E right rbrace$ or $P left lbrace E cap F right rbrace =P left lbrace E right rbrace cdot P left lbrace F right rbrace$ . Important: Mutually exclusive events are necessarily also dependent events because one&#8217;s existence depends on the other&#8217;s non-existence.Dependent events are not necessarily mutually exclusive . If $A$ and $B$ are independent then $A^c$ and $B$ are also also independent Proof: $P left(A^c |B right)= frac{P left(A^c cap B right)}{P left(B right)}= frac{P left(B right)-P left(A^{ ;} cap ; ;B right)}{P left(B right)}=1-P left(A|B right)=1-P left(A right)=P left(A^c right)$ | . Three events $E$, $F$ and $G$ are (mutually) independent if . $P left lbrace E cap F right rbrace =P left lbrace E right rbrace ldotp P left lbrace F right rbrace$ | $P left lbrace E cap G right rbrace =P left lbrace E right rbrace ldotp P left lbrace G right rbrace$ | $P left lbrace F cap G right rbrace =P left lbrace F right rbrace ldotp P left lbrace G right rbrace$ | $P left lbrace E cap F cap G right rbrace =P left lbrace E right rbrace ldotp P left lbrace F right rbrace ldotp P left lbrace G right rbrace$ | .",
            "url": "https://abhiyantaabhishek.github.io/MDS-IIT/probability/2022/08/06/CS6660-week1.html",
            "relUrl": "/probability/2022/08/06/CS6660-week1.html",
            "date": " • Aug 6, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Foundations of Machine Learning  week week",
            "content": "",
            "url": "https://abhiyantaabhishek.github.io/MDS-IIT/foundations%20of%20machine%20learning/2022/08/06/CS5590-week1.html",
            "relUrl": "/foundations%20of%20machine%20learning/2022/08/06/CS5590-week1.html",
            "date": " • Aug 6, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am an M.tech Student at IIT Hyderabad, Also working as Senior Data Scientist at Bosch. .",
          "url": "https://abhiyantaabhishek.github.io/MDS-IIT/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://abhiyantaabhishek.github.io/MDS-IIT/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}