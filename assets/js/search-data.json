{
  
    
        "post0": {
            "title": "Machine Learning Model Evaluation Measures",
            "content": "Evaluation Measures . Classification It is a measure of being right/wrong,0-1, eg: hinge loss, cross entropy loss | . | Regression loss It is a measure if how close we are to target, eg: MEA, MES | . | Ranking/search It is a measure of top K search | . | Clustering How well we have described the data ( not straight forward) | . | . Is accuracy adequate . Accuracy may not not be useful in cases where: . There is a large class skew. | There are differential misclassification cost, say getting a positive wrong costs more than getting a negative wrong. | we are most interested in a subset of high confidence predictions. | . Classification Error . . . Tip: True positive rate also called sensitivity, recall, hit rate Specificity = 1 - False Alarm sensitivity = Probability of positive test given a patient has a disease. Specificity = Probability of a negative test given a patient is well. . Utility and cost . Detection Cost: cost = $C_{ mathrm{FP}} times mathrm{FP}+C_{ mathrm{FN}} times mathrm{FN}$ | . | Fmeasure $F1= frac{2 times left( mathrm{Recall} times mathrm{Precision} right)}{ mathrm{Recall}+ mathrm{Precision}}$ | . | . ROC curve . Receiver Operative Curve | Plot between True positive rate on y axis and False positive rate on x axis | AUC : Area under the curve, higher the area, better the performance | . . Precision Recall Curve . Plot between Precision on y axis and recall (TPR) on x axis | . Other performance measures . Kullback-Leibler Diverfence : $D_{ mathrm{KL}} left(P |Q right)= sum_i P left(i right) log frac{P left(i right)}{Q left(i right)}$ | Gini Statistic : $2 times mathrm{AUC}-1$ | F1 score: $ frac{2 times left( mathrm{Recall} times mathrm{Precision} right)}{ mathrm{Recall}+ mathrm{Precision}}$ | Akaike Information Criterion (AIC): $2k-2 ln left(L right)$, here $k$ is number of model parameters, L is max value of the Likelihood function for the model | . Important points . Randomization of data is essential so that held-aside test data can be really representative of new data. | Test set should never be used in any way for normalization, hyper parameter tuning etc. | Any preprocessing done over entire data set ( feature selection, parameter tuning, threshold selection ) must not use labels from test set. | . K-Nearest Neighbors . basic idea . If it walks like a duck, quacks like a duck , then it&#39;s probably a duck. | If data points are represented well then KNN works well. | choosing K is important, if K is too small then it becomes sensitive to noise point. If k is too large, neighborhood may incudes points from other class. | Euclidean distance between two instance $d left(X_i ,X_j right)= sqrt{ sum_{r=1}^n { left(a_r left(X_i right)-a_{r left(X_j right)} right)}^2 }$ here $a_i left(X right) ;$ denotes features. | In case of continuous valued target function, Mean value of K nearest training examples is taken | . How to determinke K . experiment with different value of K starting form 1 on test set to validate the error, in case of binary classification use odd number for k to avoid ties. | . KNN is a transductive method, there is no training involved , it is refereed as Lazy learning, Learning is just storing all the training instances Similar Keywords: KNN, Memory Based Reasoning, Example Based Reasoning, Instance Based Learning, Case Based Reasoning, Lazy Learning Voronoi Diagram: Decision surface formed by the training Examples for 1 nearest neighbors classifier . Improvements . Distance weighted Nearest Neighbors | Scaling (normalization) attributes for fair computation fo distances | Measure &quot;closeness&quot; differently | Finding &quot;close&quot; example in large training set quickly , eg Efficient memory indexing using kd-tree | . Pros . Highly effective transductive inference method for noisy training data and complex target functions. | Target function for a whole space may be described as a combinations of less complex local approximations | Trains very fast (Lazy Learner) | . Cons . Curse of dimensionality | Storage: all training example are saved in memory | slow at query time, can be overcome by pre sorting and indexing training samples. | . Convergence of 1-NN . $P left( mathrm{KNNError} right)=2 left( mathrm{Bayes} ; mathrm{Optimal} ; mathrm{Error} ; mathrm{Rate} right)$ , Probability of K NN error is at most twice the bayes optimal error, bayes optimal error is best(least) error we can get using machine learning It is Possible to show that: as the size of training data set approaches infinity, the one nearest neighbor classifier guarantees an error rate of no worse than twice the bayes error rate ( the minimum achievable error rate given the distribution of the data). . Density Estimation using KNN . Non parametric Density Estimation using KNN. nstead of fixing bin width h and counting the mumber of instances, fix the instances(neighbors) k and check bin width $ hat{p} left(X right)= frac{k}{2{ mathrm{Nd}}_k left(X right)}$ here $d_k left(X right)$ is the $k_{ mathrm{th}}$ closest distance to to $X$ , This is also known as Parzen density estimation. .",
            "url": "https://abhiyantaabhishek.github.io/MDS-IIT/foundations%20of%20machine%20learning/2022/08/20/CS5590-week2.html",
            "relUrl": "/foundations%20of%20machine%20learning/2022/08/20/CS5590-week2.html",
            "date": " • Aug 20, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Discrete Random variable",
            "content": "A random variable is a function from a sample space $ Omega$ to the real numbers $ mathbb{R}$ A random variable $X$ that can take on finitely or countably infinitely many possible values is called discrete. . Indicator random variable . $X= left lbrace begin{array}{ll} 1, &amp; mathrm{if} ; mathrm{event} ;E ; mathrm{occurs} 0, &amp; mathrm{if} ; mathrm{event} ;E^c ; mathrm{occurs} end{array} right.$ $ mathrm{EX}=0 cdot p left(0 right)+1 cdot p left(1 right)=P left lbrace E right rbrace$ $ mathrm{VarX}={ mathrm{EX}}^2 -{ left( mathrm{EX} right)}^2 = left(1^2 cdot P left lbrace E right rbrace +0^2 cdot P left lbrace E^c right rbrace right)-{ left(P left lbrace E right rbrace right)}^2 =P left lbrace E right rbrace cdot left(1-P left lbrace E right rbrace right)$ $ mathrm{SD} ;X= sqrt{P left lbrace E right rbrace cdot left(1-P left lbrace E right rbrace right)}$ . Mass function . Let $X$ be a discrete random variable with possible values $X_1 ,X_2 , ldotp ldotp ldotp$ The probability mass function (pmf), or distribution of a random variable tells us the probabilities of these possible values: $p_X left(x_i right)=P left lbrace X=x_i right rbrace$ For any discrete random variable $X$ $p left(X_i right) ge 0,$ and $ sum_i p left(X_i right)=1$ . Expectation . The expection, or mean, or expected value of a discrete random variable $X$ is defined as $EX= sum_i X_i cdot p left(X_i right)$, provided that this sum exists | Expected value is not necessarily a possible value | Expected value can be infinity | Expected value many not exist | $E left( mathrm{aX}+b right)=a cdot mathrm{EX}+b$ proof: $E left( mathrm{aX}+b right)= sum_i left( mathrm{aX}+b right) cdot p left(i right)=a sum_i X cdot p left(i right)+b sum_i p left(i right)=a cdot mathrm{EX}+b$ | . | ${ mathrm{EX}}^n =E left(X^n right) not= { left( mathrm{EX} right)}^n$ | . Variance . The variance and the standard deviation of a random variable are defined as $ mathrm{VarX}:={E left(X- mathrm{EX} right)}^2$ and $ mathrm{SDX}:= sqrt{ ; mathrm{VarX}}$ . Properties of the Variance . $ mathrm{VarX}={ mathrm{EX}}^2 -{ left( mathrm{EX} right)}^2$ proof : $ begin{array}{l} mathrm{VarX}:={E left(X- mathrm{EX} right)}^2 =E left( left(X^2 -2 cdot X cdot mathrm{EX}+{ left( mathrm{EX} right)}^2 right) right) =E left(X^2 right)-E left(2 cdot X cdot mathrm{EX} right)+{E left({ left( mathrm{EX} right)}^2 right)}^{ ;} =E left(X^2 right)-2 cdot mathrm{EX} cdot mathrm{EX}+{ left( mathrm{EX} right)}^2 ={E left({ mathrm{X}}^2 right)}^{ ;} -{ left( mathrm{EX} right)}^2 end{array}$ Here $EX$ is a constant so can come out of $E left(2 cdot X cdot mathrm{EX} right)$ and it becomes $2 cdot mathrm{EX} cdot mathrm{EX}$ also expectation has no effect on ${E left({ left( mathrm{EX} right)}^2 right)}^{ ;}$ for the same reason so it becomes ${ left( mathrm{EX} right)}^2$ | corollary: for any $X,{ mathrm{EX}}^2 ge { left( mathrm{EX} right)}^2$ here equality hold only if $X=$constant a.s. (almost always means probability one) | . | $ mathrm{Var} left( mathrm{aX}+b right)=a^2 cdot mathrm{VarX}$ proof $ begin{array}{l} mathrm{Var} left( mathrm{aX}+b right)={E left( mathrm{aX}+b right)}^2 -{ left(E left( mathrm{aX}+b right) right)}^2 =E left(a^2 X^2 +2 mathrm{abX}+b^2 right)-{ left( mathrm{aEX}+b right)}^2 =a^2 { cdot mathrm{EX}}^2 +2 mathrm{ab} cdot mathrm{EX}+b^2 -a^2 cdot { left( mathrm{EX} right)}^2 -2 mathrm{ab} cdot mathrm{EX}-b^2 =a^2 left({ mathrm{EX}}^2 -{ left( mathrm{EX} right)}^2 right)=a^2 mathrm{VarX} end{array}$ | . | $ mathrm{Var} left(X+b right)= mathrm{VarX}= mathrm{Var} left(-X right)$ | . . Tip: $ mathrm{VarX}={ mathrm{EX}}^2 -{ left( mathrm{EX} right)}^2$$ mathrm{Var} left( mathrm{aX}+b right)=a^2 cdot mathrm{VarX}$$ mathrm{Var} left(X+b right)= mathrm{VarX}= mathrm{Var} left(-X right)$ . Bernoulli, Binomial . Suppose that $n$ independent trails are performed, each succeeding with probability $p$. Let $X$ count the number of success within the $n$ trails. Then $X$ has the Binomial distribution with parameters $n$ and $p$ of, in short $X~ mathrm{Binom} left(n,p right)$ . Mass Function . $p left(i right)=P left lbrace X=i right rbrace = {n choose k} p^i { left(1-p right)}^{n-i} , ; ; ; ;i=0,1, ldotp ldotp ldotp ldotp ,n$ $ mathrm{EX}= mathrm{np},$ and $ mathrm{VarX}= mathrm{np} left(1-p right)$ .",
            "url": "https://abhiyantaabhishek.github.io/MDS-IIT/mathematical%20foundations%20of%20data%20science/2022/08/13/CS6660-week2.html",
            "relUrl": "/mathematical%20foundations%20of%20data%20science/2022/08/13/CS6660-week2.html",
            "date": " • Aug 13, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Probability Theory",
            "content": "Basics . The Union and intersection . The union $E cup F ;$ of events $E$ and $F$ always means $E$ OR $F$ , The intersection $E cap F$ of events $E$ and $F$ always means $E$ AND $F$ . Tip: The union $ bigcup_i E_{i ;}$ of events $E_{i ; ;}$ always means at least one of the $E_i$&#8217;s, The intersection $ bigcap_i E_i$ of events $E_{i ;}$ always means each of the $E_i$&#8217;s . Complementary Events . The complement of an event is $E^c = bar{E} =E^* := Omega -E$ . Simple Properties of events . commutativity: $ begin{array}{l} E cup F=F cup E E cap F=F cap E end{array}$ | Associativity: $ begin{array}{l} E cup left(F cup G right)= left(E cup F right) cup G=E cup F cup G E cap left(F cap G right)= left(E cap F right) cap G=E cap F cap G end{array}$ | Distributivity: $ begin{array}{l} left(E cup F right) cap G= left(E cap G right) cup left(F cap G right) left(E cap F right) cup G= left(E cup G right) cap left(F cup G right) ; end{array}$ | De Morgan&#39;s Law: $ begin{array}{l} { left(E cup F right)}^c =E^{c ;} cap F^{c ;} { left(E cap F right)}^{c ;} =E^c cup F^c end{array}$ Similarly $ begin{array}{l} { left({ bigcup_{ ; ;} }_i E_{i ;} right)}^c ={ bigcap_{ ;} }_i E_{i ;}^{c ;} { left({ bigcap_{ ;} }_i E_{i ;} right)}^{c ;} ={ bigcup_{ ;} }_i E_i^{c ;} end{array}$ | . Definition (axioms of probability) . The probability $P$ on a sample space $ Omega$ assigns numbers to events $ Omega$ of in such a way that . The probability of any event is non-negative : $P left lbrace E right rbrace ge 0$ | The probability if the sample space is one : $P left lbrace Omega ; right rbrace =1$ | For any finitely or countably infinitely many manually exclusive events $E_{1,} E_2 , ldotp ldotp ldotp$ , $P left lbrace { bigcup_{i ;} E_{i ;} }_{ ;} right rbrace = sum_i P left lbrace E_i right rbrace ;$ | A few simple facts . Inclusion-exclusion principle: . For any events $E$ and $F$, $P left lbrace E cup F right rbrace =P left lbrace E right rbrace +P left lbrace F right rbrace -P left lbrace E cap F right rbrace$ | For any events $E$, $F$ and $G$: $P left lbrace E cup F cup G right rbrace =P left lbrace E right rbrace +P left lbrace F right rbrace +P left lbrace G right rbrace -P left lbrace E cap F right rbrace -P left lbrace E cap G right rbrace -P left lbrace F cap G right rbrace +P left lbrace E cap F cap G right rbrace$ | Generally: $p left lbrace E_1 cup E_2 cup E_3 cup ldotp ldotp ldotp cup E_n right rbrace = sum_{1 le i le n} P left lbrace E_i right rbrace - sum_{1 le i_1 le i_2 le n} P left lbrace E_{i_1 } cap E_{i_2 } right rbrace + sum_{1 le i_1 le i_2 le i_{3 ;} le n} left lbrace P left lbrace E_{i_1 } cap E_{i_2 } cap E_{i_3 } right rbrace right rbrace - ldotp ldotp ldotp ;+{ left(-1 right)}^{n+1} P left lbrace E_1 cap E_2 cap E_3 right rbrace$ | . Boole&#39;s inequality . For any events $E_1 ,E_2 , ldotp ldotp ldotp E_n$ $P left lbrace bigcup_{i=1}^n E_i right rbrace le sum_{i=1}^n P left lbrace E_i right rbrace$ | . Example . Out of $n$ people, what is the probability that there are no coinciding birthdays? $| Omega |={365}^n$ $|E|=365 ldotp 364 ldotp ldotp ldotp left(365-n+1 right)= frac{365!}{ left(365-n right)!}$ $P left lbrace E right rbrace = frac{|E|}{| Omega |}= frac{365!}{ left(365-n right)!{365}^n }$ . Conditional Probability . Let $F$ be an Event with $P left lbrace F right rbrace &gt;0$ . then the conditional probability $E$ of given $F$ is defined as: $P left lbrace E|F right rbrace := frac{P left lbrace E cap F right rbrace }{P left lbrace F right rbrace }$ . Note: Conditional Probability can be interpreted as:&quot;In what proportion of case in $F$ will also $E$ occur?&quot; or &quot;How does the probability of both $E$ and $F$ compare to the probability of $F$ only?&quot; conditional probability is a proper probability and it satisfies the axioms: . The conditional probability of any event is non-negative :$P left lbrace E|F right rbrace ge 0$ | The conditional probability if the sample space is one :$P left lbrace Omega |F ; right rbrace =1$ | For any finitely or countably infinitely many manually exclusive events $E_{1,} E_2 , ldotp ldotp ldotp$ , $P left lbrace { bigcup_{i ;} E_{i ;} |F}_{ ;} right rbrace = sum_i P left lbrace E_i |F right rbrace ;$ | Corollary . $P left lbrace E^c |F right rbrace =1-P left lbrace E|F right rbrace$ | $P left lbrace phi |F right rbrace =0$ | $P left lbrace E|F right rbrace =1-P left lbrace E^c |F right rbrace le 1$ | $P left lbrace left(E cup G right)|F right rbrace =P left lbrace E|F right rbrace +P left lbrace G|F right rbrace -P left lbrace E cap F|F right rbrace$ proof: $ begin{array}{l} P left lbrace left(E cup G right)|F right rbrace = frac{P left lbrace left(E cup G right) cap F right rbrace }{P left lbrace F right rbrace }= frac{P left lbrace left(E cap F right) cup left(G cap F right) right rbrace }{P left lbrace F right rbrace } = frac{P left(E cap F right)+P left(G cap F right)-P left lbrace left(E cap F right) cap ; left(G cap F right) right rbrace }{P left lbrace F right rbrace }= frac{P left(E cap F right)+P left(G cap F right)-P left lbrace E cap G cap F right rbrace }{P left lbrace F right rbrace } =P left lbrace E|F right rbrace +P left lbrace G|F right rbrace -P left lbrace E cap F|F right rbrace end{array}$ | if $E subseteq G$ then $P left lbrace left(G-E right)|F right rbrace =P left lbrace G|F right rbrace -P left lbrace E|F right rbrace$ proof: $ begin{array}{l} P left lbrace G|F right rbrace -P left lbrace E|F right rbrace = frac{ ;P left lbrace G cap F right rbrace }{P left lbrace mathrm{F} right rbrace }- frac{ ;P left lbrace E cap F right rbrace }{P left lbrace mathrm{F} right rbrace } = frac{P left lbrace left(G cap F right)- left(E cap F right) right rbrace ;}{P left lbrace mathrm{F} right rbrace }= frac{P left lbrace left(G cap F right) cap { left(E cap F right)}^c right rbrace ;}{P left lbrace mathrm{F} right rbrace } = frac{P left lbrace left(G cap F right) cap { left(E^{c ;} cup F^{c ;} right)}^{ ;} right rbrace ;}{P left lbrace mathrm{F} right rbrace }= frac{P left lbrace G cap left(F cap { left(E^{c ;} cup F^{c ;} right)}^{ ;} right) right rbrace ;}{P left lbrace mathrm{F} right rbrace } = frac{P left lbrace G cap left({ left({F cap ;E}^{c ;} right) cup left({F cap ;F}^{c ;} right)}^{ ;} right) right rbrace ;}{P left lbrace mathrm{F} right rbrace }= frac{P left lbrace G cap {F cap ;E}^{c ;} right rbrace ;}{P left lbrace mathrm{F} right rbrace } = frac{P left lbrace G cap { ;E}^{c ;} cap F right rbrace ;}{P left lbrace mathrm{F} right rbrace }= frac{P left lbrace left(G-{ ;E}^{ ;} right) cap F right rbrace ;}{P left lbrace mathrm{F} right rbrace }=P left lbrace left(G-E right)|F right rbrace end{array}$ if $A subseteq B$ then $P left(B right)-P left(A right)=P left(B_{ ;} -A right)$ Here as $E subseteq G$ so $ left(E cap F right) subseteq left(G cap F right)$ so we can write $P left lbrace G cap F right rbrace - ;P left lbrace E cap F right rbrace=P left lbrace left(G cap F right)- left(E cap F right) right rbrace$ | . . if $E subseteq G$ then $P left lbrace E|F right rbrace le P left lbrace G|F right rbrace$ | . Multiplication Rule . For $E_1 ,E_2 , ldotp ldotp ldotp E_n$ events: $P left lbrace E_1 cap E_2 cap ldotp ldotp ldotp cap E_n right rbrace =P left lbrace E_1 right rbrace ldotp P left lbrace E_2 |E_1 right rbrace ldotp P left lbrace E_3 |E_1 cap E_2 right rbrace ldotp ldotp ldotp ldotp P left lbrace E_n |E_1 cap E_2 cap ldotp ldotp ldotp cap E_{n-1} right rbrace$ . The law of total probability . This is also known as partition theorem For any events $E$ and $F$ $P left lbrace E right rbrace =P left lbrace E|F right rbrace ldotp P left lbrace E right rbrace +P left lbrace E|F^c right rbrace ldotp P left lbrace F^c right rbrace$ $P left lbrace E right rbrace = sum_i P left lbrace E|F_i right rbrace ldotp P left lbrace F_i right rbrace$ . Bayes&#39; Theorem . For any events $E$ and $F$ $P left lbrace F|E right rbrace = frac{P left lbrace E|F right rbrace ldotp P left lbrace F right rbrace }{P left lbrace E|F right rbrace ldotp P left lbrace F right rbrace +P left lbrace E|F^c right rbrace ldotp P left lbrace F^c right rbrace }$ . Important: if ${ left lbrace F_i right rbrace }_i$ is a complete system of events, then $$P left lbrace F_i |E right rbrace = frac{ ;P left lbrace E|F_i right rbrace ldotp P left lbrace F_i right rbrace }{ sum_j ;P left lbrace E|F_j right rbrace ldotp P left lbrace F_j right rbrace }$$ . Independence . Event $E$ and $F$ are independent if $P left lbrace E|F right rbrace =P left lbrace E right rbrace$ or $P left lbrace E cap F right rbrace =P left lbrace E right rbrace cdot P left lbrace F right rbrace$ . Important: Mutually exclusive events are necessarily also dependent events because one&#8217;s existence depends on the other&#8217;s non-existence.Dependent events are not necessarily mutually exclusive . If $A$ and $B$ are independent then $A^c$ and $B$ are also also independent Proof: $P left(A^c |B right)= frac{P left(A^c cap B right)}{P left(B right)}= frac{P left(B right)-P left(A^{ ;} cap ; ;B right)}{P left(B right)}=1-P left(A|B right)=1-P left(A right)=P left(A^c right)$ | . Three events $E$, $F$ and $G$ are (mutually) independent if . $P left lbrace E cap F right rbrace =P left lbrace E right rbrace ldotp P left lbrace F right rbrace$ | $P left lbrace E cap G right rbrace =P left lbrace E right rbrace ldotp P left lbrace G right rbrace$ | $P left lbrace F cap G right rbrace =P left lbrace F right rbrace ldotp P left lbrace G right rbrace$ | $P left lbrace E cap F cap G right rbrace =P left lbrace E right rbrace ldotp P left lbrace F right rbrace ldotp P left lbrace G right rbrace$ | .",
            "url": "https://abhiyantaabhishek.github.io/MDS-IIT/mathematical%20foundations%20of%20data%20science/2022/08/06/CS6660-week1.html",
            "relUrl": "/mathematical%20foundations%20of%20data%20science/2022/08/06/CS6660-week1.html",
            "date": " • Aug 6, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Machine Learning Introduction",
            "content": "Types of ML . Supervised Learning classification, regression | . | Unsupervised learning | Other settings of ML Reinforcement learning | Semi-supervised learning | Active learning,Transfer learning,Structured learning | . | Dimensionality Reduction (unsupervised Learning) Large sample size is required for high dimensional data | Query accuracy and efficiency degrade rapidly as the dimension increases | strategies: Feature reduction, Feature selection, Manifold learning, Kernel learning | . | . | . . IID Assumption . Identically independently distributed : This is the assumption that the training data and testing data comes from the same distribution | . Types of Models . Induction : Model Learns by Induction, ( creating it&#39;s own rules for example, if we do extensive research while buying mobile, we create set rules, it is called induction). | Transductions: Model learns from references ( for example, if we ask our friends about mobile and we buy according to their suggestion, it is called Transduction). | Online : data could be a stream, data keeps coming over time. | Offline: data is already acquired and trained offline. | Generative: Learns the distribution, the model learns joint probability distribution. | Discriminative:Learns to discriminate without learning distribution. | Parametric : The model have parameters like $ mu$ and $ sigma$ | Non parametric: The model doesn&#39;t have parameters, as in K nearest neighbor (KNN) | . Classifier evaluation . . Training Error Not very useful | Relatively easy to obtain low error | $E_{ mathrm{train}} = frac{1}{n} sum_{i=1}^n mathrm{error} left(f_D left(X_i right),y_i right)$ | . | Generalization Error Measure of how well do we do on unseen data | $E_{ mathrm{gen}} = int mathrm{error} left(f_D left(X right),y right)p left(y,X right) mathrm{dX}$ | . | . Stratified sampling . First stratify instances by class, then randomly select instances from each class proportionally | It ensures that the proportion of each class remains same in training and validation set. | . Model Selection . Re-Substitution : not useful as it suggests to re- substitute the train data for validation as well | K - Fold cross-validation : Divide the data in K fold using stratified sampling, and the select some set for training and some for validation in each iteration. | Leave-one-out N-fold cross-validation | . | .",
            "url": "https://abhiyantaabhishek.github.io/MDS-IIT/foundations%20of%20machine%20learning/2022/08/06/CS5590-week1.html",
            "relUrl": "/foundations%20of%20machine%20learning/2022/08/06/CS5590-week1.html",
            "date": " • Aug 6, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am an M.tech Student at IIT Hyderabad, Also working as Senior Data Scientist at Bosch. .",
          "url": "https://abhiyantaabhishek.github.io/MDS-IIT/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://abhiyantaabhishek.github.io/MDS-IIT/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}