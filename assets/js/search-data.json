{
  
    
        "post0": {
            "title": "Linear Algebra CS6660 week 3",
            "content": "system of Linear equations . Let us see some Systems of linear equations A company produce products $ N_1, dots ,N_n $ for which resources $R_1, dots ,R_m$ are required. To produce a unit of porduct $N_j, a_{ij}$ units of resources $R_i$ are needed, where $i=1, dots ,m$ and $j=1, dots ,n$. The objective is to find an optimal production plan, i.e., a plan of how many units $x_j$ of product $N_j$ should be produced if a total of $b_i$ units of resource $R_i$ are available and (ideally) no resources are left over. write system of equation of the same $$ left lbrace begin{array}{c} a_{11}x_1 + dots +a_{1n}x_n =b_1 vdots a_{m1}x_1+ dots +a_{mn}x_n=b_n end{array} right.$$ | John received an inheritance of $ $12000$ that he divided in three parts and invested in three ways: in a money market fund paying $3 %$ annual interest; in municipal bonds paying $4 %$ of annual interest; and in mutual fund paying $7 %$ of interest, john invested $ $4,000$ more in mutual funds than in municipal bonds. He earned $ $670$ in interest the first year.How much did john invest in each type of found $$ left lbrace begin{array}{c} x+y+z=12000 0.03x+0.04y+0.07z=670 -x+z=4000 end{array} right. $$ | solution of above question: $x=3500,y=1000,z=7500$ | . | . . If there are more variables than equations than there might be infinitely many solutions. | If $AX=b$ has infinitely many solutions then to find other solutions we find $Y$ such that $AY=b$, then we can write $A(X+Y)=b$ as $A(X+Y)=AX+AY=AX$ because $AY=0$ here, $X+Y$ is another solution here. If $AX=b$ and $AY=0$ then $A(X+Y)=b$ | . Particular and general solutions $1$. Find a particular solution to $AX=b$ | $2$. Find all the solutions to $AX=0$ | $3$. Combine the solution from step $1$ and $2$ to get the general solution | . | .",
            "url": "https://abhiyantaabhishek.github.io/MDS-IIT/mathematical%20foundations%20of%20data%20science/2022/09/03/CS6660-week3.html",
            "relUrl": "/mathematical%20foundations%20of%20data%20science/2022/09/03/CS6660-week3.html",
            "date": " • Sep 3, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Classifiers (Decision Trees, NBC) CS5590  week 3",
            "content": "Decision Trees . An efficient nonparametric method. | A hierarchical model. | Divide and conquer strategy. | Internal decision nodes Univariate : It uses a single attribute $X_i$ Numeric $X_i$: If numeric data perform Binary split:$X_i&gt;w_m$ | . | Discrete $X_i$: For discrete data perform n- way split for n possible values | . | . | Multivariate: It uses more than one attributes, $X$ | . | Leaves Classification : Class labels, or proportions | Regression : Numeric, r average, or local fit | . | Learning is greedy; find the best split recursively. | For node $m$, $N_m$ instances reach $m$, $N_m^i$ belong to $C_i$ $ hat P(C_i|X,m) equiv p_m^i = frac{N_m^i}{N_m} $ | Node $m$ is pure if $p_m^i$ is $0$ or $1$ | Measure if impurity is entropy $ displaystyle I_m=- sum_{i=1}^kp_m^i log_2p_m^i$ | . compare probability distributions vs entropy . from matplotlib import pyplot import matplotlib.pyplot as plt import numpy as np # calculate entropy def entropy(events, ets=1e-15): return -sum([p * np.log2(p + ets) for p in events]) # define probabilities probs = np.arange(0.0001,1,0.001) # create probability distribution dists = [[p, 1.0 - p] for p in probs] # calculate entropy for each distribution ents = [entropy(d) for d in dists] # plot probability distribution vs entropy plt.plot(probs, ents) plt.title(&#39;Probability Distribution vs Entropy for 2 class problem&#39;) plt.xlabel(&#39;Probability Distribution&#39;) plt.ylabel(&#39;Entropy (bits)&#39;) plt.show() . . Entropy in information theory specifies the average (expected) amount of information derived from observing an event . . How to generate decision tree . Select a root not which divides the data best based on impurity measures. | If node is pure, generate a leaf and stop, otherwise split and continue recursively. | Impurity after split: It is probability weighted entropy given by: $ displaystyle I_m^{ prime }=- sum_{j=1}^{n} frac{N_{mj}}{N_m} sum_{i=1}^kp_{mj}^i log_2p_{mj}^i$, here, $N_{mj}$ is $j^{th}$ branch of $N_m$ and $N_{mj}^i$ belongs to $i^{th}$ class. | . | . | Information gain: Expected reduction in impurity measure after split. Chose the attribute with maximum information gain. | Other impurity measure method - Gini impurity/index : $ displaystyle 1- sum_{j=1}^cp_j^2$ | . Overfitting and generalization . Noisy training example or if only small number of samples are associated leaf nodes can cause overfitting. | Using Pruning for better generalization Pruning is the process of removing subtree. Pre-pruning: Early stopping, after a predetermined performance. | Post-pruning: Grow the whole then prune the subtree which overfit on the pruning set | . | Pre-pruning is faster, post-pruning is more accurate. | . | . Occam&#39;s Razor Principle . When multiple hypotheses can solve the problem chose the simplest one . Select best tree . Measure performance over training and separate validation data set | Minimum Description Length : Minimize size(tree)+size(miscalssifications(tree)) | . Rule Extraction from Trees . Convert tree to equivalent set of rules.(&quot;if else&quot; condition for example). | Prune each rules independently of others, by removing any pre-conditions that result in improving its estimates accuracy. | Sort final rules into desired sequence for use. | . from sklearn.datasets import load_iris from sklearn import tree import matplotlib.pyplot as plt iris = load_iris() X, y = iris.data, iris.target clf = tree.DecisionTreeClassifier() clf = clf.fit(X, y) . fig, ax = plt.subplots(1, 1, figsize=(10, 10)) tree = tree.plot_tree(clf,ax=ax) . Naive Bayes . . In Naive Bayes classifier goal is to learn function $f:X rightarrow y$, where $y$ is one of $k$ classes and $X=X_1,...,X_n$: values of attributes (numeric or categorical) | It is a probabilistic classification most probable class given observation: $ displaystyle hat{y}= arg max_y P(y|x)$ | Bayesian probability of a class: $ displaystyle P left( Y|X right)= frac{P left( X|Y right)P left( Y right)}{ sum_{y&#39;}P left( X|Y&#39; right)P left( Y&#39; right)}$ | . | . Formulation . consider a record with attributes $A_1,A_2, dots ,A_n$ | Goal is to predict class $C$ | Specifically, we want to find the value of $C$ that maximizes $P(C|A_1,A_2, dots,A_n)$ | . . what is Naive about Naive Bayes? The attributes are considered independent of each other, this is Naive in Naive Bayes. | . | . AS we assume independence among attributes $A_i$ so we can write: $P(A_1,A_2, dots ,A_n|C_j)=P(A_i|C_j)P(A_2|C_j) dots P(A_n|C_j)$ | New point is classified to $C_j$ if $P(C_j) prod_{j}P(A_i|C_j)=P(C_j)P(A_i|C_j)(A_2|C_j) dots P(A_n|C_j) $ is maximal . | Assume that all hypotheses (classes) are equally probable a priori, i.e., $P(C_i)=P(C_j)$ for all $i,j$ | This is called assuming a uniform prior. It simplifies computing the posterior: $ displaystyle C_{ML}= arg max_c P(A_1,A_2, dots A_n|C)$ | This hypothesis is called the maximum likelihood hypothesis . | . Example . import pandas as pd Attributes =[&#39;Day&#39;, &#39;Outlook&#39;, &#39;Temperature&#39;, &#39;Humidity&#39;, &#39;Wind&#39;, &#39;Play Tennis&#39;] data =[[&#39;D1&#39;, &#39;Sunny&#39;, &#39;Hot&#39;, &#39;High&#39;, &#39;Weak&#39;, &#39;No&#39; ], [&#39;D2&#39;, &#39;Sunny&#39;, &#39;Hot&#39;, &#39;High&#39;, &#39;Strong&#39;, &#39;No&#39; ], [&#39;D3&#39;, &#39;Overcast&#39;, &#39;Hot&#39;, &#39;High&#39;, &#39;Weak&#39;, &#39;Yes&#39; ], [&#39;D4&#39;, &#39;Rain&#39;, &#39;Mild&#39;, &#39;High&#39;, &#39;Weak&#39;, &#39;Yes&#39; ], [&#39;D5&#39;, &#39;Rain&#39;, &#39;Cool&#39;, &#39;Normal&#39;, &#39;Weak&#39;, &#39;Yes&#39; ], [&#39;D6&#39;, &#39;Rain&#39;, &#39;Cool&#39;, &#39;Normal&#39;, &#39;Strong&#39;, &#39;No&#39; ], [&#39;D7&#39;, &#39;Overcast&#39;, &#39;Cool&#39;, &#39;Normal&#39;, &#39;Strong&#39;, &#39;Yes&#39; ], [&#39;D8&#39;, &#39;Sunny&#39;, &#39;Mild&#39;, &#39;High&#39;, &#39;Weak&#39;, &#39;No&#39; ], [&#39;D9&#39;, &#39;Sunny&#39;, &#39;Cool&#39;, &#39;Normal&#39;, &#39;Weak&#39;, &#39;Yes&#39; ], [&#39;D10&#39;, &#39;Rain&#39;, &#39;Mild&#39;, &#39;Normal&#39;, &#39;Weak&#39;, &#39;Yes&#39; ], [&#39;D11&#39;, &#39;Sunny&#39;, &#39;Mild&#39;, &#39;Normal&#39;, &#39;Strong&#39;, &#39;Yes&#39; ], [&#39;D12&#39;, &#39;Overcast&#39;, &#39;Mild&#39;, &#39;High&#39;, &#39;Strong&#39;, &#39;Yes&#39; ], [&#39;D13&#39;, &#39;Overcast&#39;, &#39;Hot&#39;, &#39;Normal&#39;, &#39;Weak&#39;, &#39;Yes&#39; ], [&#39;D14&#39;, &#39;Rain&#39;, &#39;Mild&#39;, &#39;High&#39;, &#39;Strong&#39;, &#39;No&#39; ]] df = pd.DataFrame(columns=Attributes,data=data) print(df.head()) . Day Outlook Temperature Humidity Wind Play Tennis 0 D1 Sunny Hot High Weak No 1 D2 Sunny Hot High Strong No 2 D3 Overcast Hot High Weak Yes 3 D4 Rain Mild High Weak Yes 4 D5 Rain Cool Normal Weak Yes .",
            "url": "https://abhiyantaabhishek.github.io/MDS-IIT/foundations%20of%20machine%20learning/2022/08/27/CS5590-week3.html",
            "relUrl": "/foundations%20of%20machine%20learning/2022/08/27/CS5590-week3.html",
            "date": " • Aug 27, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "FoML CS5590 Assignment 1",
            "content": "Questions . Q. 1. . k-NN: (8 marks) In $k$-nearest neighbors $(k-NN)$, the classification is achieved by majority vote in the vicinity of data. Given $n$ points in a 2-dimensional space, consider two settings: Setting $A$: imagine two classes of data each of $n/2$ points, which are overlapped to some extent in the space. | Setting $B$: imagine three classes of data each of $n/3$ points, which are overlapped to some extent in the space.Now, answer the following questions: $(a)$ (2 marks) Describe what happens to the training error (using all available data) when the neighbor size $k$ varies from $n$ to $1$ in Setting $A$. How does this behavior change in Setting $B$? | $(b)$ (3 marks) Predict and explain with a sketch how the generalization error (e.g. holding out some data for testing) would change when $k$ varies in both settings? Explain your reasoning. | $(c)$ (3 marks) Is it possible to build a univariate decision tree $($ with decisions at each node of the form “is $x &gt; a$”, “is $x &lt; b$”, “is $y &gt; c$”, or “is $y &lt; d$” for any real constants $a, b, c, d$ $)$ which classifies exactly similar to a $1-NN$ using the Manhattan (city block) distance? If so, give an example of a dataset setup where this can happen. If not, explain why not. | . | . | . Answer . $(a)$ Setting A: As the value of $k$ is varied for $n$ to $1$ the error starts to decrease , becomes &#39;almost&#39; constant after forming elbow like shape and then the error becomes $0$ when $k$ becomes $1$ but error may not reduce gradually, it may fluctuate depending on the noise present in the data. The training error becomes $0$ at $k=1$, this can also be called as over fitting. | Setting B: The error starts to decrease as value of $k$ is varied form $n$ to $1$ and becomes $0$ at $k=1$, the difference here is that it takes time for the error to become &#39;almost&#39; constant as value of $k$ is varied from $n$ to $1$, i.e. the elbow point appears later as compared to setting $A$ when the value of $k$ is varied from $n$ to $1$ | . | $(b)$ Generalization error is plotted in below picture&#39;s right most column (instead of hand sketch), The generalization error will also decrease in both the setting as value of $k$ is varied from $n$ to $1$, error becomes constant after forming elbow shape, but when value of $k$ is further reduced the model starts to over-fit and when $n$ becomes $1$ the generalization error increases by a significant amount , as the model highly over fits to the data at this point. The difference among the two settings is that the elbow point appears earlier in setting $A$ and later in setting $B$ as the value of $k$ is varied from $n$ to $1$. | . Below is the proof of answer given above for setting $A$ and $B$ . import matplotlib.pyplot as plt import numpy as np from sklearn.neighbors import KNeighborsClassifier from sklearn import metrics from sklearn.model_selection import StratifiedShuffleSplit def get_multivariate(_mean,_cov,_numData,_class): np.random.seed(0) data = np.random.multivariate_normal(_mean,_cov,_numData) target = np.ones(_numData)*_class return data,target def get_dataAndTarget(numData,numClass): meanA = [1,1] covA = [[0.01,0.001], [0.001,0.01]] meanB = [1.3,1.3] covB = [[0.01,0.001], [0.001,0.01]] meanC = [1.4,0.8] covC = [[0.01,0.001], [0.001,0.01]] if(numClass==3): dataA,targetA = get_multivariate(meanA,covA,numData//3,0) dataB,targetB = get_multivariate(meanB,covB,numData//3,1) dataC,targetC = get_multivariate(meanC,covC,numData//3,2) data = np.vstack([dataA,dataB,dataC]) target = np.hstack([targetA,targetB,targetC]) return data,target if(numClass==2): dataA,targetA = get_multivariate(meanA,covA,numData//2,0) dataB,targetB = get_multivariate(meanB,covB,numData//2,1) data = np.vstack([dataA,dataB]) target = np.hstack([targetA,targetB]) return data,target def runKNeighborsClassifier(numData,numClass): data ,target = get_dataAndTarget(numData,numClass) sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0) train_index, test_index = list(sss.split(data, target))[0] X_train, X_test = data[train_index], data[test_index] y_train, y_test = target[train_index], target[test_index] k_range = range(1,X_train.shape[0]) scores_list = [] for k in k_range: knn = KNeighborsClassifier(n_neighbors=k) knn.fit(X_train,y_train) y_pred=knn.predict_proba(X_train) scores_list.append(metrics.log_loss(y_train,y_pred)) scores_list_test = [] for k in k_range: knn = KNeighborsClassifier(n_neighbors=k) knn.fit(X_train,y_train) y_pred=knn.predict_proba(X_test) scores_list_test.append(metrics.log_loss(y_test,y_pred)) return scores_list,scores_list_test,data,X_train, X_test, num_data=1200 scores_list_2,scores_list_test_2,data_2,X_train_2, X_test_2 =runKNeighborsClassifier(num_data,2) scores_list_3,scores_list_test_3,data_3,X_train_3, X_test_3 =runKNeighborsClassifier(num_data,3) fig, ax = plt.subplots(2, 4, figsize=(24, 10)) ax[0,0].scatter(data_2[:num_data//2,0],data_2[:num_data//2,1]) ax[0,0].scatter(data_2[num_data//2:,0],data_2[num_data//2:,1]); ax[0,0].set_title(&#39;Setting A, Data distribution&#39;) ax[0,1].scatter(X_train_2[:,0],X_train_2[:,1]) ax[0,1].scatter(X_test_2[:,0],X_test_2[:,1]); ax[0,1].set_title(&#39;Setting A, Trained and test distribution&#39;) ax[0,2].plot(scores_list_2) ax[0,2].set_xlabel(&#39;Value of K for KNN&#39;) ax[0,2].set_ylabel(&#39;Train Error&#39;); ax[0,2].set_title(&#39;Setting A, K vs Train Error&#39;) ax[0,2].invert_xaxis() ax[0,2].set_xticks(np.hstack([np.arange(len(scores_list_2),0,-len(scores_list_2)//5),1])) ax[0,2].set_xticklabels(np.hstack([np.arange(len(scores_list_2),0,-len(scores_list_2)//5),1])) ax[0,3].plot(scores_list_test_2) ax[0,3].set_xlabel(&#39;Value of K for KNN&#39;) ax[0,3].set_ylabel(&#39;Test Error&#39;); ax[0,3].set_title(&#39;Setting A, K vs Test Error&#39;) ax[0,3].invert_xaxis(); ax[0,3].set_xticks( np.hstack([np.arange(len(scores_list_2),0,-len(scores_list_2)//5),1])) ax[0,3].set_xticklabels(np.hstack([np.arange(len(scores_list_2),0,-len(scores_list_2)//5),1])) ax[1,0].scatter(data_3[:num_data//3,0],data_3[:num_data//3,1]) ax[1,0].scatter(data_3[num_data//3:2*num_data//3,0],data_3[num_data//3:2*num_data//3,1]); ax[1,0].scatter(data_3[2*num_data//3:,0],data_3[2*num_data//3:,1]); ax[1,0].set_title(&#39;Setting B, Data distribution&#39;) ax[1,1].scatter(X_train_3[:,0],X_train_3[:,1]) ax[1,1].scatter(X_test_3[:,0],X_test_3[:,1]); ax[1,1].set_title(&#39;Setting B, Trained and test distribution&#39;) ax[1,2].plot(scores_list_3) ax[1,2].set_xlabel(&#39;Value of K for KNN&#39;) ax[1,2].set_ylabel(&#39;Train Error&#39;); ax[1,2].set_title(&#39;Setting B, K vs Train Error&#39;) ax[1,2].invert_xaxis() ax[1,2].set_xticks( np.hstack([np.arange(len(scores_list_3),0,-len(scores_list_3)//5),1])) ax[1,2].set_xticklabels( np.hstack([np.arange(len(scores_list_3),0,-len(scores_list_3)//5),1])) ax[1,3].plot(scores_list_test_3) ax[1,3].set_xlabel(&#39;Value of K for KNN&#39;) ax[1,3].set_ylabel(&#39;Test Error&#39;); ax[1,3].set_title(&#39;Setting B, K vs Test Error&#39;) ax[1,3].invert_xaxis(); ax[1,3].set_xticks(np.hstack([np.arange(len(scores_list_3),0,-len(scores_list_3)//5),1])) ax[1,3].set_xticklabels(np.hstack([np.arange(len(scores_list_3),0,-len(scores_list_3)//5),1])); . . $(c)$ Yes it is possible to build a univariate tree as described in the question, which classifies exactly similar to $1-NN$ using Manhattan distance. Below is the example of the dataset: For simplicity we consider $2$ class classification setting, in $2$ dimensional space, Class $A$ and Class $B$ each contains just one point: $A = {(1,3)}$ $B = {(3,3)}$ Consider value of $a=2,b=4,c=2$ and $d=4$ Decision Tree: consider point $a,b$ lies on $X$ axis and pint $d,c$ lies on $Y$ axis,Classes can be distinguished with just one question $x&gt;a$ Point $(1,3)$: here $x=1,y=3$, if we make decision using the rules, $x&gt;a$, it is false, hence we classify it for class $A$. | Point $(3,3)$: here $x=3,y=3$, if we make decision using the rules, $x&gt;a$, it is true, $x&lt;b$ is true, $y&gt;c$ is true, $y&lt;d$ is true, hence we classify the point for class $B$ | . | $1-NN$: As the training dataset contains points $(1,3),(3,3)$ so any points which has $X$ value greater than $2$ will be classified to class $B$ and lesser will be classified for class $A$. | . | . This is explained in below code and picture plotted : . from sklearn import tree,neighbors import matplotlib.pyplot as plt import seaborn as sns from matplotlib.colors import ListedColormap import numpy as np from sklearn.inspection import DecisionBoundaryDisplay # Use scikit-learn version 1.1.0 to use Decision Boundary, pip install scikit-learn==1.1.0 clf = tree.DecisionTreeClassifier() X=np.array([[1,3], [3,3]]) y= [0,1] className = [&#39;A&#39;,&#39;B&#39;] clf = clf.fit(X, y) fig, ax = plt.subplots(1, 2, figsize=(8, 3)) tree = tree.plot_tree(clf,ax=ax[0],filled=True,impurity=False,class_names=className) ax[0].set_title(&#39;Decision tree &#39;) knn = neighbors.KNeighborsClassifier(n_neighbors=1,metric=&#39;cityblock&#39;) knn.fit(X,y) xx, yy = np.meshgrid(np.arange(0, 5, 0.1), np.arange(0, 5, 0.01)) Z = knn.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape) cmap_light = ListedColormap([&quot;orange&quot;, &quot;cyan&quot;]) cmap_bold = [&quot;darkorange&quot;, &quot;c&quot;] DecisionBoundaryDisplay.from_estimator(knn,X,cmap=cmap_light,ax=ax[1],response_method=&quot;predict&quot;,plot_method=&quot;pcolormesh&quot;) sns.scatterplot(x=X[:, 0], y=X[:, 1],hue=className,palette=cmap_bold,alpha=1.0,edgecolor=&quot;black&quot;) ax[1].set_title(&#39;Decision boundey of 1 -NN&#39;); . . Q. 2. . Bayes Classifier: (6 marks) $(a)$ (3 marks) Consider a classification problem with $K$ classes for which the feature vector $ phi$ has $M$ dimensions (categorical variables) each of which can take $L$ discrete states. Let the values of the dimensions be represented by a one-hot ($1$-of-$L$) binary coding scheme. Further suppose that, conditioned on the class $C_k$, the $M$ dimensions of $ phi$ are independent, so that the class-conditional density factorizes with respect to the feature vector components (also referred to as the naive Bayes assumption). Show that the quantities given below:$$a_k= ln p( phi|C_k)p(C_k)$$ are linear function of the components of $ phi$. | $(b)$ (3 marks) You are now going to make a text classifier. To begin with, you attempt to classify documents as either sport or politics. You decide to represent each document as a (row) vector of attributes describing the presence or absence of the following words. $X$ = (goal,football,golf,defence,offence,wicket,office,strategy) Training data from sport documents and from politics documents is represented below in a matrix in which each row represents the 8 attributes. $$ x_{politics}= left lbrack begin{array}{cccccccc} 1 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 end{array} right rbrack$$ $$ x_{sport} = left lbrack begin{array}{cccccccc} 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 end{array} right rbrack$$ Using a maximum likelihood naive Bayes classifier, what is the probability that the document $x = (1, 0, 0, 1, 1, 1, 1, 0)$ is about politics? | . | . Answer . $(a)$ A function is said to be liner if it satisfies below two properties: . Distributive over addition: $f(x+a)=f(x)+f(a)$ | Homogenous of Degree one: $f(ax)=af(x)$ . The give equation is : $a_k= ln p( phi|C_k)p(C_k)$ . | $ ln p( phi+a|C_k)p(C_k) = ln left( p( phi|C_k)+p(a|C_k)-p((a cap phi)|C_k) right)p(C_k) = ln left( p( phi|C_k)+p(a|C_k) right)p(C_k)$, as $ phi$ is independent of $a$ $p((a cap phi)|C_k)$ | $a_k= ln p(a phi|C_k)p(C_k)=a ln p( phi|C_k)p(C_k)$ | . | . Q. 3. . Model Selection: (6 marks) Install the pydataset module (if you haven’t already): . from pydataset import data import pandas as pd melanoma_data = data ( &#39;Melanoma&#39; , show_doc=True ) . The Melanoma dataset consists of measurements of patients with malignant melanoma (a type of cancer). For each patient, the dataset specifies if the patient died or lived at the end of the trial. Moreover, some patients died due to causes unrelated to melanoma. Your task is to do the following: . $(a)$ (1 mark) Remove those patients who died due to causes unrelated to Melanoma, and plot patient status vs age and patient status vs thickness - for your own understanding. | $(b)$ (4 marks) Split the data into 80% training and 20% test set using random stratified sampling. Now, on the 80% training data, perform 3-fold cross-validation using a classifier to predict the status of the patient (do not use the 20% held-aside test data for cross-validation; we will use it later to study generalization performance). Also, remember to use stratified sampling inside cross-validation too. You are allowed to use any existing machine learning library of your choice: scikitlearn, pandas, Weka (we recommend scikitlearn) - but you should use only the decision tree, k-NN or the naive Bayes classifier (to align with what we have covered in class so far, random forests not allowed too). Report the mean of the three quantities (accuracy, precision, recall) on 3-fold cross-validation. | $(c)$ (1 mark) Once you have picked the best classification model in cross-validation, train the best-performing setting on the entire 80% training data, and report performance on the held-aside test set. Report your observation on how representative the training/ validation data was w.r.t test data. | Deliverables: Code | Data splits (for us to verify your reported results) | Brief report (PDF) with your solutions for the above questions | . | . | . Answer . The Melanoma data has following attributes:, . time: survival time in days | status: 1 died from melanoma, 2 alive, 3 dead from other causes. | sex: 1 = male, 0 = female. | age: age in years. | year: year of operation. | thickness: tumour thickness in mm. | ulcer: 1 = presence, 0 = absence. | . $(a)$ : | . from pydataset import data import pandas as pd import matplotlib.pyplot as plt import seaborn as sns melanoma_data = data(&#39;Melanoma&#39;) melanoma_data.drop(melanoma_data[melanoma_data[&#39;status&#39;]==3].index,inplace=True) # drp the data with status &#39;3&#39; fig,ax = plt.subplots(1,2,figsize=(30, 3)) ax[0].scatter(melanoma_data[&#39;age&#39;],melanoma_data[&#39;status&#39;]) ax[0].set_xlabel(&#39;age&#39;,fontsize=14) ax[0].set_ylabel(&#39;status&#39;,fontsize=14) ax[1].scatter(melanoma_data[&#39;thickness&#39;],melanoma_data[&#39;status&#39;]) ax[1].set_xlabel(&#39;thickness&#39;,fontsize=14) ax[1].set_ylabel(&#39;status&#39;,fontsize=14); . corr = melanoma_data.corr() sns.heatmap(corr,cmap=&quot;YlGnBu&quot;, annot=True); . melanoma_data.head() . time status sex age year thickness ulcer . 3 35 | 2 | 1 | 41 | 1977 | 1.34 | 0 | . 5 185 | 1 | 1 | 52 | 1965 | 12.08 | 1 | . 6 204 | 1 | 1 | 28 | 1971 | 4.84 | 1 | . 7 210 | 1 | 1 | 77 | 1972 | 5.16 | 1 | . 9 232 | 1 | 1 | 49 | 1968 | 12.88 | 1 | . X = melanoma_data[[&#39;time&#39;,&#39;status&#39;,&#39;sex&#39;,&#39;age&#39;,&#39;year&#39;,&#39;thickness&#39;]].to_numpy() y = melanoma_data[&#39;ulcer&#39;].to_numpy() . from sklearn import tree,neighbors,naive_bayes import matplotlib.pyplot as plt from sklearn import metrics import seaborn as sns import numpy as np from sklearn.model_selection import StratifiedShuffleSplit dtree = tree.DecisionTreeClassifier() knn = neighbors.KNeighborsClassifier(n_neighbors=1) nb = naive_bayes.GaussianNB() sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0) train_val_index, test_index = list(sss1.split(X, y))[0] X_train_val, X_test = X[train_val_index], X[test_index] y_train_val, y_test = y[train_val_index], y[test_index] sss2 = StratifiedShuffleSplit(n_splits=3, test_size=0.2, random_state=0) for train_index, val_index in sss1.split(X_train_val, y_train_val): X_train, X_val = X_train_val[train_index], X_train_val[val_index] y_train, y_val = y_train_val[train_index], y_train_val[val_index] dtree = tree.DecisionTreeClassifier() dtree.fit(X_train,y_train) y_pred = dtree.predict(X_test) metrics.accuracy_score(y_val, y_pred) . Q. 4. . Decision Trees: (10 marks) In this question, you will use the Car Evaluation dataset, a popular dataset from the UCI Machine Learning Repository. It contains 1728 car sample information with 7 attributes, including one class feature that tells whether the car is in acceptable conditions (class labels: Unacceptable, Acceptable, VeryGood, Good). More details of the dataset description are available on https://archive.ics.uci.edu/ml/datasets/car+evaluation. You must not use the last column as an input feature when you classify the data. You can pre-process the variables into one-hot/ordinal values as you deem fit for each variable. $(a)$ (5 marks) Decision Tree Implementation: Implement your own version of the decision tree using binary univariate split, entropy and information gain. | $(b)$ (2 marks) Cross-Validation: Evaluate your decision tree using 5-fold cross validation. (Divide the entire dataset into 5 parts using stratified sampling, and run this experiment; no test set is required for this question.) Report the average of the 5 folds’ accuracies. With correct implementation of both parts (decision tree and cross validation), your classification accuracy should be around 0.8. | $(c)$ (3 marks) Improvement Strategies: Now, try and improve your decision tree algorithm. Some things you could do include (not exhaustive): Use Gini index instead of entropy | Use multi-way split (instead of binary split) | Use multivariate split (instead of univariate) | Prune the tree after splitting for better generalization Report your performance as an outcome of ANY TWO improved strategies. Deliverables: | . | Code | Brief report (PDF) with: (i) Accuracy of your initial implementation; (ii) Accuracy of your improved implementation, along with your explanation of why the accuracy improved with this change. | . | .",
            "url": "https://abhiyantaabhishek.github.io/MDS-IIT/quizzes%20and%20assignments%20-%20foundations%20of%20machine%20learning/2022/08/26/CS5590-Assignment-1.html",
            "relUrl": "/quizzes%20and%20assignments%20-%20foundations%20of%20machine%20learning/2022/08/26/CS5590-Assignment-1.html",
            "date": " • Aug 26, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Probability CS6660 Quiz 1",
            "content": "Question 1 . Suppose X is a random variable that takes values from the set {0, 1}. If $E(X) = 0.3$, then what is $E(X^2)$? . Answer:we know for Indicator random variable $E left(X right)=0 cdot p left(0 right)+1 cdot p left(1 right)$ so substituting values we get : $0 ldotp 3=1 ldotp p left(1 right)$ $ Rightarrow p left(1 right)=0 ldotp 3 ;,p left(0 right)=0 ldotp 7$ Now: $E left(X^2 right)=0^2 cdot p left(0 right)+1^2 cdot p left(1 right)$ $ Rightarrow E left(X^2 right)=1 cdot p left(1 right)=0 ldotp 3$ . Question 2 . Suppose $A$ and $B$ are two mutually exclusive events with $P(A) = 0.3$ and $P(B) = 0.4$. Then $P(A|B) =?$ and $P(A|B^C) =?$ . Answer:$p left(A|B right)= frac{p left(A cap B right)}{p left(B right)}$ $p left(A cap B right)=0$, mutually exclusive $ Rightarrow p left(A|B right)=0$ Now: $p left(A|B^c right)= frac{p left(A cap B^c right)}{p left(B^c right)}$ $ Rightarrow p left(A|B^c right)= frac{p left(A right)-p left(A cap ;B right)}{p left(B^c right)}$ $ Rightarrow p left(A|B^c right)= frac{0 ldotp 3-0}{1-0 ldotp 4}=0 ldotp 5$ . Question 3 . Suppose $X$ and $Y$ are random variables such that $E(X) = 10, Var(X) = 50, E(Y) = 5,$ and $Var(Y) = 40.$ Then what is $E(2X+Y)?$ . Answer:$E left(2X+Y right)=E left(2X right)+E left(Y right)=2E left(X right)+E left(Y right)$ $ Rightarrow 2 times 10+5=25$ . Question 4 . Suppose $A$ and $B$ are events such that $P(A) = 0.3, P(B) = 0.2$ and $P(A cap B) = 0.1$. Then what is $P(A^c cap B^C)$? . Answer:$P left(A^c cap B^c right)={P left(A cup ;B right)}^c$ $P left(A cup B right)=P left(A right)+P left(B right)-P left(A cap B right)$ $ Rightarrow P left(A cup B right)=0 ldotp 3+0 ldotp 2-0 ldotp 1$ $ Rightarrow P left(A cup B right)=0 ldotp 4$ ${P left(A cup B right)}^c =1-P left(A cup B right)=1-0 ldotp 4=0 ldotp 6$ . Question 5 . Suppose $A$ and $B$ are independent events such that $P(A) = 0.2$ and $P(B) = 0.5$. Then what is $P(A cup B)$? . Answer:$P left(A cup B right)=P left(A right)+P left(B right)-P left(A cap B right)$ $P left(A cap ;B right)=P left(A right) cdot P left(B right)$, Because $A$ and $B$ are independent here. Hence, $P left(A cup B right)=P left(A right)+P left(B right)-P left(A right) cdot P left(B right)$ $ Rightarrow P left(A cup B right)=0 ldotp 2+0 ldotp 5-0 ldotp 2*0 ldotp 5=0 ldotp 6$ . Question 6 . Suppose $X$ is a random variable such that $E(X) = 10$ and $E(X^2) = 150$. What is the variance of $X$? . Answer:$ mathrm{Var} left(X right)=E left(X^2 right)-{ left(E left(X right) right)}^2$ $ Rightarrow mathrm{Var} left(X right)=150-{10}^2 =50$ . Question 7 . Suppose $A$ and $B$ are events such that $P(A) = 0.8, P(B) = 0.6,$ and $P(A^c cap B^c) = 0.2.$ Then which of the following is true? . Neither A nor B is contained in the other | A and B are mutually exclusive | None of the choices can be inferred | A is a subset of B | B is a subset of A | . Answer:$P left(A^c cap B^c right)={P left(A cup B right)}^c$ $P left(A cup B right)=1-{P left(A cup B right)}^c =1-0 ldotp 2=0 ldotp 8$ $P left(A cup B right)=P left(A right)+P left(B right)+P left(A cap B right)$ $ Rightarrow 0 ldotp 8=0 ldotp 8+0 ldotp 6+P left(A cap B right)$ $ Rightarrow P left(A cap B right)=0 ldotp 6$ Notice: $P left(A right)=0 ldotp 8,P left(B right)=0 ldotp 6 ;,P left(A cap B right)=0 ldotp 6$ Hence B is subset of A . Question 8 . Suppose you roll two fair dice and look at the sum of the two results. The probability of getting a value of at least (greater than or equal to) 10 is ? . Answer:The sum value of 10, 11 and 12 qualifies for at least greater than or equal to 10 so the events are ${(6,4),(5,5),(4,6),(6,5),(5.6),(6,6)}$ so the probability is $ frac{6}{36}= frac{1}{6} $ . Question 9 . Let $X$ be a random variable which takes values from $ left lbrace 1,2,3 right rbrace$ with equal probability. What is the variance of $X$? . Answer:$ mathrm{Var} left(X right)=E left(X^2 right)-{ left(E left(X right) right)}^2$ $E left(X right)= frac{1+2+3}{3}=2$ $E left(X^2 right)= frac{1^2 +2^2 +3^2 }{3}= frac{14}{3}$ $ Rightarrow mathrm{Var} left(X right)= frac{14}{3}-4= frac{2}{3}$ . Question 10 . Let $X$ be a random variable which takes values from $ left lbrace 10,20,30 right rbrace$ with equal probability. What is the variance of $X$? . Answer:The answer can be calculated as in question 9, but we can notice that the values is scaled by $10$ as compared to question 9, so the variance will be scaled by $10^2$, hence the answer is $ frac{200}{3}$ . Question 11 . Suppose the probability of a random person of the population having covid is $0.1$. Given that the person has covid, the probability of the person having cough is $0.9$. Given that the person does not have covid, the probability of having cough is $0.01$. Suppose a random person from the population has cough. What is the probability that he/she has covid? . Answer:Given: $P left( mathrm{covid} right)=0 ldotp 1$ $p left( mathrm{cough}| mathrm{covid} right)=0 ldotp 9$ $p left( mathrm{cough}|{ mathrm{covid}}^c right)=0 ldotp 01$ Find: $p left( mathrm{covid}| mathrm{cough} right)=?$ calculate probability of not covid $P left({ mathrm{covid}}^c right)=1-P left( mathrm{covid} right)=1-0 ldotp 1=0 ldotp 9$ Use Bayes theorem $p left( mathrm{covid}| mathrm{cough} right)= frac{p left( mathrm{cough}| mathrm{covid} right) times p left( mathrm{covid} right)}{p left( mathrm{cough}| mathrm{covid} right) times p left( mathrm{covid} right)+p left( mathrm{cough}|{ mathrm{covid}}^c right) times p left({ mathrm{covid}}^c right)}$ $ Rightarrow p left( mathrm{covid}| mathrm{cough} right)= frac{0 ldotp 9 times 0 ldotp 1}{0 ldotp 9 times 0 ldotp 1+0 ldotp 01 times 0 ldotp 9}= frac{10}{11}$ . Question 12 . Let $A$ be an event, and let $P(A)$ denote the probability of the event $A$. Which of the following options is correct? . $P(A)$ is at most $1$ but has no lower bound (unbounded) | $P(A)$ is at least $0$ and at most $1$ | $P(A)$ is at least $0$ but has no upper bound (unbounded) | $P(A)$ has neither an upper bound nor a lower bound | . Answer:P(A) is at least 0 and at most 1 . Question 13 . Let $A$ and $B$ be two events, such that $P(A) = 1$ and $P(B) &gt; 0$. Which of the following is true? . A and B are nether independent nor mutually exclusive | A and B are independent and mutually exclusive | A and B are mutually exclusive but not independent | A and B are independent, but not mutually exclusive | Cannot conclude any of the choices from the information given. | . Answer: . Question 14 . Suppose there is a meeting of 5 friends. The probability that a given member of the population is covid positive is 0.1. Which of the following is an upper bound on the probability that at least one in the meeting is covid positive? . 0.05 | 0.00001 | 0.25 | 0.5 | 0.3 | . Answer:$p left( mathrm{covid} right)=0 ldotp 1$ $ mathrm{At} ; mathrm{least} ; mathrm{one}= mathrm{complement} ; mathrm{of} ; mathrm{none}$ $p left( mathrm{at} ; mathrm{least} ; mathrm{one} ; mathrm{covid} ; mathrm{positive} right)={p left( mathrm{none} ; mathrm{covid} ; mathrm{positive} right)}^c$ $p left( mathrm{none} ; mathrm{covid} ; mathrm{positive} right)=0 ldotp 9 times 0 ldotp 9 times 0 ldotp 9 times 0 ldotp 9 times 0 ldotp 9=0 ldotp 59$ $p left( mathrm{at} ; mathrm{least} ; mathrm{one} ; mathrm{covid} ; mathrm{positive} right)=1-0 ldotp 59=0 ldotp 41$ So the upper bound is 0.5 .",
            "url": "https://abhiyantaabhishek.github.io/MDS-IIT/quizzes%20and%20assignments%20-%20mathematical%20foundations%20of%20data%20science/2022/08/20/CS6660-Quiz-1.html",
            "relUrl": "/quizzes%20and%20assignments%20-%20mathematical%20foundations%20of%20data%20science/2022/08/20/CS6660-Quiz-1.html",
            "date": " • Aug 20, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Machine Learning Model Evaluation Measures and KNN CS5590  week 2",
            "content": "Evaluation Measures . Classification It is a measure of being right/wrong,0-1, eg: hinge loss, cross entropy loss | . | Regression loss It is a measure if how close we are to target, eg: MEA, MES | . | Ranking/search It is a measure of top K search | . | Clustering How well we have described the data ( not straight forward) | . | . Is accuracy adequate . Accuracy may not not be useful in cases where: . There is a large class skew. | There are differential misclassification cost, say getting a positive wrong costs more than getting a negative wrong. | we are most interested in a subset of high confidence predictions. | . Classification Error . . . Tip: Precision = How many retrieved items are relevant? Recall = How many relevant items are retrieved? . . Tip: sensitivity = Probability of positive test given a patient has a disease. Specificity = Probability of a negative test given a patient is well.Specificity = 1 - False Alarm . Utility and cost . Detection Cost: cost = $C_{ mathrm{FP}} times mathrm{FP}+C_{ mathrm{FN}} times mathrm{FN}$ | . | Fmeasure $F1= frac{2 times left( mathrm{Recall} times mathrm{Precision} right)}{ mathrm{Recall}+ mathrm{Precision}}$ | . | . ROC curve . Receiver Operative Curve | Plot between True positive rate on y axis and False positive rate on x axis | AUC : Area under the curve, higher the area, better the performance | . A Receiver Operating Characteristic (ROC) curve plots the TP rate vs. the FP rate as a threshold on the confidence of an instance being positive is varied : . Precision Recall Curve . Plot between Precision on y axis and recall (TPR) on x axis. It is used for class imbalanced dataset mostly, It is also used when we can not calculate True Negative. | . A precision/recall curve plots the precision vs. recall (TP rate) as a threshold on the confidence of an instance being positive is varied. . A nice way to define precision recall etc. . Consider $Y$ as true label and $ hat{Y}$ as predicted label. . precision $=P left(Y=1| ; hat{Y} =1 right)$ | Recall (TPR) $=P left( hat{Y} =1| ;Y=1 right)$ | False positive Rate (FPR) $=P left( hat{Y} =1| ;Y=0 right)$ | True Negative Rate (TNR)= $=P left( hat{Y} =0| ;Y=0 right)$ . Note: Notice that TPR and FPR which makes ROC curve is conditioned on the true value, and precision which is used in PR curve is conditioned on predicted label. This is the reason, PR curve is used for class imbalanced dataset, or where positive class is more interesting then negative class. If question is: &quot;How meaningful is a positive result from my classifier given the baseline probabilities of my problem?&quot;, use a PR curve. If question is, &quot;How well can this classifier be expected to perform in general, at a variety of different baseline probabilities?&quot;, go with a ROC curve. | . ROC curve vs PR curve . Consider a dataset having 100 positive cases and 10,000 negative cases. Now consider 2 classifiers $A$ and $B$. $A$ predicts 9 as true positive, 40 as false positive whereas $B$ predicts 9 as true positive 1000 as false positive. We can observe that as both the classifier predicts 9 out of 10 as true positive so both has same recall value, also FPR is small(as it can be seen in below picture), so the ROC curve which is drawn between recall (TPR) and FPR, will not differentiate among the two classifier. But PR curve which is drawn between precision and recall, will look totally different here as both has different precision. . import numpy as np from sklearn.metrics import confusion_matrix import seaborn as sns import numpy as np import matplotlib.pyplot as plt import seaborn as sns def get_conf_matrix_labels(cf_matrix): group_names = [&#39;True Neg&#39;,&#39;False Pos&#39;,&#39;False Neg&#39;,&#39;True Pos&#39;] group_counts = [&#39;{0:0.0f}&#39;.format(value) for value in cf_matrix.flatten()] group_percentages = [&#39;{0:.2%}&#39;.format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)] labels = [f&#39;{v1} n{v2} n{v3}&#39; for v1, v2, v3 in zip(group_names,group_counts,group_percentages)] labels = np.asarray(labels).reshape(cf_matrix.shape[0],cf_matrix.shape[1]) accuracy = np.trace(cf_matrix) / float(np.sum(cf_matrix)) precision = cf_matrix[1,1] / sum(cf_matrix[:,1]) recall = cf_matrix[1,1] / sum(cf_matrix[1,:]) fpr = cf_matrix[0,1] / sum(cf_matrix[0,:]) stats_text = &quot; n nAccuracy={:0.3f} nPrecision={:0.3f} nRecall={:0.3f} nFPR={:0.4f}&quot;.format( accuracy,precision,recall,fpr) return labels,stats_text YTrue = np.hstack([np.ones(10),np.zeros(100000)]) # data with 100 positive and 10,000 negative cases yPredA = np.hstack([np.ones(9),np.zeros(1),np.ones(40),np.zeros(99960)]) # A predicts 9 True positive, 40 False Positive, yPredB = np.hstack([np.ones(9),np.zeros(1),np.ones(1000),np.zeros(99000)]) # B predicts 9 True positive, 1000 False positive cf_matrixA = confusion_matrix(YTrue, yPredA) cf_matrixB = confusion_matrix(YTrue, yPredB) fig, ax = plt.subplots(1, 2, figsize=(10, 5)) axis_labels = [&#39;Negative Class&#39;,&#39;Positive Class&#39;] labels,stats_text=get_conf_matrix_labels(cf_matrixA) sns.heatmap(cf_matrixA, annot=labels, fmt=&#39;&#39;, cmap=&#39;Blues&#39;,ax=ax[0],cbar=False,xticklabels=axis_labels, yticklabels=axis_labels) ax[0].set_title(&#39;Classifier A&#39;) ax[0].set_xlabel(&#39;Predicted label&#39;+stats_text) ax[0].set_ylabel(&#39;True label&#39;) labels,stats_text=get_conf_matrix_labels(cf_matrixB) sns.heatmap(cf_matrixB, annot=labels, fmt=&#39;&#39;, cmap=&#39;Blues&#39;,ax=ax[1],cbar=False,xticklabels=axis_labels, yticklabels=axis_labels) ax[1].set_title(&#39;Classifier B&#39;) ax[1].set_xlabel(&#39;Predicted label&#39;+stats_text) ax[1].set_ylabel(&#39;True label&#39;); . . Accuracy also is not a good measure in above case. As both the classifier has similar accuracy. . Other performance measures . Kullback-Leibler Diverfence : $D_{ mathrm{KL}} left(P |Q right)= sum_i P left(i right) log frac{P left(i right)}{Q left(i right)}$ | Gini Statistic : $2 times mathrm{AUC}-1$ | F1 score: $ frac{2 times left( mathrm{Recall} times mathrm{Precision} right)}{ mathrm{Recall}+ mathrm{Precision}}$ | Akaike Information Criterion (AIC): $2k-2 ln left(L right)$, here $k$ is number of model parameters, L is max value of the Likelihood function for the model | . Important points . Randomization of data is essential so that held-aside test data can be really representative of new data. | Test set should never be used in any way for normalization, hyper parameter tuning etc. | Any preprocessing done over entire data set ( feature selection, parameter tuning, threshold selection ) must not use labels from test set. | . K-Nearest Neighbors . basic idea . If it walks like a duck, quacks like a duck , then it&#39;s probably a duck. | If data points are represented well then KNN works well. | choosing K is important, if K is too small then it becomes sensitive to noise point. If k is too large, neighborhood may incudes points from other class. | Euclidean distance between two instance $d left(X_i ,X_j right)= sqrt{ sum_{r=1}^n { left(a_r left(X_i right)-a_r{ left(X_j right)} right)}^2 }$ here $a_i left(X right) ;$ denotes features. | In case of continuous valued target function, Mean value of K nearest training examples is taken | . How to determinke K . experiment with different value of K starting form 1 on test set to validate the error, in case of binary classification use odd number for k to avoid ties. | . KNN is a transductive method, there is no training involved , it is refereed as Lazy learning, Learning is just storing all the training instances Similar Keywords: KNN, Memory Based Reasoning, Example Based Reasoning, Instance Based Learning, Case Based Reasoning, Lazy Learning Voronoi Diagram: Decision surface formed by the training Examples for 1 nearest neighbors classifier . Improvements . Distance weighted Nearest Neighbors | Scaling (normalization) attributes for fair computation fo distances | Measure &quot;closeness&quot; differently | Finding &quot;close&quot; example in large training set quickly , eg Efficient memory indexing using kd-tree | . Pros . Highly effective transductive inference method for noisy training data and complex target functions. | Target function for a whole space may be described as a combinations of less complex local approximations | Trains very fast (Lazy Learner) | . Cons . Curse of dimensionality | Storage: all training example are saved in memory | slow at query time, can be overcome by pre sorting and indexing training samples. | . Convergence of 1-NN . $P left( mathrm{KNNError} right)=2 left( mathrm{Bayes} ; mathrm{Optimal} ; mathrm{Error} ; mathrm{Rate} right)$ Probability of K NN error is at most twice the bayes optimal error, bayes optimal error is best(least) error we can get using machine learning It is Possible to show that: as the size of training data set approaches infinity, the one nearest neighbor classifier guarantees an error rate of no worse than twice the bayes error rate ( the minimum achievable error rate given the distribution of the data). . Density Estimation using KNN . Non parametric Density Estimation using KNN. nstead of fixing bin width h and counting the mumber of instances, fix the instances(neighbors) k and check bin width $ hat{p} left(X right)= frac{k}{2{ mathrm{Nd}}_k left(X right)}$ here $d_k left(X right)$ is the $k_{ mathrm{th}}$ closest distance to to $X$ , This is also known as Parzen density estimation. . KNN example using sklearn . Import libraries . import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap from sklearn import neighbors, datasets iris = datasets.load_iris() . Print shapes and class information . print(&#39;features: &#39;,iris.feature_names) print(&#39;target: &#39;,iris.target) print(&#39;classess: &#39;,iris.target_names) X = iris.data y = iris.target print(&#39;input data shape:&#39;,X.shape) print(&#39;target shape: &#39;,y.shape) . features: [&#39;sepal length (cm)&#39;, &#39;sepal width (cm)&#39;, &#39;petal length (cm)&#39;, &#39;petal width (cm)&#39;] target: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] classess: [&#39;setosa&#39; &#39;versicolor&#39; &#39;virginica&#39;] input data shape: (150, 4) target shape: (150,) . Plot the data . plt.scatter(iris.data[:,1],iris.data[:,2],c=iris.target, cmap=plt.cm.Paired) plt.xlabel(iris.feature_names[1]) plt.ylabel(iris.feature_names[2]) plt.show() plt.scatter(iris.data[:,0],iris.data[:,3],c=iris.target, cmap=plt.cm.Paired) plt.xlabel(iris.feature_names[0]) plt.ylabel(iris.feature_names[3]) plt.show() . split the dataset into test set and train set . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0) . Fit the classifier with different values of k . from sklearn.neighbors import KNeighborsClassifier from sklearn import metrics k_range = range(1,26) scores = {} scores_list = [] for k in k_range: knn = KNeighborsClassifier(n_neighbors=k) knn.fit(X_train,y_train) y_pred=knn.predict(X_test) scores[k] = metrics.accuracy_score(y_test,y_pred) scores_list.append(metrics.accuracy_score(y_test,y_pred)) . Plot the scores: . plt.plot(k_range,scores_list) plt.xlabel(&#39;Value of K for KNN&#39;) plt.ylabel(&#39;Testing Accuracy&#39;); . chose the best value of K for final model. . Tip: In KNN, finding the value of k is not easy. A small value of k means that noise will have a higher influence on the result and a large value make it computationally expensive. Data scientists usually choose as an odd number if the number of classes is 2 and another simple approach to select k is set k=sqrt(n). There is one more widely used method called Elbow Method which is also used to find the value of K, here we plot error rate vs k value and chose k value at elbow point. . knn = KNeighborsClassifier(n_neighbors=5) knn.fit(X_train,y_train); . from sklearn.metrics import confusion_matrix print(&#39;confusion matrix on test data: &#39;) print(confusion_matrix(y_test, y_pred)) . confusion matrix on test data: [[11 0 0] [ 0 13 0] [ 0 0 6]] . from sklearn.metrics import classification_report print(&#39;classification report on test data:&#39;) print(classification_report(y_test, y_pred, target_names=iris.target_names)) . classification report on test data: precision recall f1-score support setosa 1.00 1.00 1.00 11 versicolor 1.00 1.00 1.00 13 virginica 1.00 1.00 1.00 6 accuracy 1.00 30 macro avg 1.00 1.00 1.00 30 weighted avg 1.00 1.00 1.00 30 .",
            "url": "https://abhiyantaabhishek.github.io/MDS-IIT/foundations%20of%20machine%20learning/2022/08/20/CS5590-week2.html",
            "relUrl": "/foundations%20of%20machine%20learning/2022/08/20/CS5590-week2.html",
            "date": " • Aug 20, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "FoML CS5590 Assignment 0",
            "content": "This homework is intended to review basic pre-requisites in the following topics: . Linear Algebra, Probability, Python programming | . Practice Questions . The questions below are only for your practice - no submission required. (The exam could include questions from this list though!) . 1. Numpy . The following questions should be done using numpy operations. Each exercise can be solved with a maximum of 4-5 lines of Python code. Please avoid the use of iterative constructs (such as for loops) to the extent possible, and use matrix/vector operations to achieve the objectives. Please use https://docs.python.org/3/tutorial/ for any reference required. . (a) Import the numpy package under the name np. Print the numpy version and the configuration. . import numpy as np np.set_printoptions(precision=3) print(&#39;Version: n&#39;,np.__version__) # print(&#39; nconfiguration: n&#39;,np.show_config()) . Version: 1.21.5 . (b) Create a null vector of size 10, and output the vector to the terminal. . null_vector = np.zeros(10) print(null_vector) . [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] . (c) Create a null vector of size 10 but the fifth value which is 1. Output the vector to the terminal. . null_vector = (np.arange(10) == 4).astype(int) print(null_vector) . [0 0 0 0 1 0 0 0 0 0] . (d) Reverse a vector (first element becomes last). . vector=np.array([1,2,3,4,5]) print(vector[::-1]) . [5 4 3 2 1] . (e) Create an $n × n$ array with checkerboard pattern of zeros and ones. . n=5 checkerboard = np.ones(shape=(5,5)) checkerboard[::2,::2]=0 checkerboard[1::2,1::2]=0 print(checkerboard) . [[0. 1. 0. 1. 0.] [1. 0. 1. 0. 1.] [0. 1. 0. 1. 0.] [1. 0. 1. 0. 1.] [0. 1. 0. 1. 0.]] . (f) Given an $n × n$ array, sort the rows of array according to $m^{th}$ column of array. . n=5 m=2 # sort by column random_array = np.random.randint(1,100,size=(n,n)) print(&#39;Original array: n&#39;,random_array) print(&#39;sorted array: n&#39;,random_array[random_array[:,m].argsort()]) . Original array: [[88 22 64 73 98] [ 7 63 33 96 69] [22 22 32 79 55] [33 18 76 85 44] [17 93 71 87 88]] sorted array: [[22 22 32 79 55] [ 7 63 33 96 69] [88 22 64 73 98] [17 93 71 87 88] [33 18 76 85 44]] . (g) Create an $n × n$ array with $(i + j)^{th}$-entry equal to $i + j$. . n=5 print(np.add.outer(range(n),range(n))) . [[0 1 2 3 4] [1 2 3 4 5] [2 3 4 5 6] [3 4 5 6 7] [4 5 6 7 8]] . (h) Consider a $ left(6,7,8 right)$ shape array, what is the index $ left(x,y,z right)$ of the $100^{th}$ element (of the entire structure)? . array = np.arange(0,6*7*8).reshape((6,7,8)) n=100 x = int(n/(7*8)) n=n%(7*8) y = int(n/(8)) z=n%(8) print(&#39;Answer: (x,y,z) = ({},{},{})&#39;.format(x,y,z)) assert array.flatten()[x*7*8+y*8+z] == array[x,y,z] # OR print (&#39;Answer: (x,y,z) = ({},{},{})&#39;.format(*np.unravel_index(100, (6,7,8)))) . Answer: (x,y,z) = (1,5,4) Answer: (x,y,z) = (1,5,4) . (i) Multiply a $5 × 3$ matrix by a $3 × 2$ matrix (real matrix product). . matA = np.random.randint(9,size=(5,3)) matB = np.random.randint(9,size=(3,2)) prod = np.matmul(matA,matB) print(&#39;matA: n&#39;,matA) print(&#39;matB: n&#39;,matB) print(&#39;product: n&#39;,prod) . matA: [[3 2 6] [6 1 6] [8 1 8] [4 8 4] [5 0 2]] matB: [[3 7] [5 4] [5 1]] product: [[49 35] [53 52] [69 68] [72 64] [25 37]] . (j) Create random vector of size $10$ and replace the maximum value by $0$. . mat = np.random.randint(9,size=10) print(&#39;mat: n&#39;,mat) mat[mat.argmax()]=0 print(&#39;New mat: n&#39;,mat) . mat: [7 8 4 3 8 2 0 3 7 0] New mat: [7 0 4 3 8 2 0 3 7 0] . (k) How to find the closest value (to a given scalar) in an array? . mat = np.random.randint(9,size=10) scalar = 5 print(&#39;mat: n&#39;,mat) closest = mat[np.absolute(mat-scalar).argmin()] print(&#39;closest value to scalar={} is {}&#39;.format(scalar,closest)) . mat: [4 5 3 7 8 6 3 4 0 4] closest value to scalar=5 is 5 . (l) Subtract the mean of each row from each corresponding row of a matrix. . mat = np.random.randint(9,size=(4,3)) print(&#39;mat: n&#39;,mat) mean = mat.mean(axis=1) new_mat = mat-mean.T[...,None] print(&#39;After subtracting mean from each row: n&#39;,new_mat) . mat: [[8 6 4] [5 2 7] [5 1 4] [8 0 1]] After subtracting mean from each row: [[ 2. 0. -2. ] [ 0.333 -2.667 2.333] [ 1.667 -2.333 0.667] [ 5. -3. -2. ]] . (m) Consider a given vector, how to add 1 to each element indexed by a second vector (be careful with repeated indices - you should consider it only once)? . mat = np.random.randint(9,size=4) indexes = np.array([0,0,3]) print(&#39;mat: n&#39;,mat) mat[np.unique(indexes)]+=1 print(&#39;mat: n&#39;,mat) . mat: [4 5 6 7] mat: [5 5 6 8] . (n) How to find the most frequent value in an array? . mat = np.random.randint(3,size=5) print(&#39;mat: n&#39;,mat) value,count = np.unique(mat,return_counts=True) print(&#39;most frequent value = {}&#39;.format(value[count.argmax()])) . mat: [0 2 1 0 0] most frequent value = 0 . (o) Extract all the contiguous $3 × 3$ blocks from a random $10 × 10$ matrix. . mat = np.random.randint(0,9,(10,10)) n=3 i=1+(mat.shape[0]-n) j=1+(mat.shape[1]-n) res = np.lib.stride_tricks.as_strided(mat,shape=(i,j,n,n),strides=mat.strides+mat.strides) . (p) Compute the rank, trace and determinant of a matrix. . mat = np.random.randint(0,9,(4,4)) print(&#39;Matrix : n{}&#39;.format(mat)) print(&#39;Matrix Rank :{}&#39;.format(np.linalg.matrix_rank(mat))) print(&#39;Matrix Determinant :{:.2f}&#39;.format(np.linalg.det(mat))) . Matrix : [[2 2 2 0] [5 2 5 0] [7 3 5 4] [8 5 2 1]] Matrix Rank :4 Matrix Determinant :-132.00 . 2. Linear Algebra . (a) Prove or disprove: Empty set is a vector space. . Answer:Axioms of vector spaces: A real vector space is a set $X$ with a special element $0$, and three operations: Addition: Given two elements $x$, $y$ in $X$, one can form the sum $x+y$, which is also an element of $X$. | Inverse: Given an element $x$ in $X$, one can form the inverse $-x$, which is also an element of $X$. | Scalar multiplication: Given an element $x$ in $X$ and a real number $c$, one can form the product $cx$, which is also an element of $X$. | . | As Empty set doesn&#39;t contain element $0$ Hence we can say that Empty set is NOT a vector space | . | . (b) Show that the inverse of $M=I+ left({ mathbf{uv}}^T right)$ is of the type $I+ alpha left({ mathbf{uv}}^T right)$, where $ mathit{ mathbf{u}}, mathit{ mathbf{v}} in { mathbb{R}}^n ,{ mathit{ mathbf{v}}}^T mathit{ mathbf{u}} not= 0$ . Continuing from previous question, find $ alpha$. | For what $u$ and $v$ is $M$ singular? | Find the null space of $M$, if it is singular. | Answer: | . (c) . Consider the 2 × 2 matrix: $A= left lbrack begin{array}{cc} -2 &amp; 2 -6 &amp; 5 end{array} right rbrack$ There exists vectors such that when the matrix $A$ acts on those vectors, the subspace of the vectors does not change. Mathematically, $Ax = lambda I$. The vectors $x$ are called eigenvectors and the values $ lambda$ are the corresponding eigenvalues. Find the eigenvalues and corresponding eigenvectors for the matrix $A$. | Consider a diagonal matrix $ Lambda$ which has the eigenvalues of $A$ as its diagonal entries. Find the matrix $U$ such that the equation $AU = U Lambda$ holds. | Note that we can write the matrix $A$ as $A$ = $U Lambda U^{−1}$. Find the inverse of the matrix $U$ computed in the previous question and verify. | Answer: | . $A= left lbrack begin{array}{cc} -2 &amp; 2 -6 &amp; 5 end{array} right rbrack$ $ mathrm{Ax}= lambda x$ $ Rightarrow mathrm{Ax}- lambda ;x=0 Rightarrow left(A- lambda I right)x=0$ $ left lbrack begin{array}{cc} -2 &amp; 2 -6 &amp; 5 end{array} right rbrack - lambda left lbrack begin{array}{cc} 1 &amp; 0 0 &amp; 1 end{array} right rbrack =0$ $ left lbrack begin{array}{cc} -2 &amp; 2 -6 &amp; 5 end{array} right rbrack - left lbrack begin{array}{cc} lambda &amp; 0 0 &amp; lambda end{array} right rbrack =0$ $ left lbrack begin{array}{cc} -2- lambda &amp; 2 -6 &amp; 5- lambda end{array} right rbrack =0$ $ left(-2- lambda right) left(5- lambda right)-2 left(-6 right)=0$ $-10+2 lambda -5 lambda_{ ;} + lambda^2 +12=0$ $ lambda^2 -3 lambda +2=0$ $ left( lambda -2 right) left( lambda -1 right)=0$ $ lambda =2, lambda =1$ for $ lambda =2$ put $ lambda$ value in $ left(A- lambda I right)x=0$ $ left lbrack begin{array}{cc} -2- lambda &amp; 2 -6 &amp; 5- lambda end{array} right rbrack left lbrack begin{array}{c} x y end{array} right rbrack = left lbrack begin{array}{c} 0 0 end{array} right rbrack$ $ left lbrack begin{array}{cc} -2-2 &amp; 2 -6 &amp; 5-2 end{array} right rbrack left lbrack begin{array}{c} x y end{array} right rbrack = left lbrack begin{array}{c} 0 0 end{array} right rbrack$ $ left lbrack begin{array}{cc} -4 &amp; 2 -6 &amp; 3 end{array} right rbrack left lbrack begin{array}{c} x y end{array} right rbrack = left lbrack begin{array}{c} 0 0 end{array} right rbrack$ We get below 2 equations: $-4x+2y=0$ $-6x+3y=0$ After solving we get : $ left lbrack begin{array}{c} x y end{array} right rbrack = left lbrack begin{array}{c} 1 2 end{array} right rbrack$ Similarly for $ lambda$=1, we get: $ left lbrack begin{array}{cc} -2- lambda &amp; 2 -6 &amp; 5- lambda end{array} right rbrack left lbrack begin{array}{c} x y end{array} right rbrack = left lbrack begin{array}{c} 0 0 end{array} right rbrack$ $ left lbrack begin{array}{cc} -2-1 &amp; 2 -6 &amp; 5-1 end{array} right rbrack left lbrack begin{array}{c} x y end{array} right rbrack = left lbrack begin{array}{c} 0 0 end{array} right rbrack$ $ left lbrack begin{array}{cc} -3 &amp; 2 -6 &amp; 4 end{array} right rbrack left lbrack begin{array}{c} x y end{array} right rbrack = left lbrack begin{array}{c} 0 0 end{array} right rbrack$ $-3x+2y=0$ $-6x+4y=0$ After solving above 2 equations we get: $ left lbrack begin{array}{c} x y end{array} right rbrack = left lbrack begin{array}{c} 2 3 end{array} right rbrack$ | $ mathrm{AU}=U Lambda$ holds when $U$ is a the matrix with all the eigen vectors. Hence $U= left lbrack begin{array}{cc} 2 &amp; 1 3 &amp; 2 end{array} right rbrack$ | $A$ = $U Lambda U^{−1}$ $U= left lbrack begin{array}{cc} 2 &amp; 1 3 &amp; 2 end{array} right rbrack$ $U^{-1} = frac{1}{|U|} cdot mathrm{Adj} ;U$ $ Rightarrow U^{-1} = frac{1}{4-3} left lbrack begin{array}{cc} 2 &amp; -1 -3 &amp; 2 end{array} right rbrack$ $ Rightarrow U^{-1} = left lbrack begin{array}{cc} 2 &amp; -1 -3 &amp; 2 end{array} right rbrack$ $U^{ ;} Lambda U^{-1} = left lbrack begin{array}{cc} 2 &amp; 1 3 &amp; 2 end{array} right rbrack left lbrack begin{array}{cc} 1 &amp; 0 0 &amp; 2 end{array} right rbrack left lbrack begin{array}{cc} 2 &amp; -1 -3 &amp; 2 end{array} right rbrack$ $ Rightarrow U^{ ;} Lambda U^{-1} = left lbrack begin{array}{cc} -2 &amp; 2 -6 &amp; 5 end{array} right rbrack$ Which is ame as $A$, hence proved. | Below is the solution of above question using Numpy, The Eigen Vector seems to be different because it&#39;s normalized one. . import numpy as np m = np.array([[-2, 2], [-6, 5]]) eigValue,eigVector = np.linalg.eig(m) print(&#39;eigen Value: n&#39;,eigValue) print(&#39;eigen Vector: n&#39;,eigVector) . eigen Value: [1. 2.] eigen Vector: [[-0.555 -0.447] [-0.832 -0.894]] . U = np.array([[2, 1], [3, 2]]) Lambda = np.array([[1, 0], [0, 2]]) U_inv = np.linalg.inv(U) print(U.dot(Lambda).dot(U_inv)) . [[-2. 2.] [-6. 5.]] . (c) Show that for any square matrix $A$, the eigenvectors of $A$ are also eigenvectors of $A^2$. What are the eigenvalues for $A^2$? .",
            "url": "https://abhiyantaabhishek.github.io/MDS-IIT/quizzes%20and%20assignments%20-%20foundations%20of%20machine%20learning/2022/08/20/CS5590-Assignment-0.html",
            "relUrl": "/quizzes%20and%20assignments%20-%20foundations%20of%20machine%20learning/2022/08/20/CS5590-Assignment-0.html",
            "date": " • Aug 20, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Discrete Random variable  CS6660 week 2",
            "content": "A random variable is a function from a sample space $ Omega$ to the real numbers $ mathbb{R}$ A random variable $X$ that can take on finitely or countably infinitely many possible values is called discrete. . Indicator random variable . $X= left lbrace begin{array}{ll} 1, &amp; mathrm{if} ; mathrm{event} ;E ; mathrm{occurs} 0, &amp; mathrm{if} ; mathrm{event} ;E^c ; mathrm{occurs} end{array} right.$ $ mathrm{EX}=0 cdot p left(0 right)+1 cdot p left(1 right)=P left lbrace E right rbrace$ $ mathrm{VarX}={ mathrm{EX}}^2 -{ left( mathrm{EX} right)}^2 = left(1^2 cdot P left lbrace E right rbrace +0^2 cdot P left lbrace E^c right rbrace right)-{ left(P left lbrace E right rbrace right)}^2 =P left lbrace E right rbrace cdot left(1-P left lbrace E right rbrace right)$ $ mathrm{SD} ;X= sqrt{P left lbrace E right rbrace cdot left(1-P left lbrace E right rbrace right)}$ . Mass function . Let $X$ be a discrete random variable with possible values $X_1 ,X_2 , ldotp ldotp ldotp$ The probability mass function (pmf), or distribution of a random variable tells us the probabilities of these possible values: $p_X left(x_i right)=P left lbrace X=x_i right rbrace$ For any discrete random variable $X$ $p left(X_i right) ge 0,$ and $ sum_i p left(X_i right)=1$ . Expectation . The expection, or mean, or expected value of a discrete random variable $X$ is defined as $EX= sum_i X_i cdot p left(X_i right)$, provided that this sum exists | Expected value is not necessarily a possible value | Expected value can be infinity | Expected value many not exist | $E left( mathrm{aX}+b right)=a cdot mathrm{EX}+b$ proof: $E left( mathrm{aX}+b right)= sum_i left( mathrm{aX}+b right) cdot p left(i right)=a sum_i X cdot p left(i right)+b sum_i p left(i right)=a cdot mathrm{EX}+b$ | . | ${ mathrm{EX}}^n =E left(X^n right) not= { left( mathrm{EX} right)}^n$ | . Variance . The variance and the standard deviation of a random variable are defined as $ mathrm{VarX}:={E left(X- mathrm{EX} right)}^2$ and $ mathrm{SDX}:= sqrt{ ; mathrm{VarX}}$ . Properties of the Variance . $ mathrm{VarX}={ mathrm{EX}}^2 -{ left( mathrm{EX} right)}^2$ proof : $ begin{array}{l} mathrm{VarX}:={E left(X- mathrm{EX} right)}^2 =E left( left(X^2 -2 cdot X cdot mathrm{EX}+{ left( mathrm{EX} right)}^2 right) right) =E left(X^2 right)-E left(2 cdot X cdot mathrm{EX} right)+{E left({ left( mathrm{EX} right)}^2 right)}^{ ;} =E left(X^2 right)-2 cdot mathrm{EX} cdot mathrm{EX}+{ left( mathrm{EX} right)}^2 ={E left({ mathrm{X}}^2 right)}^{ ;} -{ left( mathrm{EX} right)}^2 end{array}$ Here $EX$ is a constant so can come out of $E left(2 cdot X cdot mathrm{EX} right)$ and it becomes $2 cdot mathrm{EX} cdot mathrm{EX}$ also expectation has no effect on ${E left({ left( mathrm{EX} right)}^2 right)}^{ ;}$ for the same reason so it becomes ${ left( mathrm{EX} right)}^2$ | corollary: for any $X,{ mathrm{EX}}^2 ge { left( mathrm{EX} right)}^2$ here equality hold only if $X=$constant a.s. (almost always means probability one) | . | $ mathrm{Var} left( mathrm{aX}+b right)=a^2 cdot mathrm{VarX}$ proof $ begin{array}{l} mathrm{Var} left( mathrm{aX}+b right)={E left( mathrm{aX}+b right)}^2 -{ left(E left( mathrm{aX}+b right) right)}^2 =E left(a^2 X^2 +2 mathrm{abX}+b^2 right)-{ left( mathrm{aEX}+b right)}^2 =a^2 { cdot mathrm{EX}}^2 +2 mathrm{ab} cdot mathrm{EX}+b^2 -a^2 cdot { left( mathrm{EX} right)}^2 -2 mathrm{ab} cdot mathrm{EX}-b^2 =a^2 left({ mathrm{EX}}^2 -{ left( mathrm{EX} right)}^2 right)=a^2 mathrm{VarX} end{array}$ | . | $ mathrm{Var} left(X+b right)= mathrm{VarX}= mathrm{Var} left(-X right)$ | . . Tip: $ mathrm{VarX}={ mathrm{EX}}^2 -{ left( mathrm{EX} right)}^2$$ mathrm{Var} left( mathrm{aX}+b right)=a^2 cdot mathrm{VarX}$$ mathrm{Var} left(X+b right)= mathrm{VarX}= mathrm{Var} left(-X right)$ . Bernoulli, Binomial . Suppose that $n$ independent trails are performed, each succeeding with probability $p$. Let $X$ count the number of success within the $n$ trails. Then $X$ has the Binomial distribution with parameters $n$ and $p$ of, in short $X~ mathrm{Binom} left(n,p right)$ . Mass Function . $p left(i right)=P left lbrace X=i right rbrace = {n choose k} p^i { left(1-p right)}^{n-i} , ; ; ; ;i=0,1, ldotp ldotp ldotp ldotp ,n$ $ mathrm{EX}= mathrm{np},$ and $ mathrm{VarX}= mathrm{np} left(1-p right)$ .",
            "url": "https://abhiyantaabhishek.github.io/MDS-IIT/mathematical%20foundations%20of%20data%20science/2022/08/13/CS6660-week2.html",
            "relUrl": "/mathematical%20foundations%20of%20data%20science/2022/08/13/CS6660-week2.html",
            "date": " • Aug 13, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Probability Theory CS6660 week 1",
            "content": "Basics . The Union and intersection . The union $E cup F ;$ of events $E$ and $F$ always means $E$ OR $F$ , The intersection $E cap F$ of events $E$ and $F$ always means $E$ AND $F$ . Tip: The union $ bigcup_i E_{i ;}$ of events $E_{i ; ;}$ always means at least one of the $E_i$&#8217;s, The intersection $ bigcap_i E_i$ of events $E_{i ;}$ always means each of the $E_i$&#8217;s . Complementary Events . The complement of an event is $E^c = bar{E} =E^* := Omega -E$ . Simple Properties of events . commutativity: $ begin{array}{l} E cup F=F cup E E cap F=F cap E end{array}$ | Associativity: $ begin{array}{l} E cup left(F cup G right)= left(E cup F right) cup G=E cup F cup G E cap left(F cap G right)= left(E cap F right) cap G=E cap F cap G end{array}$ | Distributivity: $ begin{array}{l} left(E cup F right) cap G= left(E cap G right) cup left(F cap G right) left(E cap F right) cup G= left(E cup G right) cap left(F cup G right) ; end{array}$ | De Morgan&#39;s Law: $ begin{array}{l} { left(E cup F right)}^c =E^{c ;} cap F^{c ;} { left(E cap F right)}^{c ;} =E^c cup F^c end{array}$ Similarly $ begin{array}{l} { left({ bigcup_{ ; ;} }_i E_{i ;} right)}^c ={ bigcap_{ ;} }_i E_{i ;}^{c ;} { left({ bigcap_{ ;} }_i E_{i ;} right)}^{c ;} ={ bigcup_{ ;} }_i E_i^{c ;} end{array}$ | . Definition (axioms of probability) . The probability $P$ on a sample space $ Omega$ assigns numbers to events $ Omega$ of in such a way that . The probability of any event is non-negative : $P left lbrace E right rbrace ge 0$ | The probability if the sample space is one : $P left lbrace Omega ; right rbrace =1$ | For any finitely or countably infinitely many manually exclusive events $E_{1,} E_2 , ldotp ldotp ldotp$ , $P left lbrace { bigcup_{i ;} E_{i ;} }_{ ;} right rbrace = sum_i P left lbrace E_i right rbrace ;$ | A few simple facts . Inclusion-exclusion principle: . For any events $E$ and $F$, $P left lbrace E cup F right rbrace =P left lbrace E right rbrace +P left lbrace F right rbrace -P left lbrace E cap F right rbrace$ | For any events $E$, $F$ and $G$: $P left lbrace E cup F cup G right rbrace =P left lbrace E right rbrace +P left lbrace F right rbrace +P left lbrace G right rbrace -P left lbrace E cap F right rbrace -P left lbrace E cap G right rbrace -P left lbrace F cap G right rbrace +P left lbrace E cap F cap G right rbrace$ | Generally: $p left lbrace E_1 cup E_2 cup E_3 cup ldotp ldotp ldotp cup E_n right rbrace = sum_{1 le i le n} P left lbrace E_i right rbrace - sum_{1 le i_1 le i_2 le n} P left lbrace E_{i_1 } cap E_{i_2 } right rbrace + sum_{1 le i_1 le i_2 le i_{3 ;} le n} left lbrace P left lbrace E_{i_1 } cap E_{i_2 } cap E_{i_3 } right rbrace right rbrace - ldotp ldotp ldotp ;+{ left(-1 right)}^{n+1} P left lbrace E_1 cap E_2 cap E_3 right rbrace$ | . Boole&#39;s inequality . For any events $E_1 ,E_2 , ldotp ldotp ldotp E_n$ $P left lbrace bigcup_{i=1}^n E_i right rbrace le sum_{i=1}^n P left lbrace E_i right rbrace$ | . Example . Out of $n$ people, what is the probability that there are no coinciding birthdays? $| Omega |={365}^n$ $|E|=365 ldotp 364 ldotp ldotp ldotp left(365-n+1 right)= frac{365!}{ left(365-n right)!}$ $P left lbrace E right rbrace = frac{|E|}{| Omega |}= frac{365!}{ left(365-n right)!{365}^n }$ . Conditional Probability . Let $F$ be an Event with $P left lbrace F right rbrace &gt;0$ . then the conditional probability $E$ of given $F$ is defined as: $P left lbrace E|F right rbrace := frac{P left lbrace E cap F right rbrace }{P left lbrace F right rbrace }$ . Note: Conditional Probability can be interpreted as:&quot;In what proportion of case in $F$ will also $E$ occur?&quot; or &quot;How does the probability of both $E$ and $F$ compare to the probability of $F$ only?&quot; conditional probability is a proper probability and it satisfies the axioms: . The conditional probability of any event is non-negative :$P left lbrace E|F right rbrace ge 0$ | The conditional probability if the sample space is one :$P left lbrace Omega |F ; right rbrace =1$ | For any finitely or countably infinitely many manually exclusive events $E_{1,} E_2 , ldotp ldotp ldotp$ , $P left lbrace { bigcup_{i ;} E_{i ;} |F}_{ ;} right rbrace = sum_i P left lbrace E_i |F right rbrace ;$ | Corollary . $P left lbrace E^c |F right rbrace =1-P left lbrace E|F right rbrace$ | $P left lbrace phi |F right rbrace =0$ | $P left lbrace E|F right rbrace =1-P left lbrace E^c |F right rbrace le 1$ | $P left lbrace left(E cup G right)|F right rbrace =P left lbrace E|F right rbrace +P left lbrace G|F right rbrace -P left lbrace E cap F|F right rbrace$ proof: $ begin{array}{l} P left lbrace left(E cup G right)|F right rbrace = frac{P left lbrace left(E cup G right) cap F right rbrace }{P left lbrace F right rbrace }= frac{P left lbrace left(E cap F right) cup left(G cap F right) right rbrace }{P left lbrace F right rbrace } = frac{P left(E cap F right)+P left(G cap F right)-P left lbrace left(E cap F right) cap ; left(G cap F right) right rbrace }{P left lbrace F right rbrace }= frac{P left(E cap F right)+P left(G cap F right)-P left lbrace E cap G cap F right rbrace }{P left lbrace F right rbrace } =P left lbrace E|F right rbrace +P left lbrace G|F right rbrace -P left lbrace E cap F|F right rbrace end{array}$ | if $E subseteq G$ then $P left lbrace left(G-E right)|F right rbrace =P left lbrace G|F right rbrace -P left lbrace E|F right rbrace$ proof: $ begin{array}{l} P left lbrace G|F right rbrace -P left lbrace E|F right rbrace = frac{ ;P left lbrace G cap F right rbrace }{P left lbrace mathrm{F} right rbrace }- frac{ ;P left lbrace E cap F right rbrace }{P left lbrace mathrm{F} right rbrace } = frac{P left lbrace left(G cap F right)- left(E cap F right) right rbrace ;}{P left lbrace mathrm{F} right rbrace }= frac{P left lbrace left(G cap F right) cap { left(E cap F right)}^c right rbrace ;}{P left lbrace mathrm{F} right rbrace } = frac{P left lbrace left(G cap F right) cap { left(E^{c ;} cup F^{c ;} right)}^{ ;} right rbrace ;}{P left lbrace mathrm{F} right rbrace }= frac{P left lbrace G cap left(F cap { left(E^{c ;} cup F^{c ;} right)}^{ ;} right) right rbrace ;}{P left lbrace mathrm{F} right rbrace } = frac{P left lbrace G cap left({ left({F cap ;E}^{c ;} right) cup left({F cap ;F}^{c ;} right)}^{ ;} right) right rbrace ;}{P left lbrace mathrm{F} right rbrace }= frac{P left lbrace G cap {F cap ;E}^{c ;} right rbrace ;}{P left lbrace mathrm{F} right rbrace } = frac{P left lbrace G cap { ;E}^{c ;} cap F right rbrace ;}{P left lbrace mathrm{F} right rbrace }= frac{P left lbrace left(G-{ ;E}^{ ;} right) cap F right rbrace ;}{P left lbrace mathrm{F} right rbrace }=P left lbrace left(G-E right)|F right rbrace end{array}$ if $A subseteq B$ then $P left(B right)-P left(A right)=P left(B_{ ;} -A right)$ Here as $E subseteq G$ so $ left(E cap F right) subseteq left(G cap F right)$ so we can write $P left lbrace G cap F right rbrace - ;P left lbrace E cap F right rbrace=P left lbrace left(G cap F right)- left(E cap F right) right rbrace$ | . . if $E subseteq G$ then $P left lbrace E|F right rbrace le P left lbrace G|F right rbrace$ | . Multiplication Rule . For $E_1 ,E_2 , ldotp ldotp ldotp E_n$ events: $P left lbrace E_1 cap E_2 cap ldotp ldotp ldotp cap E_n right rbrace =P left lbrace E_1 right rbrace ldotp P left lbrace E_2 |E_1 right rbrace ldotp P left lbrace E_3 |E_1 cap E_2 right rbrace ldotp ldotp ldotp ldotp P left lbrace E_n |E_1 cap E_2 cap ldotp ldotp ldotp cap E_{n-1} right rbrace$ . The law of total probability . This is also known as partition theorem For any events $E$ and $F$ $P left lbrace E right rbrace =P left lbrace E|F right rbrace ldotp P left lbrace E right rbrace +P left lbrace E|F^c right rbrace ldotp P left lbrace F^c right rbrace$ $P left lbrace E right rbrace = sum_i P left lbrace E|F_i right rbrace ldotp P left lbrace F_i right rbrace$ . Bayes&#39; Theorem . For any events $E$ and $F$ $P left lbrace F|E right rbrace = frac{P left lbrace E|F right rbrace ldotp P left lbrace F right rbrace }{P left lbrace E|F right rbrace ldotp P left lbrace F right rbrace +P left lbrace E|F^c right rbrace ldotp P left lbrace F^c right rbrace }$ . Important: if ${ left lbrace F_i right rbrace }_i$ is a complete system of events, then $$P left lbrace F_i |E right rbrace = frac{ ;P left lbrace E|F_i right rbrace ldotp P left lbrace F_i right rbrace }{ sum_j ;P left lbrace E|F_j right rbrace ldotp P left lbrace F_j right rbrace }$$ . Independence . Event $E$ and $F$ are independent if $P left lbrace E|F right rbrace =P left lbrace E right rbrace$ or $P left lbrace E cap F right rbrace =P left lbrace E right rbrace cdot P left lbrace F right rbrace$ . Important: Mutually exclusive events are necessarily also dependent events because one&#8217;s existence depends on the other&#8217;s non-existence.Dependent events are not necessarily mutually exclusive . If $A$ and $B$ are independent then $A^c$ and $B$ are also also independent Proof: $P left(A^c |B right)= frac{P left(A^c cap B right)}{P left(B right)}= frac{P left(B right)-P left(A^{ ;} cap ; ;B right)}{P left(B right)}=1-P left(A|B right)=1-P left(A right)=P left(A^c right)$ | . Three events $E$, $F$ and $G$ are (mutually) independent if . $P left lbrace E cap F right rbrace =P left lbrace E right rbrace ldotp P left lbrace F right rbrace$ | $P left lbrace E cap G right rbrace =P left lbrace E right rbrace ldotp P left lbrace G right rbrace$ | $P left lbrace F cap G right rbrace =P left lbrace F right rbrace ldotp P left lbrace G right rbrace$ | $P left lbrace E cap F cap G right rbrace =P left lbrace E right rbrace ldotp P left lbrace F right rbrace ldotp P left lbrace G right rbrace$ | .",
            "url": "https://abhiyantaabhishek.github.io/MDS-IIT/mathematical%20foundations%20of%20data%20science/2022/08/06/CS6660-week1.html",
            "relUrl": "/mathematical%20foundations%20of%20data%20science/2022/08/06/CS6660-week1.html",
            "date": " • Aug 6, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Machine Learning Introduction CS5590  week 1",
            "content": "Types of ML . Supervised Learning classification, regression | . | Unsupervised learning | Other settings of ML Reinforcement learning | Semi-supervised learning | Active learning,Transfer learning,Structured learning | . | Dimensionality Reduction (unsupervised Learning) Large sample size is required for high dimensional data | Query accuracy and efficiency degrade rapidly as the dimension increases | strategies: Feature reduction, Feature selection, Manifold learning, Kernel learning | . | . | . . IID Assumption . Identically independently distributed : This is the assumption that the training data and testing data comes from the same distribution | . Types of Models . Induction : Model Learns by Induction, ( creating it&#39;s own rules for example, if we do extensive research while buying mobile, we create set rules, it is called induction). | Transductions: Model learns from references ( for example, if we ask our friends about mobile and we buy according to their suggestion, it is called Transduction). | Online : data could be a stream, data keeps coming over time. | Offline: data is already acquired and trained offline. | Generative: Learns the distribution, the model learns joint probability distribution. | Discriminative:Learns to discriminate without learning distribution. | Parametric : The model have parameters like $ mu$ and $ sigma$ | Non parametric: The model doesn&#39;t have parameters, as in K nearest neighbor (KNN) | . Classifier evaluation . . Training Error Not very useful | Relatively easy to obtain low error | $E_{ mathrm{train}} = frac{1}{n} sum_{i=1}^n mathrm{error} left(f_D left(X_i right),y_i right)$ | . | Generalization Error Measure of how well do we do on unseen data | $E_{ mathrm{gen}} = int mathrm{error} left(f_D left(X right),y right)p left(y,X right) mathrm{dX}$ | . | . Stratified sampling . First stratify instances by class, then randomly select instances from each class proportionally | It ensures that the proportion of each class remains same in training and validation set. | . Model Selection . Re-Substitution : not useful as it suggests to re- substitute the train data for validation as well | K - Fold cross-validation : Divide the data in K fold using stratified sampling, and the select some set for training and some for validation in each iteration. | Leave-one-out N-fold cross-validation | . | .",
            "url": "https://abhiyantaabhishek.github.io/MDS-IIT/foundations%20of%20machine%20learning/2022/08/06/CS5590-week1.html",
            "relUrl": "/foundations%20of%20machine%20learning/2022/08/06/CS5590-week1.html",
            "date": " • Aug 6, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am an M.tech Student at IIT Hyderabad, Also working as Senior Data Scientist at Bosch. .",
          "url": "https://abhiyantaabhishek.github.io/MDS-IIT/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://abhiyantaabhishek.github.io/MDS-IIT/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}